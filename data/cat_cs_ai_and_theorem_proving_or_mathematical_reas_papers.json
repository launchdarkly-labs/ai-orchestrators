[
  {
    "id": "2504.16021v1",
    "title": "Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support",
    "authors": "Dinithi Dissanayake, Suranga Nanayakkara",
    "published": "2025-04-22",
    "category": "cs.HC",
    "arxiv_url": "http://arxiv.org/abs/2504.16021v1",
    "pdf_url": "https://arxiv.org/pdf/2504.16021v1",
    "abstract": "Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support DINITHI DISSANAYAKE, Augmented Human Lab, National University of Singapore, Singapore SURANGA NANAYAKKARA, Augmented Human Lab, National University of Singapore, Singapore Flow Theory describes an optimal cognitive state where individuals experience deep focus and intrinsic motivation when a task\u2019s difficulty aligns with their skill level. In AI-augmented reasoning, interventions that disrupt the state of cognitive flow can hinder rather than enhance decision-making. This paper proposes a context-aware cognitive augmentation framework that adapts interventions based on three key contextual factors: type, timing, and scale. By leveraging multimodal behavioral cues (e.g., gaze behavior, typing hesitation, interaction speed), AI can dynamically adjust cognitive support to maintain or restore flow. We introduce the concept of cognitive flow, an extension of flow theory in AI-augmented reasoning, where interventions are personalized, adaptive, and minimally intrusive. By shifting from static interventions to context-aware augmentation, our approach ensures that AI systems support deep engagement in complex decision-making and reasoning without disrupting cognitive immersion. 1 Introduction The theory of flow, proposed by Csikszentmihalyi [ 3], defines an optimal psychological state in which individuals experience deep focus and intrinsic motivation when the challenge of a task is perfectly matched to their skill level. When a task is too easy, users become bored, while excessive difficulty leads to frustration. For example, in video games, the enjoyment of a game depends on several factors, including the player\u2019s skill level and the challenge the game presents. A game that appropriately matches a player\u2019s skill level fosters a sense of engagement, encouraging continued play. Similarly, any new intervention or augmentation should support the user in either attaining or maintaining the state of flow of whatever the current task the user is engaged in. In the domain of AI-augmented reasoning systems, the goal",
    "introduction": "is to enhance human decision-making by providing intelligent feedback on logical reasoning, bias detection, and argumentation [ 4,5,7]. The effectiveness of such systems depends on several factors, with the type, timing, and scale of the intervention playing crucial roles in determining the quality of the augmentation. For example, poorly timed or intrusive interventions can disrupt a user\u2019s state of cognitive flow (whereby we define the optimal cognitive state one would prefer), leading to disengagement rather than improving reasoning. Current AI reasoning assistants often adopt a static or one-size-fits-all approach, assuming that interventions should be uniformly applied across all users [ 4]. However, individuals respond to cognitive support in diverse ways [10]. Some may prefer direct interventions, such as explicit fact-checking and counterarguments, while others may benefit from Socratic questioning, which promotes self-guided reflection. The timing of the intervention is equally significant\u2014interrupting a user mid-thought with a suggestion can break their focus, whereas the ability to understand subtle nudges, such as gaze-based or gesture-based cues, might be able to guide the intervention to present their reasoning seamlessly without causing disruption. Thereby, we identify three key factors that impact the effectiveness of these interventions: This paper was presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning (AIREASONING-2025-01). This is the authors\u2019 version for arXiv. Authors\u2019 Contact Information: Dinithi Dissanayake, dinithi@ahlab.org, Augmented Human Lab, National University of Singapore, Singapore; Suranga Nanayakkara, suranga@ahlab.org, Augmented Human Lab, National University of Singapore, Singapore. 1arXiv:2504.16021v1 [cs.HC] 22 Apr 2025 2 Dinithi Dissanayake and Suranga Nanayakkara \u2022Type of intervention : Some individuals may prefer direct feedback, while others may respond better to Socratic questioning. \u2022Timing of intervention : Understanding when users are \"stuck\" through subtle cues, such as gaze or gesture- based signals, is critical. \u2022Scale of intervention : The magnitude of the intervention\u2014whether subtle or more direct\u2014also determines its impact. These three factors; type, timing, and scale, determine the effectiveness of interventions, which can be broadly categorized under the concept of context . This paper proposes that context-aware reasoning interventions should be designed to dynamically adapt based on real-time user engagement signals, ensuring that interventions enhance rather than disrupt the cognitive flow. Contextual awareness plays a pivotal role in identifying the appropriate type, timing, and scale of interventions, thus enabling personalized cognitive assistance. By maintaining this balance, the intervention fosters a sense of autonomy in task completion, ensuring that users feel they have accomplished the task themselves, rather than perceiving it as something imposed by an external system, thus supporting the notion of assistive augmentation [9]. With recent developments in multimodal AI models [ 1,6,11], deriving contextual awareness becomes a matter of mapping input modalities to cognitive assistance and understanding when, what, and how to intervene. By leveraging multimodal cues, such as gaze behavior, typing hesitation, and interaction patterns, AI systems can infer cognitive load and deliver interventions at moments that sustain deep reasoning rather than hinder it. Furthermore, longitudinal engagement data can be utilized to cluster users based on preferred reasoning styles and",
    "conclusion": "an individual\u2019s cognitive state. In the context of cognitive flow, AI systems must intelligently map multimodal inputs to determine whether an individual is in a state of deep engagement, facing cognitive overload, or experiencing disengagement due to insufficient challenge. Learned representations derived from behavioral data provide a means to infer cognitive load and engagement levels, enabling real-time adjustments to interventions. By balancing challenge and support based on real-time multimodal inputs, the system ensures that users neither disengage due to excessive difficulty nor lose interest due to low complexity. By integrating context-aware AI-driven augmentation with flow theory, we move toward systems that do more than assist with reasoning\u2014they optimize the conditions under which reasoning and critical thinking can thrive. Rather than disrupting engagement, AI enhances 4 Dinithi Dissanayake and Suranga Nanayakkara cognition in ways that feel seamless and natural, preserving a sense of agency while fostering deeper intellectual engagement. 4 Call to Action This paper presents a case for the importance of context awareness in AI-driven cognitive interventions. We introduce the concept of Cognitive Flow Alignment, emphasizing that interventions are most effective when they dynamically adjust to an individual\u2019s cognitive state, neither disrupting engagement nor allowing stagnation. Using insights from flow theory, we argue that AI systems should infer behavioral cues from multimodal data to sustain an optimal cognitive state. While current AI-based augmentation focuses on task performance and engagement metrics, we propose that future systems should be evaluated on their ability to contextually regulate cognitive flow. This requires a shift from static intervention strategies to adaptive models that personalize intervention type, intensity, and timing based on real-time user states. We advocate for a mixed-method evaluation framework combining behavioral analytics with subjective feedback to refine personalized intervention strategies that optimize cognitive augmentation while respecting individual differences in engagement and cognitive load."
  },
  {
    "id": "2512.20745v2",
    "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent",
    "authors": "Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang",
    "published": "2025-12-23",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2512.20745v2",
    "pdf_url": "https://arxiv.org/pdf/2512.20745v2",
    "abstract": "Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved re-\nmarkable progress in natural language reasoning with long chain-of-thought.\nHowever, they remain computationally inefficient and struggle with accuracy\nwhen solving problems requiring complex mathematical operations. In this work,\nwe present AgentMath, an agent framework that seamlessly integrates language\nmodels\u2019 reasoning capabilities with code interpreters\u2019 computational precision to\nefficiently tackle complex mathematical problems. Our approach introduces three\nkey innovations: (1) An automated method that converts natural language chain-\nof-thought into structured tool-augmented trajectories, generating high-quality su-\npervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic rein-\nforcement learning (RL) paradigm that dynamically interleaves natural language\ngeneration with real-time code execution. This enables models to autonomously\nlearn optimal tool-use strategies through multi-round interactive feedback, while\nfostering emergent capabilities in code refinement and error correction; (3) An\nefficient training system incorporating innovative techniques, including request-\nlevel asynchronous rollout scheduling, agentic partial rollout, and prefix-aware\nweighted load balancing, achieving 4-5\u00d7 speedup and making efficient RL train-\ning feasible on ultra-long sequences with scenarios with massive tool calls. Ex-\ntensive evaluations show that AgentMath achieves state-of-the-art performance on\nchallenging mathematical competition benchmarks including AIME24, AIME25,\nand HMMT25, substantially outperforming frontier open-source models of com-\nparable size. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8%\naccuracy respectively, achieving advanced capabilities. These results validate the\neffectiveness of our approach and pave the way for building more sophisticated\nand scalable mathematical reasoning agents.\n1",
    "introduction": "Large Reasoning Models (LRMs) such as o3 and DeepSeek-R1 have made remarkable progress\nin natural language reasoning with long chain-of-thought (CoT)(OpenAI et al., 2024; Team et al.,\n2025a; DeepSeek-AI et al., 2025; xAI, 2023; Claude, 2025; Team et al., 2023; Wei et al., 2022).\nHowever, when tackling mathematical problems that demand precise computation or intricate sym-\nbolic manipulation, including large-number arithmetic, complex equation solving, and geometric\nreasoning, pure text-based reasoning still has limitations: frequent computational errors necessitate\nredundant corrections, which in turn leads to inefficiency and erroneous results.\n\u2020Corresponding authors. This work was conducted during Luo\u2019s internship at Tencent and was supported\nby the CIE-Tencent Ph.D. Student Research Incentive Program (Tencent Hunyuan Special Fund).\n1arXiv:2512.20745v2  [cs.AI]  27 Dec 2025\nPreprint\nTo enhance computational efficiency and accuracy, recent work has explored incorporating external\ntools (i.e., code interpreters), delegating complex and error-prone computational steps to external\nenvironments(Li et al., 2025e; Zhou et al., 2025; Lin & Xu, 2025; Zhang et al., 2025b; Chen et al.,\n2023; Gao et al., 2023; Gou et al., 2023b). For instance, models like o3 and o4-mini have signif-\nicantly improved mathematical reasoning accuracy through tool invocation. Nevertheless, existing",
    "conclusion": "complex doesn\\'t define __round__ method \\n \\n </interpreter> \\n \\n Oops, the code above appears to be throwing an error. I need to fix this to ensure it runs successfully.< The error occurs because the `round()` function cannot handle complex numbers directly. I will modify the code to handle complex numbers correctly. \\n \\n <code> \\n ```python \\n import cmath \\n import math \\n \\n # Let omega be the primitive 5th root of unity, omega = exp(2*pi*i / 5) \\n omega = cmath.exp(2 * cmath.pi * 1j / 5) \\n \\n # Calculate the five terms g(omega^k) \\n g_0 = (omega**0)**2 - 2 * (omega**0) + 2 \\n g_1 = omega**2 - 2 * omega + 2 \\n g_2 = omega**4 - 2 * omega**2 + 2 \\n g_3 = omega**6 - 2 * omega**3 + 2 \\n g_4 = omega**8 - 2 * omega**4 + 2 \\n \\n # Calculate the product of the five terms \\n total_product = g_0 * g_1 * g_2 * g_3 * g_4 \\n \\n # Print the individual terms and the final product \\n print(f\"g(1) = {g_0.real:.5f}{g_0.imag:+.5f}j\") \\n print(f\"g(omega) = {g_1.real:.5f}{g_1.imag:+.5f}j\") \\n print(f\"g(omega^2) = {g_2.real:.5f}{g_2.imag:+.5f}j\") \\n print(f\"g(omega^3) = {g_3.real:.5f}{g_3.imag:+.5f}j\") \\n print(f\"g(omega^4) = {g_4.real:.5f} {g_4.imag:+.5f}j\") \\n print(f\"Total product = {total_product.real:.5f} {total_product.imag:+.5f}j\") \\n ``` \\n </code> \\n <interpreter>\\ng(1) = 1.00000+0.00000j \\n g(omega) = 0.57295 - 1.31433j \\n g(omega^2) = 3.92705 - 2.12663j \\n g(omega^3) = 3.92705+2.12663j \\n g(omega^4) = 0.57295+1.31433j \\n Total product = 41.00000-0.00000j \\n \\n </interpreter> \\n\\n The numerical calculation shows the total product is 41. The small imaginary part is due to floating-point inaccuracies\u2026 So here f=g(x), so f2=1, f1=-2, f0=2 \\n\\n g=r(x), so g1=-64, g0=-1 \\n\\n ......Thus P=8321. \\n\\n Now, 8321 mod 1000 is 321. \\n\\n So the remainder is 321. Figure 9: AgentMath Case study For code self-correction 34"
  },
  {
    "id": "2510.17496v2",
    "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models",
    "authors": "Giacomo Camposampiero, Michael Hersche, Roger Wattenhofer, Abu Sebastian, Abbas Rahimi",
    "published": "2025-10-20",
    "category": "cs.LG",
    "arxiv_url": "http://arxiv.org/abs/2510.17496v2",
    "pdf_url": "https://arxiv.org/pdf/2510.17496v2",
    "abstract": "We introduce I-RA VEN-X, a symbolic benchmark designed to evaluate generaliza-\ntion and robustness in analogical and mathematical reasoning for Large Language\nModels (LLMs) and Large Reasoning Models (LRMs). I-RA VEN-X extends\nI-RA VEN by increasing operand complexity, attribute range, and introducing per-\nceptual uncertainty. Compared to LLMs, empirical results on I-RA VEN-X show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. For instance, LRMs experience a\nsignificantly smaller degradation on arithmetic accuracy ( 80.5%\u219263.0% ) com-\npared to LLMs ( 59.3%\u21924.4% ). However, LRMs are still significantly challenged\nby reasoning under uncertainty ( \u221261.8% in task accuracy) and cannot effectively\nexplore multiple probabilistic outcomes in superposition.\n1",
    "introduction": "benchmarks for reasoning proficiency in LLMs and LRMs. Firstly, most of the problems involve only a few operands, representing an overly simplistic subset of analogical and mathematical relations. Most importantly, the test problems and their corresponding solutions are openly available online, increasing the risk of potential data leakage from the model\u2019s pre- and post-training stages as previously observed in other settings [ 12]. Furthermore, assuming the availability of anoracle perceptionhas become a standard practice in their translation from visual to textual (symbolic) tasks (necessary to test language-only models) [ 13,14]. This assumption is reasonable when the scope of the investigation is limited to the reasoning component; however, it falls short when we zoom out to more complex, end-to-end systems, as it bypasses crucial steps of the original visual analogical reasoning, such as filtering irrelevant attributes and accounting for the uncertainty of the perception module. 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: MATH-AI.arXiv:2510.17496v2 [cs.LG] 31 Oct 2025 To tackle these problems, this paper makes the following contributions: 1.introduces I-RA VEN-X, an enhanced, symbolic version of the standard I-RA VEN benchmark that enables testing the generalization and robustness to simulated perceptual uncertainty in text-based language and reasoning models (see Figure 1), 2.highlights that LRMs consistently generalize better than LLMs in terms of productivity and systematicity, but significantly fail to reason under uncertainty. 2 Methods 2.1 I-RA VEN-X: testing generalization and robustness of reasoning in LLMs and LRMs We propose a fully-symbolic, parametrizable dataset to evaluate LLMs and LRMs, dubbed I-RA VEN- X. Some examples from the dataset are included in Figure 1. I-RA VEN-X enhances the original I-RA VEN (more extensively described in Appendix A) over different dimensions: 1.Productivity: we parametrize the number of operands in the reasoning relations (e.g., using 3\u00d710 matrices instead of 3\u00d73, in Figure 1); 2.Systematicity: we introduce larger dynamic ranges for the operand values (e.g., 1000 attribute values instead of 10, in Figure 1); 3.Robustness to confounding factors: we augment the set of original attributes in RPM with randomly sampled values, which do not contribute to the reasoning ( in Figure 1); 4.Robustness to non-degenerate value distributions: we smoothen the distributions of the input values corresponding to the generative factors ( in Figure 1). Practically speaking, 1. and 2. enable testing the generalization of LLMs and LRMs to longer reasoning chains and an increased number of concepts. On the other hand, 3. and 4. allow to loosen the strong assumption of an oracle perception, simulating an imperfect sensory front-end while operating with text-only language models, hence providing an additional focus on the robustness of reasoning under uncertainty. More details on the design of 1\u22124 are included in Appendix B. In addition, the original I-RA VEN was narrowed down to a single constellation ( center , containing a single object per panel), which was observed to be simultaneously a strong test for a wide range of logical and arithmetic skills and unexpectedly challenging. Figure 1: This figure highlights all the different axes of generalization and",
    "conclusion": "<0.02::888,0.95::889,0.03::890>, <0.01::874,0.54::875,0.45::876>); row 3: (<0.21::531,0.77::532,0.02::533>, <0.01::496,0.88::497,0.11::498>, <0.07::830,0.62::831,0.31::832>), (<0.20::24,0.79::25,0.01::26>, <0.19::495,0.62::496,0.19::497>, <0.06::830,0.92::831,0.02::832>), (<0.17::30,0.56::31,0.27::32>, <0.27::494,0.64::495,0.09::496>, <0.02::830,0.98::831,0.00::832>), (<0.00::42,0.98::43,0.02::44>, <0.38::493,0.58::494,0.04::495>, <0.19::830,0.53::831,0.28::832>), (<0.07::573,0.52::574,0.41::575>, <0.01::492,0.99::493,0.00::494>, <0.01::830,0.81::831,0.18::832>), (<0.26::760,0.55::761,0.19::762>, <0.13::491,0.83::492,0.04::493>, <0.05::830,0.82::831,0.13::832>), (<0.47::575,0.52::576,0.01::577>, <0.15::490,0.59::491,0.26::492>, <0.16::830,0.81::831,0.03::832>), (<0.03::290,0.82::291,0.15::292>, <0.29::489,0.52::490,0.19::491>, <0.03::830,0.85::831,0.12::832>), (<0.08::916,0.81::917,0.11::918>, <0.05::488,0.83::489,0.12::490>, <0.09::830,0.64::831,0.27::832>), Answer set: Answer #0: (<0.06::289,0.83::290,0.11::291>, <0.00::487,1.00::488,0.00::489>, <0.03::874,0.82::875,0.15::876>) Answer #1: (<0.01::850,0.78::851,0.21::852>, <0.00::487,0.99::488,0.01::489>, <0.08::874,0.85::875,0.07::876>) Answer #2: (<0.03::289,0.57::290,0.40::291>, <0.15::450,0.75::451,0.10::452>, <0.15::830,0.62::831,0.23::832>) Answer #3: (<0.06::289,0.52::290,0.42::291>, <0.03::487,0.92::488,0.05::489>, <0.31::830,0.61::831,0.08::832>) Answer #4: (<0.02::850,0.95::851,0.03::852>, <0.16::450,0.63::451,0.21::452>, <0.20::874,0.52::875,0.28::876>) Answer #5: (<0.02::850,0.86::851,0.12::852>, <0.18::487,0.80::488,0.02::489>, <0.14::830,0.79::831,0.07::832>) Answer #6: (<0.01::289,0.96::290,0.03::291>, <0.38::450,0.59::451,0.03::452>, <0.08::874,0.68::875,0.24::876>) Answer #7: (<0.08::850,0.62::851,0.30::852>, <0.15::450,0.82::451,0.03::452>, <0.09::830,0.87::831,0.04::832>) Table 6: Example prompt for the I-RA VEN-X task with smooth distributions. 12 D Comparison between OpenAI o3-mini and o1 This Appendix presents a small ablation study on two different closed-source LRMs, OpenAI o1 and OpenAI o3-mini. The goal of these experiments was to measure the difference, if any, in the reasoning capabilities of the o3-mini model compared to its bigger, more expensive predecessor. We restricted the size of the test set to 100 test examples for both I-RA VEN and I-RA VEN-X. The results, presented in Table 7, show that the two models achieve roughly comparable performance on both I-RA VEN and I-RA VEN-X, with o3-mini being consistently slightly less accurate than o1. However, o1 is also considerably more expensive compared to o3: o1 is priced at $15 and $60 per million input and output tokens, respectively, while o3-mini costs only $1.1 and $4.4 per million input and output tokens (approximately 14\u00d7 less expensive). Hence, we opt to use only o3-mini in the full evaluation. Model SettingI-RA VEN I-RA VEN-X Range 10 Range 100 Range 1000 Task Arithm. Task Arithm. Task Arithm. OpenAI o1 Entangled 88.0 79.7 86.0 68.2 86.0 68.2 OpenAI o3-mini Entangled 86.6 81.4 84.0 63.6 81.0 60.8 Table 7: Task and arithmetic accuracy (%) comparison of two different LRMs on a subset of 100 test examples of I-RA VEN and I-RA VEN-X. 13"
  },
  {
    "id": "2404.12534v3",
    "title": "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean",
    "authors": "Peiyang Song, Kaiyu Yang, Anima Anandkumar",
    "published": "2024-04-18",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2404.12534v3",
    "pdf_url": "https://arxiv.org/pdf/2404.12534v3",
    "abstract": "Neural theorem proving combines large language models (LLMs) with proof assistants such\nas Lean, where the correctness of formal proofs can be rigorously verified, leaving no room for\nhallucination. With existing neural theorem provers pretrained on a fixed collection of data and\noffering valuable suggestions at times, it is challenging for them to continually prove novel theorems\nin a fully autonomous mode, where human insights may be critical. In this paper, we explore LLMs\nas copilots that assist humans in proving theorems. We introduce Lean Copilot, a general framework\nfor running LLM inference natively in Lean. It enables programmers to build various LLM-based\nproof automation tools that integrate seamlessly into the workflow of Lean users. Lean users can\nuse our pretrained models or bring their own ones that run either locally (with or without GPUs) or\non the cloud. Using Lean Copilot, we build LLM-based tools that suggest proof steps, complete\nproof goals, and select relevant premises. Experimental results on the Mathematics in Lean textbook\ndemonstrate the effectiveness of our method compared to existing rule-based proof automation in\nLean ( AESOP ), confirming the significance of having LLMs integrated into the theorem proving\nworkflow in Lean. When assisting humans, Lean Copilot requires only 2.08 manually-entered proof\nsteps on average (3.86 required by AESOP ); when automating the theorem proving process, Lean\nCopilot automates 74.2% proof steps on average, 85% better than AESOP (40.1%). We open source\nall code and artifacts under a permissive MIT license to facilitate further research.\nKeywords: Neural Theorem Proving, Proof Automation, Large Language Models, Neuro-Symbolic\nReasoning, AI for Mathematics\n1.",
    "introduction": "85% better than AESOP (40.1%). We open source all code and artifacts under a permissive MIT license to facilitate further research. Keywords: Neural Theorem Proving, Proof Automation, Large Language Models, Neuro-Symbolic Reasoning, AI for Mathematics 1. Introduction Neural theorem proving (Li et al., 2024) combines the strong learning capability of neural networks with the rigor of symbolic proof checking. The neural component trains LLMs to effectively generate formal proofs, yet LLMs\u2019 tendency to hallucinate (Huang et al., 2025a) prevents the generated proofs from guaranteed correct. The symbolic component uses proof assistants to verify proof correctness, but the interactive nature of proof assistants require substantial amount of human efforts to formalize proofs. Note that each component\u2019s weakness is naturally complemented by the other. Combining both, neural theorem proving trains LLMs to generate candidate proofs and verifies them by proof assistants, forming a powerful neuro-symbolic system for formal reasoning (Yang et al., 2024). On the symbolic side, Lean (de Moura et al., 2015) has been a widely used proof assistant especially in mathematical theorem proving, thanks to its large math library Mathlib4 (mathlib \u00a9 2025 P. Song, K. Yang & A. Anandkumar.arXiv:2404.12534v3 [cs.AI] 11 May 2025 SONG YANG ANANDKUMAR SUGGEST_T ACTICS Proof Goal SELECT_PREMISES SEARCH_PROOF LLMs Locally with CTranslate2 Server OR Lean Copilot Figure 1: Lean Copilot provides a general framework for running LLM inference in Lean, either locally via CTranslate2 or on a server. This framework enables creating various proof automation tools, including those for tactic suggestion, proof search, and premise selection. Community, 2020) with over 189K theorems1from diverse domains, which is still expanding. On the neural side, LeanDojo (Yang et al., 2023) novelly augments tactic generation with premise retrieval, and pairs tactic (individual proof step in Lean) generation with proof search to form complete proofs in a GPT- f(Ayers et al., 2023) style. Later works propose methods to further improve both tactic generation (Lin et al., 2024) and proof search (Huang et al., 2025b). An alternative approach is whole- proof generation (First et al., 2023), where synthetic data is used to address the data scarcity problem (Frieder et al., 2024). By scaling up, this approach has given birth to powerful proof-generation LLMs, marked by DeepSeek-Prover-v1.5 (Xin et al., 2024) and Goedel-Prover (Lin et al., 2025). All these existing LLM-based provers aim to prove theorems fully autonomously without human intervention. They wrap the proof assistant (e.g., Lean) into a gym-like (Brockman et al., 2016) environment. The model interacts with the proof environment and is evaluated by the number of test theorems it proves. The interaction happens solely on the backend server, without any collaboration with humans. While an autonomous AI mathematician is desirable in the long run, current LLMs often fail to prove theorems that are truly novel or challenging, especially those from a different domain than the training data (Zheng et al., 2022). This is in part because each branch of mathematics uses different lemmas and requires distinct intuitions, limiting the generalization of LLMs. However, with the development in Lean",
    "conclusion": "We have introduced Lean Copilot: a framework for running LLM inference natively in Lean.\nUsing Lean Copilot, we have built LLM-based proof automation tools for generating tactic sug-\ngestions ( SUGGEST _TACTICS ), searching for proofs ( SEARCH _PROOF ), and selecting premises\n(SELECT _PREMISES ). Lean Copilot also provides general interfaces between LLMs and Lean,\nallowing users to bring their own models and/or build other proof automation tools. Experimental\nresults on the Mathematics in Lean textbook demonstrate the effectiveness of our method compared\nto existing rule-based proof automation in Lean ( AESOP ). When assisting humans, Lean Copilot\nrequires only 2.08 manually-entered proof steps on average (3.86 required by AESOP ); when automat-\ning the theorem proving process, Lean Copilot automates 74.2% proof steps on average, 85% better\nthan AESOP (40.1%). These results confirm the benefit of integrating LLMs into the theorem proving\npipeline in Lean. We open source all code and artifacts to facilitate future research, and we hope to\nsee more LLM-based proof automation built upon Lean Copilot to help create more high-quality\nformal data, which would in turn enhance LLMs\u2019 capability in formal math."
  },
  {
    "id": "2504.17017v2",
    "title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification",
    "authors": "Balaji Rao, William Eiers, Carlo Lipizzi",
    "published": "2025-04-23",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2504.17017v2",
    "pdf_url": "https://arxiv.org/pdf/2504.17017v2",
    "abstract": "Formally verifying properties of software code has been a highly desirable task, especially with\nthe emergence of LLM-generated code. In the same vein, they provide an interesting avenue for\nthe exploration of formal verification and mechanistic interpretability. Since the",
    "introduction": "of powerful training regimes like Direct Preference Optimization\n(DPO) (Rafailov et al., 2023), Proximal Policy Optimization (PPO) (Schulman et al., 2017),\nand GRPO (Shao et al., 2024). RL frameworks for theorem proving model interactions between\nLLMs and generated proofs as a Markov Decision Process (MDP). The reward function is\ntypically designed around binary proof completion: assigning a reward of 1 if the proof is\nverified as correct and 0 otherwise (Dong et al., 2024). This binary reward system provides\nclear feedback for optimizing performance. Other approaches incorporate search algorithms into\nRL frameworks, such as Best-First Search (BFS) which guides proof generation by prioritizing\npromising paths based on heuristic evaluations (Yang et al., 2023); and Monte Carlo Tree\nSearch (MCTS) which explores potential proof paths systematically by balancing exploration\nand exploitation (Lample et al., 2022). RL-based approaches aim to improve both single-\npass and stepwise proof generation models. In single-pass methods, RL optimizes full-proof\ngeneration by rewarding logical consistency using Chain-of-Thought tokens and verification\nsuccess. In stepwise methods, RL enhances tactic prediction by refining intermediate steps based\non feedback from symbolic verifiers. Recent works such as DeepSeekMath (Xin et al., 2024)\ndemonstrate that reinforcement learning can significantly enhance models\u2019 reasoning abilities\nby improving their capacity to generate coherent and verifiable proofs over time.\n3. Method\nBuilding on the work of the workflow described in DSP (Jiang et al., 2022a) and the proof\nconstruction method in ProofAug (Liu et al., 2025), in this section we present our framework\n3\nRao Eiers Lipizzi\nFigure 1: The two core components within theProofSeekframework: (a) the fine-tuning\nlanguage model module, (b) the proof generation and verification module\nProofSeekthat leverages the strengths of proof-step and whole-proof generation, as well as the\nnatural language generation paradigms of LLMs.ProofSeekconsists of two core components:\na component for fine-tuning a language model using SFT and RL, and a proof generation and\nverification component for generating and building the formal proofs. Both components of\nProofSeekare shown in Figure 1.Our framework is generalizable across domains\nwhere the input is a mathematical statement, policy code, or natural language\nstatement, and the output is a verified proof state or a failure state. The details of\nProofSeekis shown in Algorithm 1. We first fine-tune a whole-proof generation model using\nour two-stage approach. Then, we build a formal statement that represents the provided policy\ncode or mathematical statement. Finally, we leverage the fine-grained proof structure analysis",
    "conclusion": "24policy_allows ec2 _instance_policy RunInstances Images\u2227 25policy_allows ec2 _instance_policy RunInstances Instances\u2227 26policy_allows ec2 _instance_policy RunInstances Volumes\u2227 27policy_allows ec2 _instance_policy RunInstances NetworkInterfaces\u2227 28policy_allows ec2 _instance_policy RunInstances KeyPairs \" 29oops Listing 6: Initial Proof Attempt 1(* Proof of the theorem *) 2(* 3proof - 4have \" policy_allows ec2 _instance_policy RunInstances AllResources \" 5by ( simp add: ec2 _instance_policy_def ) 6moreover have \" policy_allows ec2 _instance_policy RunInstances Images \" 7by ( simp add: ec2 _instance_policy_def ) 8moreover have \" policy_allows ec2 _instance_policy RunInstances Instances \" 9by ( simp add: ec2 _instance_policy_def ) 10moreover have \" policy_allows ec2 _instance_policy RunInstances Volumes \" 11by ( simp add: ec2 _instance_policy_def ) 12moreover have \" policy_allows ec2 _instance_policy RunInstances NetworkInterfaces \" 13by ( simp add: ec2 _instance_policy_def ) 14moreover have \" policy_allows ec2 _instance_policy RunInstances KeyPairs \" 15by ( simp add: ec2 _instance_policy_def ) 16ultimately show ? thesis by simp 17qed 18*) Listing 7: Sorry Proof (Commented) 1(* Proof of the theorem *) 2(* 3proof - 4have \" policy_allows ec2 _instance_policy RunInstances AllResources \" 5by ( simp add: ec2 _instance_policy_def ) 6moreover have \" policy_allows ec2 _instance_policy RunInstances Images \" 7by ( simp add: ec2 _instance_policy_def ) 8moreover have \" policy_allows ec2 _instance_policy RunInstances Instances \" 9by ( simp add: ec2 _instance_policy_def ) 10moreover have \" policy_allows ec2 _instance_policy RunInstances Volumes \" 11by ( simp add: ec2 _instance_policy_def ) 12moreover have \" policy_allows ec2 _instance_policy RunInstances NetworkInterfaces \" 13by ( simp add: ec2 _instance_policy_def ) 14moreover have \" policy_allows ec2 _instance_policy RunInstances KeyPairs \" 15by ( simp add: ec2 _instance_policy_def ) 16ultimately show ? thesis by simp 17qed 15 Rao Eiers Lipizzi 18*) Listing 8: State Information 1{ 2\" success \": true , 3\" i_try \": 0, 4\" success_stage \": \" init_proof \", 5\" has_timeout \": false , 6\" extra_calls \": 0, 7\" has_sc \": false 8} 16"
  },
  {
    "id": "2505.05758v5",
    "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning",
    "authors": "Azim Ospanov, Farzan Farnia, Roozbeh Yousefzadeh",
    "published": "2025-05-09",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2505.05758v5",
    "pdf_url": "https://arxiv.org/pdf/2505.05758v5",
    "abstract": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning Azim Ospanov\u2217\u2020 aospanov9@cse.cuhk.edu.hkFarzan Farnia\u2020 farnia@cse.cuhk.edu.hkRoozbeh Yousefzadeh\u2217 roozbeh.yz@gmail.com Abstract Formal reasoning and automated theorem proving constitute a challenging subfield of machine learning, in which machines are tasked with proving mathematical theorems using formal languages like Lean. A formal verification system can check whether a formal proof is correct or not almost instantaneously, but generating a completely correct formal proof with large language models (LLMs) remains a formidable task. The usual approach in the literature is to prompt the LLM many times (up to several thousands) until one of the generated proofs passes the verifi- cation system. In this work, we present APOLLO (AutomatedPrOof repair via LLM andLean cOllaboration), a modular, model -agnostic agentic framework that combines the strengths of the Lean compiler with an LLM\u2019s reasoning abilities to achieve better proof-generation results at a low token and sampling budgets.Apollo directs a fully automated process in which the LLM generates proofs for theorems, a set of agents analyze the proofs, fix the syntax errors, identify the mistakes in the proofs using Lean, isolate failing sub -lemmas, utilize automated solvers, and invoke an LLM on each remaining goal with a low top -Kbudget. The repaired sub -proofs are recombined and reverified, iterating up to a user -controlled maximum number of attempts. On the miniF2F benchmark, we establish a new state -of-the-art accuracy of 84.9% among sub 8B -parameter models (as of August 2025) while keeping the sampling budget below one hundred. Moreover,Apolloraises the state -of-the-art accuracy for Goedel -Prover -SFT to 65.6% while cutting sample complexity from 25,600 to a few hundred. General -purpose models (o3 -mini, o4 -mini) jump from 3\u20137% to over 40% accuracy. Our results demonstrate that targeted, compiler -guided repair of LLM outputs yields dramatic gains in both efficiency",
    "introduction": "and correctness, sug- gesting a general paradigm for scalable automated theorem proving. The codebase is available athttps://github.com/aziksh-ospanov/APOLLO 1 Introduction Formal reasoning has emerged as one of the most challenging fields of AI with recent achievements such as AlphaProof and AlphaGeometry doing well at the International Math Olympiad competing with humans [ 1,2,3]. Formal reasoning relies both on AI models and a formal verification system that can automatically verify whether a mathematical proof is correct or not. In recent years, formal verification systems such as Lean [ 4] have facilitated a new form of doing mathematical research by enabling mathematicians to interact with the formal verification system and also with each other via the system, enabling larger numbers of mathematicians to collaborate with each other on a single project. As such, these formal verification systems are also called proof assistants as one can use them interactively to write a formal proof and instantaneously observe the current state of the proof and any possible errors or shortcomings in the proof generated by the compiler. Immediate access \u2217Huawei Hong Kong Research Center \u2020Department of Computer Science & Engineering, The Chinese University of Hong Kong 39th Conference on Neural Information Processing Systems (NeurIPS 2025).arXiv:2505.05758v5 [cs.AI] 4 Nov 2025 Common Approach: Whole-Proof Generation Pipeline repeat up to K times LLM Lean Server proof attemptexit loop continue LLM Lean Server proof attempt(s) sub-problem(s) to proveproof state compilation errors syntax errorsApollo Proof Repair Agent Subproof Extractor Auto Solverrepeat up to r times exit loop continueOur Proposed Apollo PipelineFigure 1: The summary of whole-proof generation pipeline vs. proposed Apollo agentic pipeline. LLM refers to a chosen formal theorem generator model. to the output of Lean compiler can help a mathematician to fix possible errors in the proof. At the same time, when a proof passes the compiler with no error, other collaborators do not need to verify the correctness of the proof. This type of collaboration is transforming the field of mathematics enabling large groups of collaborators to engage in large projects of mathematical research such as the Prime Number Theorem And More project [ 5]. Moreover, it has given rise to the digitization of mathematics [6]. In the AI front, large language models (LLMs) are improving at mathematical reasoning in natural language, and at the same time, they have shown a remarkable ability to learn the language of those formal verification systems and write mathematical proofs in formal language. This has led to the field of Automated Theorem Proving (ATP) where the standard approach is to prompt the LLM to generate a number of candidate proofs for a given theorem which will then be verified automatically using the formal verification system. Better models, better training sets, and better training methods has led to significant advances in this field [7, 8, 9, 10, 11]. Despite these advances, the LLMs do not really get the chance to interact with the verification system the way a human does. LLMs generate many possible proofs, sometimes as many as tens of thousands,",
    "conclusion": ".\nDependence on base model\u2019s proof sketch quality.Apollo\u2019s ability to produce a correct formal\nproof largely depends on the base model and whether its initial proof has a coherent proof sketch. As\n[44] observe, models often tend to \u201ccut corners,\u201d producing proofs that are much shorter than the\nrigorous, multi-step arguments required to generate a valid proof. When a base model fails to write a\ncorrect proof strategy (e.g. omitting critical lemmas or suggesting irrelevant tactics, or taking a wrong\napproach), the Apollo is less likely to repair such a proof. Enhancing the robustness of Apollo to very\npoor initial sketches remains an open challenge.\n6 Conclusion\nIn this work, we present Apollo, a novel, modular fully automated agentic system that combines syntax\ncleaning, automatic solvers, and LLM -driven sub -proof generation to transform an LLM\u2019s initial\nproof sketch into a fully verified Lean4 proof. This framework harnesses the full power of the Lean\ncompiler and integrated solvers, merging them with LLM systems. Applied across five whole -proof\ngeneration models, ranging from general -purpose LLMs (o3 -mini, o4 -mini) to specialized provers\n(Kimina -Prover -Preview -Distill -7B, Goedel -Prover -SFT, Goedel-V2), Apollo consistently establishes\nnew best accuracies on the miniF2F benchmark while reducing token and sampling budgets by one\nto two orders of magnitude. Our empirical analysis shows that Apollo not only raises overall proof\nsuccess rates but also guides models to generate longer, more structured proofs, as evidenced by a\ndrastic increase in average successful proof length. We further demonstrate how the recursion -depth\nparameter rtrades off sample complexity against accuracy, achieving robust gains with only a few\nhundred samples. We believe that Apollo\u2019s collaboration between LLMs, automatic solvers, and the\nLean compiler defines a paradigm of agentic systems that produce high -quality, verifiable proofs for\nincreasingly challenging theorems.\n10"
  },
  {
    "id": "2310.03731v1",
    "title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
    "authors": "Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li",
    "published": "2023-10-05",
    "category": "cs.CL",
    "arxiv_url": "http://arxiv.org/abs/2310.03731v1",
    "pdf_url": "https://arxiv.org/pdf/2310.03731v1",
    "abstract": "MATHCODER : S EAMLESS CODE INTEGRATION IN LLM S FOR ENHANCED MATHEMATICAL REASONING Ke Wang1\u2217Houxing Ren1\u2217Aojun Zhou1\u2217Zimu Lu1\u2217Sichun Luo3\u2217 Weikang Shi1\u2217Renrui Zhang1Linqi Song3Mingjie Zhan1\u2020Hongsheng Li1,2\u2021 1Multimedia Laboratory (MMLab), The Chinese University of Hong Kong 2Shanghai Artificial Intelligence Laboratory3City University of Hong Kong {wangk.gm, renhouxing, aojunzhou, zmjdll}@gmail.com hsli@ee.cuhk.edu.hk ABSTRACT The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language ,code , and execution results . We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/ MathCoder . 1 I NTRODUCTION Recently, closed-source large language models (LLMs) such as GPT-4 (OpenAI, 2023) and PaLM- 2 (Anil et al., 2023), paired with methods such as Chain-of-Thought (CoT) (Wei et al., 2022) and Program-Aided Language models (PAL) (Gao et al., 2023), have shown remarkable performance on mathematical reasoning tasks. In contrast, current open-source LLMs (Touvron et al., 2023; Penedo et al., 2023; Zhang et al.,",
    "introduction": "2022) still lag significantly behind in this area. Even Llama-2- 70B (Touvron et al., 2023), one of the most potent open-source models, only scores 56.8% and 13.5% respectively on GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets, remarkably lower than GPT-4 Code Interpreter1, which scores 97% and 69.7% (Zhou et al., 2023a). To narrow the gap between open-source and closed-source models in math problem solving, recent works, such as the WizardMath (Luo et al., 2023) and RFT (Yuan et al., 2023), have tried to tune open-source models with math problems and CoT solutions, achieving a significant gain in performance compared to their base model, Llama-2. On the other hand, methods such as PAL (Gao et al., 2023), PoT (Chen et al., 2022), and CSV (Zhou et al., 2023a) encourage code usage in solving \u2217Equal contribution. \u2020Project leader. \u2021Corresponding author. 1https://openai.com/blog/chatgpt-plugins#code-interpreter 1arXiv:2310.03731v1 [cs.CL] 5 Oct 2023 Table 1: Comparison with different Instruction-following datasets. The baseline datasets include recent RFT-u13b (Yuan et al., 2023) and WizardMath (Luo et al., 2023). Datasets Seed Annotation RFT-100k GSM8K Llama WizardMath-96k GSM8K+MATH GPT-4 Ours-49k GSM8K+MATH GPT-4 Ours-80k GSM8K+MATHGPT-4 + Self-distillation 7B 70B1020304050607080Accuracy 17.824.846.250.773.1MathCoder WizardMath (SoTA) Llama-1 RFTFigure 1: Performance comparison between MathCoder, WizardMath, and Llama-1 RFT models with different model sizes. math problems, showing promising improvements when paired with closed-source models like GPT- 3.5, GPT-4 and GPT-4 Code Interpreter. In particular, GPT-4 Code Interpreter surpasses the previous SOTA by a clear margin. Recent study (Zhou et al., 2023a) shows that this excellent performance can be attributed to its ability to generate and assess the execution results of a chain of code interlaced with natural language reasoning steps. However, existing open-source models fail to benefit from this sophisticated mechanism since they lag behind closed-source models in both code generation and natural language reasoning. Therefore, we still lack an effective recipe to deliver open-source models to solve math problems in a manner similar to GPT-4 Code Interpreter . In this paper, leveraging the strengths of GPT-4 Code Interpreter (Zhou et al., 2023a), we introduce a simple yet effective framework, MathCoder , designed to enhance the mathematical reasoning capabilities of open-source models. This framework can be categorized into two parts: (1) math instruction-following dataset construction and (2) customized supervised fine-tuning. Specifically , the instruction-following dataset, termed as MathCodeInstruct , consists exclusively of 80k math problems and their corresponding solutions. Each solution is interwoven with natural language for reasoning ,code for execution , and execution results . The comparison between MathCodeInstruct and other math instruction-tuning datasets is shown in Tab. 1. MathCodeInstruct is created in two steps. The first step is collecting GPT-4 Code Interpreter- style solutions for the GSM8K and MATH training sets. GSM8K and MATH are two important datasets of math problems for improving and evaluating models\u2019 mathematical abilities, which consist of grade school math word problems and challenging competition mathematics problems, respectively. Using this data, we trained our initial models, termed MathCoder-Initial . The second step is to augment more math problems by",
    "conclusion": "E m3, n3 = 2, 1 # ratio for point F # Using section formula to find coordinates of D, E, F D = ((m1 *B[0] + n1 *A[0]) / (m1 + n1), (m1 *B[1] + n1 *A[1]) / (m1 + n1)) E = ((m2 *C[0] + n2 *B[0]) / (m2 + n2), (m2 *C[1] + n2 *B[1]) / (m2 + n2)) F = ((m3 *A[0] + n3 *C[0]) / (m3 + n3), (m3 *A[1] + n3 *C[1]) / (m3 + n3)) D, E, F <|endofblock |> <|execution |> ((6.0, 4.0), (7.0, 10.0), (2.0, 4.0)) < |endofblock |> <|text|> The coordinates of the points are as follows: - Point Dis located at (6,4) - Point Eis located at (7,10) - Point Fis located at (2,4) Now, let\u2019s proceed to find the areas of triangles ABC andDEF using the formula for the area of a triangle, and then find the ratio of these areas. <|endofblock |> <|code|> # Function to find the area of a triangle given its vertices def triangle_area(v1, v2, v3): return 0.5 *abs(v1[0] *(v2[1] - v3[1]) + v2[0] *(v3[1] - v1[1]) + v3[0] *(v1[1] - v2[1])) # Areas of triangles ABC and DEF area_ABC = triangle_area(A, B, C) area_DEF = triangle_area(D, E, F) # Ratio of areas ratio_areas = area_DEF / area_ABC area_ABC, area_DEF, ratio_areas <|endofblock |> <|execution |> (36.0, 12.0, 0.3333333333333333) < |endofblock |> <|text|> So, the ratio of the area of triangle DEF to the area of triangle ABC is1 3\u2713or0.3333 . <|endofblock |><|endofmessage |> Figure 23: Example of LCE solution with special token. The problem is from the test set of MATH in geometry with id 500. In contrast to CoT, which consists solely of natural language, and PoT, which consists solely of code, our LCE solution intertwines natural language, code, and execution results. 32"
  },
  {
    "id": "2410.16429v2",
    "title": "Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4",
    "authors": "Leni Aniva, Chuyue Sun, Brando Miranda, Clark Barrett, Sanmi Koyejo",
    "published": "2024-10-21",
    "category": "cs.LO",
    "arxiv_url": "http://arxiv.org/abs/2410.16429v2",
    "pdf_url": "https://arxiv.org/pdf/2410.16429v2",
    "abstract": ". Machine-assisted theorem proving refers to the process of\nconducting structured reasoning to automatically generate proofs for\nmathematical theorems. Recently, there has been a surge of interest in\nusing machine learning models in conjunction with proof assistants to\nperform this task. In this paper, we introduce Pantograph, a tool that\nprovides a versatile interface to the Lean 4 proof assistant and enables\nefficient proof search via powerful search algorithms such as Monte Carlo\nTree Search. In addition, Pantograph enables high-level reasoning by\nenabling a more robust handling of Lean 4\u2019s inference steps. We provide\nan overview of Pantograph\u2019s architecture and features. We also report on\nan illustrative use case: using machine learning models and proof sketches\nto prove Lean 4 theorems. Pantograph\u2019s innovative features pave the way\nfor more advanced machine learning models to perform complex proof\nsearches and high-level reasoning, equipping future researchers to design\nmore versatile and powerful theorem provers.\n1",
    "introduction": "machine learning to automatically search for proofs in proof assistants (e.g., [ 21], [22], [5], [6], [12], [11], [8], [9], [20]). While these efforts have produced promising results, many proofs are still beyond the reach of machine learning-based automation. In order to continue to make progress in this area, several challenges need to be addressed. One of these challenges is the need for better interfaces betweenarXiv:2410.16429v2 [cs.LO] 31 Jan 2025 proof assistants and machine learning systems. In this paper, we introduce Pan- tograph ,1an API and Read-Eval-Print Loop (REPL) for Lean 4, whose primary goal is to provide a convenient interface for training and evaluating theorem proving agents. The name \u201cPantograph\u201d alludes to the process of recording a proof during proof search.2 The main motivation for creating Pantograph is to overcome the limitations of the interface provided by the Lean 4 Language Server Protocol (LSP), which is the standard interface provided for interactive use by a human user. Although the LSP provides interactive feedback for a human operator of Lean 4, it suffers from a number of problems as a machine interface. The LSP interface requires its user to keep track of positions of a cursor in text, and a machine user would be burdened with tracking these redundant data. Moreover, there is no straightforward way to extract tactic training data from the LSP interface or sketch out a proof to be finished by automation tactics. In contrast, Pantograph is designed from the ground up as an efficient and convenient interface for machine (and especially machine learning) agents. The main contributions of Pantograph are: 1.Unlike prior work, the user can decide to solve goals independently. This enables more powerful search algorithms such as Monte Carlo Tree Search (MCTS), which have been successful in other domains (e.g., AlphaGo and AlphaZero [ 17,18]), achieving superhuman performance on complex games like Go, Chess, and Shogi.3To do this, Pantograph handles metavariable coupling, which is a phenomenon that complicates tree search [13]. 2.In contrast to prior work in Lean 4 [ 23], Pantograph supports the use of the advanced reasoning steps (called tactics) have,let,conv, and calc. These tactics are crucial for supporting high-level reasoning strategies like proof sketching [8]. 3.Pantograph fully supports essential data extraction tasks (e.g., it can extract the before- and after-goal states of tactic executions, which are usually not available in raw Lean 4 scripts). In addition, Pantograph introduces several novel data extraction capabilities, including the ability to extract entire proof scripts with associated comments, which can be used for tasks like autoformalization, and the important ability to extract proof representations as programs, which allows for one-shot prediction of proofs. 4.Pantograph provides feedback from partially executed convand calctactics, which was not possible in preceding works. 5.Pantograph allows the user to resume an incomplete proof containing the sorrykeyword in Lean 4. This is useful for machine learning models which produce a proof draft before resolving the details in the proofs. 1https://github.com/stanford-centaur/PyPantograph 2A Pantograph is a mechanism for recording the movement of a pen",
    "conclusion": ".\n6 Conclusion\nIn this work, we introduce Pantograph, a Machine-to-Machine interaction library\nfor Lean 4. We compare its features against existing tools used for training\nmachine learning models for theorem proving, and we provide a list of its novel\nfeatures. We also illustrate an application by implementing the first Lean 4\nimplementation of the Draft-Sketch-Prove approach.\nIn future work, we plan to use Pantograph to build and train various machine\nlearning approaches for theorem proving. We also expect and hope that others\nwill use it in interesting and novel ways and that these use cases will provide\nfeedback for additional improvements and extensions of Pantograph.\nOur evaluation also demonstrates one way that formal tools like Lean can be\nused to address potential harm from Language Models such as the one used in\nthe evaluation section. Language models, though powerful, still face the problem\nof hallucination and generation of illogical results. These can be mitigated by\napplying formal techniques to the results produced of language models. The\ndraft-sketch-prove experiment is an instance of this general idea, where proof\nautomation formally checks the potentially incorrect result generated by an LLM.\nIn the future, Pantograph could be used for other hybrid reasoning approaches\ncombining generative AI and formal reasoning."
  },
  {
    "id": "2506.17104v1",
    "title": "Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving",
    "authors": "Chuxue Cao, Mengze Li, Juntao Dai, Jinluan Yang, Zijian Zhao, Shengyu Zhang, Weijie Shi, Chengzhong Liu, Sirui Han, Yike Guo",
    "published": "2025-06-20",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2506.17104v1",
    "pdf_url": "https://arxiv.org/pdf/2506.17104v1",
    "abstract": "Large language models (LLMs) have shown\npromising first-order logic (FOL) reasoning\ncapabilities with applications in various areas.\nHowever, their effectiveness in complex mathe-\nmatical reasoning involving multi-step FOL de-\nductions is still under-researched. While LLMs\nperform competitively on established mathe-\nmatical reasoning benchmarks, they struggle\nwith multi-step FOL tasks, as demonstrated by\nDeepseek-Prover-V2-7B\u2019s low accuracy (4.2%)\non our proposed theorem proving dataset. This\nissue arises from the limited exploration of di-\nverse proof strategies and the potential for early\nreasoning mistakes to undermine entire proofs.\nTo address these issues, we propose DREAM, a\nself-adaptive solution that enhances the Diver-\nsity and REAsonability of LLMs\u2019 generation\nstrategies. DREAM incorporates an Axiom-\nDriven Strategy Diversification mechanism to\npromote varied strategic outcomes and a Sub-\nProposition Error Feedback to help LLMs re-\nflect on and correct their proofs. Our contri-\nbutions include pioneering advancements in\nLLMs\u2019 mathematical reasoning through FOL\ntheorem proving, introducing a novel inference\nstage solution that improves performance by\n0.6% to 6.4%, and providing a curated dataset\nof 447 mathematical theorems in Lean 4 format\nfor evaluation.\n1",
    "introduction": "2: Sub-proposition Error Feedback Axiom 1 ... Strategy generation prompt Strategy Sub-pr oposition 1: Assume member A aI_bDa and derive a contradiction - Infer ence rules used: Proof by contradiction - Axioms used: member_of_intersection_is_member_of_set1,.. Conjectur e: theorem prove_aI_bDa_is_empty {A : \u03b1} (a b : \u03b1) : \u00ac HasMember .member A (aI_bDa a b) := sorry Axiom 2 Axiom m Axiom Set 1 Axiom Set n LLM invocations ... Randomly selected Axiom Set Prove prompt 1 Strategy Formal Pr oof theorem prove_aI_bDa_is_empty {A : \u03b1} (h_dif f : difference b a bDa) (h_no_inter : \u00ac intersection a bDa aI_bDa) : \u00ac member A aI_bDa := by by_contra h_mem -- Proof by contradiction... Error messages Sub-proposition identified Proof Sub-pr oposition Err or Feedback Sub-pr oposition 1 /- Show that assuming menmbership in aIbDa leads to a contradiction -/ ... /- Error messages -/ ... Analyze prompt Sub-proposition error feedback from previous attempts Error Analysis 1. Err or Patterns: - Common mistakes identified - Root causes of errors - Typical misunderstandings 2. Strategic Recommendations - Propose specific proof strategies - Suggest alternative tactics - Outline required corrections Formal compiler Step1: Strategy Generation Step3: Sub-proposition Error Feedback Step2: Theorem Proving Axiom 1 ... Strategy generation prompt Strategy Sub-proposition 1: Assume member A aI_bDa and derive a contradiction - Inference rules used: Proof by contradiction - Axioms used: member_of_intersection_is_member_of_set1,.. Conjecture: theorem prove_aI_bDa_is_empty {A : \u03b1 } (a b : \u03b1 ) : \u00ac HasMember.member A (aI_bDa a b) := sorry Axiom selection prompt Axiom 2 Axiom m Axiom Set 1 Axiom Set n LLM invocations ... Randomly selected Axiom Set Prove prompt Strategy Formal Proof theorem prove_aI_bDa_is_empty {A : \u03b1 } (h_di\ufb00 : di\ufb00erence b a bDa) (h_no_inter : \u00ac intersection a bDa aI_bDa) : \u00ac member A aI_bDa := by by_contra h_mem -- Proof by contradiction... Error messages error: unsolved goals\\n \u03b1 Type\\nb a bDa aI_bDa : \u03b1 \u2192 Prop\\nA : \u03b1 \\nh_di\ufb00 : di\ufb00erence b a bDa\\nh_no_inter : ... Sub-proposition identified Proof Error messages Sub-proposition Error Feedback Sub-proposition 1 /- Show that assuming menmbership in aIbDa leads to a contradiction -/ ... /- Error messages -/ ... Analyze prompt Sub-proposition error feedback from previous a\ue039empts Error Analysis 1. Error Pa\ue039erns: - Common mistakes identified - Root causes of errors - Typical misunderstandings 2. Strategic Recommendations - Propose specific proof strategies - Suggest alternative tactics - Outline required corrections Prove Prompt Alternative Strategy Formal compiler Prove prompt 2 Axiom selection prompt Sub-proposition labeling prompt Axioms and Conjectur e in TPTP Axioms and Conjectur e in Lean 4 Formal Compiler Wrong Step 2: Post-pr ocessing variable (Point : Type) (Line : Type) variable (distinct_points : Point \u2192 Point \u2192 Prop)... axiom apart1 : \u2200 (X : Point), \u00ac distinct_points X X axiom apart4 : \u2200 (X Y Z : Point), distinct_points X Y \u2192 (distinct_points X Z \u2228 distinct_points Y Z)... theorem conjecture : \u2200 (X Y Z : Point), distinct_points X Y \u2192 apart_point_and_line Z (line_connecting X Y) \u2192 apart_point_and_line X (line_connecting Z Y)",
    "conclusion": "multiplicative_identity '-/ --Sub-proposition 2: Multiplication is Well -Defined have h\u2082 := well_definedness_of_multiplication multiplicative_identity a have h\u2083 := well_definedness_of_multiplication multiplicative_identity b --Sub-proposition 3: Multiplication is Compatible with Equality have h\u2084 := compatibility_of_equality_and_multiplication a multiplicative_identity b have h\u2085 := compatibility_of_equality_and_multiplication d multiplicative_identity b --Sub-proposition 4: `a`is Defined have h\u2086 := a_equals_d have h\u2087 := a_is_defined have h\u2088 := b_is_defined have h\u2089 := d_is_defined intro h simp_all <;> aesopFigure 16: An illustrative example of cascading error. System prompt : Your task is to convert TPTP format axioms and conjectures into Lean 4 format . Follow these guidelines : 1. Type Declarations : - Declare all necessary types using `Type ` - Define type variables when needed using uppercase letters (e.g., `A`,`B`) 2. Axiom Conversion : - Convert each TPTP axiom into a complete Lean 4 definition - Use appropriate Lean 4 syntax for logical operators : - Do not use `sorry `in axiom definitions 3. Conjecture Conversion : - Convert the conjecture into a theorem statement - Use `theorem `for the declaration - End the theorem with `sorry ` - Do not provide the proof 4. Code Format : - Wrap all Lean 4 code with ```lean ``` markers - Use proper indentation - Include necessary imports - Add brief comments explaining complex translations 5. Variable Handling : - Declare all variables with appropriate types - Maintain consistent variable naming between axioms and conjecture - Use meaningful variable names when possible Please ensure each conversion preserves the original logical meaning while following Lean 4 's syntax and type system . User prompt : Input TPTP Format : Axioms : { axioms } Conjecture : { conjecture } Please provide the Lean 4 conversion following the guidelines above . Figure 17: Prompts for converting first-order axioms and conjectures from TPTP format to Lean4 format."
  },
  {
    "id": "2511.06522v1",
    "title": "FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis",
    "authors": "Jan Ondras, Marek \u0160uppa",
    "published": "2025-11-09",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2511.06522v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06522v1",
    "abstract": "Mathematical reasoning requires abstracting symbolic rules from visual patterns\u2014\ninferring the infinite from the finite. We investigate whether multimodal AI systems\npossess this capability throughFractalBench, a benchmark evaluating fractal pro-\ngram synthesis from images. Fractals provide ideal test cases: Iterated Function\nSystems with only a few contraction maps generate complex self-similar patterns\nthrough simple recursive rules, requiring models to bridge visual perception with\nmathematical abstraction. We evaluate four leading MLLMs\u2014GPT-4o, Claude 3.7\nSonnet, Gemini 2.5 Flash, and Qwen 2.5-VL\u2014on 12 canonical fractals. Models\nmust generate executable Python code reproducing the fractal, enabling objective\nevaluation. Results reveal a striking disconnect: 76% generate syntactically valid\ncode but only 4% capture mathematical structure. Success varies systematically\u2014\nmodels handle geometric transformations (Koch curves: 17-21%) but fail at branch-\ning recursion (trees: <2%), revealing fundamental gaps in mathematical abstraction.\nFractalBenchprovides a contamination-resistant diagnostic for visual-mathematical\nreasoning and is available at https://github.com/NaiveNeuron/FractalBench\nCantor Set Cantor Dust Koch Curve Koch Snowflake\nSierpi ski Gasket\n Sierpi ski Carpet\n Sierpi ski Pentagon\n Heighway Dragon\nL\u00e9vy Dragon McWorter Pentigree Pythagoras Tree Symmetric Binary Tree\nFigure 1: Twelve canonical fractals testing different mathematical reasoning capabilities: linear\nrecursion (Cantor), geometric transformations (Koch), multi-scale self-similarity (Sierpi \u00b4nski), space-\nfilling curves (dragons), and branching recursion (trees). All defined via Iterated Function Systems.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: MATH-AI.arXiv:2511.06522v1  [cs.AI]  9 Nov 2025\n1",
    "introduction": "compactly defined by an Iterated Function System (IFS) [ 1,2]\u2014typically 2-8 contraction mappings\u2014yet generates arbitrarily complex patterns through recursive self-similarity. Successfully synthesizing fractal code demands three interconnected capabil- ities: recognizing scale invariance across recursive levels, inferring precise geometric transformations from visual evidence, and achieving recursive abstraction\u2014understanding the generative process rather than enumerating visible patterns. We introduceFractalBench, comprising 12 canonical fractals spanning distinct challenges: Koch curves test geometric transformations, Sierpi \u00b4nski structures probe multi-scale self-similarity, dragon curves evaluate space-filling navigation, and tree fractals assess branching recursion. This diversity enables systematic diagnosis: which mathematical capabilities do current models possess, and where do they fail? Evaluating four leading MLLMs on 7,320 fractal images (610 unique test images across 12 model- prompt combinations) reveals a striking disconnect: 76% execution success but only 4% visual correctness. Koch curves achieve 17-21% accuracy, Sierpi \u00b4nski fractals 3-18%, while tree fractals fail catastrophically at <2%, revealing models can compose local operations but lack recursive abstraction. This work makes three contributions to understanding visual-mathematical reasoning in AI systems. First, we establish a diagnostic framework connecting fractal synthesis to specific mathematical reasoning requirements, enabling systematic capability assessment. Second, we provide empirical evidence that current MLLMs possess geometric capabilities but fundamentally lack recursive abstraction\u2014findings with implications for mathematical AI beyond fractals. Third, we demonstrate contamination-resistant evaluation through parameterizable complexity, offering a methodology applicable to future benchmarking efforts. 2 Related Work Existing benchmarks reveal gaps in visual-mathematical reasoning.TurtleBench[ 3] achieves only 19% accuracy on simple geometric shapes, testinggeometric perception.MathVista[ 4] andMATH- Vision[ 5] evaluate mathematical problem-solving with visual contexts, whileMATHGLANCE[ 6] reveals models\u201cdo not know where to look\u201din mathematical diagrams.GeoGramBench[ 7] targets geometric program reasoning, showing performance degradation with structural complexity. These benchmarks primarily testapplyingmathematical knowledge to solve visual problems. FractalBenchtestsmathematical abstraction\u2014inferring recursive generative rules from self-similar patterns. WhereTurtleBench[ 3] asks \u201ccan you draw what you see?\u201d,FractalBenchasks \u201ccan you infer the infinite process generating finite observations?\u201d This capability\u2014abstracting symbolic rules from visual examples\u2014is central to mathematical discovery and reasoning. Fractals uniquely target this gap through recursive self-similarity, precise geometric transformations, and objective pixel-perfect evaluation.FractalBenchis not a general vision or code benchmark, but rather a targeted diagnostic of visual-mathematical abstraction. Notably, the same difficulty patterns we observe (e.g., failures on branching recursion) appear in broader benchmarks such asGeoGramBench[ 7], MathVista[ 4], andMATHGLANCE[ 6], indicating that the limitations we expose are conceptual rather than API-specific. See App. A for a comprehensive survey. 3 FractalBench 3.1 Fractal Definitions via Iterated Function Systems Self-similar fractals are defined as attractors of contractive Iterated Function Systems (IFS). Given contraction mappings f1, . . . , f m:Rd\u2192Rd, the IFS attractor is the unique compact set K satisfying K=Sm i=1fi(K). We consider 12 classic fractals [ 1,2] (Fig. 1), spanning Cantor sets, Koch curves, Sierpi \u00b4nski structures, dragon curves, and tree fractals. Each has contraction ratio r\u2208(0,1) determining scale reduction per iteration. Complete definitions are provided in App. B, and parameters in App. C. 2 3.2 Benchmark Design and Mathematical Reasoning Requirements FractalBenchcomprises 610 images ( 1,024\u00d71,024 pixels, 4-12",
    "conclusion": "specific geometric or recursive properties models capture versus miss. Such metrics would distinguish, for example, between code that produces visually similar output through incorrect means (e.g., iterative approximation) versus code that implements the correct generative process with minor visual artifacts. K.2 Benchmark Scope Program Synthesis Baselines.FractalBenchis designed as a diagnostic benchmark for current multimodal large language models (MLLMs), focusing on vision-to-code reasoning rather than full program synthesis pipelines. We therefore do not include comparisons with traditional program synthesis baselines such as symbolic search, neurosymbolic inference, and constraint-based syn- thesis [ 15,16]. Such comparisons would, however, provide useful performance context, situating MLLM capabilities relative to specialized synthesis techniques. Model Coverage.Our evaluation covers four representative MLLMs available at the time of benchmark development (GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL). More recent reasoning-specialized models (e.g., OpenAI o1, DeepSeek-R1) or math-focused models are not included. Evaluating such models would help determine whetherFractalBenchdistinguishes genuinely stronger reasoning capabilities or primarily exposes universal failure modes in visual- mathematical abstraction. Expanding coverage to these systems would sharpen the benchmark\u2019s diagnostic scope. K.3 Analysis Limitations Observational Findings.The observed differences in prompting strategies\u2014where direct code generation unexpectedly outperforms reasoning-first prompts (Sec. 4.2)\u2014remain observational. We do not perform stepwise ablations to isolate whether the performance gap stems from prompt complexity overload, genuine incompatibility between verbal reasoning and geometric precision, or other factors. Controlled experiments varying prompt length, reasoning depth, and instruction complexity independently would enable causal interpretation beyond our current hypotheses. Model Improvement Feedback Loop.WhileFractalBenchsystematically identifies characteristic failure modes in recursive and branching reasoning, we do not demonstrate how these diagnostics could guide targeted model improvements through few-shot tuning, structured prompting refinements, or tool integration. Establishing such a feedback loop\u2014where benchmark insights lead to measurable capability gains\u2014would further validateFractalBenchas a tool for advancing visual\u2013mathematical reasoning research. 25"
  },
  {
    "id": "2412.16075v1",
    "title": "Formal Mathematical Reasoning: A New Frontier in AI",
    "authors": "Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, Dawn Song",
    "published": "2024-12-20",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2412.16075v1",
    "pdf_url": "https://arxiv.org/pdf/2412.16075v1",
    "abstract": "AI for Mathematics (AI4Math) is not only intriguing intellectually but also crucial\nfor AI-driven discovery in science, engineering, and beyond. Extensive efforts on\nAI4Math have mirrored techniques in NLP, in particular, training large language\nmodels on carefully curated math datasets in text form. As a complementary yet\nless explored avenue, formal mathematical reasoning is grounded in formal systems\nsuch as proof assistants, which can verify the correctness of reasoning and provide\nautomatic feedback. In this position paper, we advocate for formal mathematical\nreasoning and argue that it is indispensable for advancing AI4Math to the next\nlevel. In recent years, we have seen steady progress in using AI to perform formal\nreasoning, including core tasks such as theorem proving and autoformalization, as\nwell as emerging applications such as verifiable generation of code and hardware\ndesigns. However, significant challenges remain to be solved for AI to truly master\nmathematics and achieve broader impact. We summarize existing progress, discuss\nopen challenges, and envision critical milestones to measure future success. At\nthis inflection point for formal mathematical reasoning, we call on the research\ncommunity to come together to drive transformative advancements in this field.\n1",
    "introduction": "field is that mathematical problems are a proxy for a broad array of reasoning and planning tasks. Another attraction is that math plays a foundational role in quantitative disciplines, so AI4Math has the potential to revolutionize AI for science, engineering, and beyond. For these reasons, designers of large language models (LLMs) [ 3,4] have frequently highlighted LLMs\u2019 success in math problems, and there have also been efforts to build AI systems that outperform humans at math competitions [5\u20137]. Given the importance of AI4Math, substantial research has been dedicated to developing math LLMs, using techniques borrowed from natural language processing (NLP). A common approach is to continue pretraining LLMs on math data, such as arXiv papers and web pages from MathOverflow, and then finetune the model on curated datasets of math problems with detailed, step-by-step solutions. We call this the \u201cinformal\u201d approach to distinguish it from the formal approach that will be introduced later (Sec. 2). Just like LLMs in general, math LLMs have a simple recipe, but the secret sauce is often data curation [ 8\u201311]. Carefully curated training data plus inference-time techniques, including chain-of-thought prompting [ 12], self-consistency [ 13], and tool use [ 14], have led to remarkable success on widely used benchmarks such as GSM8K [ 15] and MATH [ 16], as well as in the AIMO Progress Prize [ 6]. However, at the time of writing, the success of the informal approach has been Preprint. Under review.arXiv:2412.16075v1 [cs.AI] 20 Dec 2024 mostly limited to high school math not exceeding the AIME level.1This raises a key question: How far can we go by scaling up the informal approach? Will it enable math LLMs to solve more challenging competition problems (e.g., IMO, International Mathematical Olympiad) or even problems in mathematical research? Moving from high school to more advanced mathematics, the informal approach faces challenges that are hard to resolve by merely scaling up the training. First, training math LLMs requires high-quality data, which is scarce in advanced mathematics. For novel research math problems, it is infeasible to find solutions to similar problems on the Internet or manually annotate the data on a large scale. Without scaling up the data, we cannot fully benefit from the scaling laws for LLMs [ 18,19]. Second, solutions to many advanced problems are not numbers that can be evaluated by comparing them with the ground truth. Instead, they carry out a chain of intricate reasoning steps, e.g., a proof. LLMs are notorious for hallucinating seemingly valid reasoning steps, making it challenging to evaluate the correctness of model output or collect useful feedback for learning. These challenges are difficult to address by scaling up the informal approach during training. If training-time scaling is not enough, what else do we need? One emerging direction, exemplified by OpenAI o1 [ 17], is to scale up the informal approach during inference, potentially combining search with neural verifiers to mitigate hallucinated reasoning [ 15]. While this approach has gained traction, its effectiveness on advanced mathematical problems is an",
    "conclusion": ", community building, and a clear roadmap for the future.\n26\nThe narrative in this paper is rooted in the approach of AI and machine learning researchers, empha-\nsizing general-purpose learning algorithms applied to well-defined tasks that can be automatically\nevaluated using benchmarks. While this paradigm has dominated AI research in recent decades,\nit has limitations. Mathematicians have explored many ways of using AI in their work, including\nbrainstorming ideas and inspirations, writing assistance, and organizing or searching mathematical\nliterature. Many of these use cases, however, resist straightforward evaluation through benchmarks or\nautomated metrics. Even in areas like theorem proving\u2014where automated evaluation is feasible\u2014\nperformance on benchmarks may not fully capture what human users find meaningful or helpful\n(Sec. 4.3). For example, benchmarks such as LeanDojo [ 35] and PutnamBench [ 209] often fail to\nmeasure how the prover performs on new and evolving formalization projects, such as formalizing the\nproof of Fermat\u2019s Last Theorem [ 288]. This gap underscores the need for human-centered evaluation\napproaches that draw on insights from human-computer interaction and cognitive science [ 289,290]."
  },
  {
    "id": "2507.23726v2",
    "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
    "authors": "Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, Cheng Ren, Jiawei Shen, Wenlei Shi, Tong Sun, He Sun, Jiahui Wang, Siran Wang, Zhihong Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Yuchen Wu, Yihang Xia, Huajian Xin, Fan Yang, Huaiyuan Ying, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang, Ge Zhang, Tianyun Zhao, Jianqiu Zhao, Yichi Zhou, Thomas Hanwen Zhu",
    "published": "2025-07-31",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2507.23726v2",
    "pdf_url": "https://arxiv.org/pdf/2507.23726v2",
    "abstract": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement\nlearning with long chain-of-thought, yet they continue to struggle with theorem proving due\nto the lack of clear supervision signals when solely using natural language. Dedicated domain-\nspecific languages like Lean provide clear supervision via formal verification of proofs, enabling\neffective training through reinforcement learning. In this work, we propose Seed-Prover , a lemma-\nstyle whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean\nfeedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design\nthree test-time inference strategies that enable both deep and broad reasoning. Seed-Prover\nproves 78.1%of formalized past IMO problems, saturates MiniF2F, and achieves over 50% on\nPutnamBench, outperforming the previous state-of-the-art by a large margin. To address the\nlack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry ,\nwhich outperforms previous formal geometry engines. We use these two systems to participate in\nIMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in\nautomated mathematical reasoning, demonstrating the effectiveness of formal verification with\nlong chain-of-thought reasoning.\nProject Page: https://github.com/ByteDance-Seed/Seed-Prover\n2021 2022 2023 2024 2025\nPublication Date30405060708090100Pass Rate on miniF2F-test (%)\nProof Artifact\nCo-trainingCurriculum LearningHypertree Proof\nSearch\nReProverCOPRADeepSeek-Prover-V1\nInternLM2-Math-Plus\nTheoremLlamaLean-ST aRInternLM2-StepProverDeepSeek-Prover-V1.5\nABEL\nAlchemyInternLM2.5-StepProver\n3D-ProverHunyuanProver\nSTPBFS-Prover\nGoedel-ProverLeanabell-ProverKimina-Prover-PreviewDeepSeek-Prover-V2\nDSP+Kimina-ProverGoedel-Prover-V2 Delta-ProverSeed-Prover\nFigure 1 Growth in MiniF2F-Test performance over time.\n1arXiv:2507.23726v2  [cs.AI]  1 Aug 2025\n1",
    "introduction": "reinforcement learning to the training of large language models to prove mathematical statements. Unlike natural language, formal languages such as Lean can provide a clear and automatic signal on the correctness of a formalized proof. A noteworthy work from AlphaProof [3] uses Lean to successfully solve 3 problems from the 2024 International Mathematical Olympaid (IMO). AlphaProof demonstrates that LLMs using formal language are capable of proving very challenging problems that LLMs using natural language fail to prove. There are two types of LLM formal provers, step-level provers [ 3,14,22,23,25] and whole-proof generation provers[5,24]. Step-level provers incrementally generate Lean code line-by-line. While this enables close interaction with the Lean environment, it requires special scaffolding to generate a complete Lean proof, and the interaction is often too granular to allow high-level reasoning. In contrast, whole-proof models generate an entire Lean proof at once, but typically lack interaction with the Lean compiler. Recent work has shown that combining whole-proof models with long chain-of-thought reasoning [ 9,15,21] substantially outperforms step-level provers. In this work, we propose Seed-Prover , a whole-proof model with following features: \u2022Lemma-Style Proving : Seed-Prover tries to generate useful intermediate lemmas before proving the main statement. These lemmas serve as shared knowledge across different inference paths. \u2022Iterative Proof Refinement : Seed-Prover iteratively refines its proof based on Lean compiler feedback, previous proved lemmas, and self-summarization. \u2022Test-Time Scaling : We implement a three-tiered inference strategy that enables Seed-Prover to think both deeply and broadly\u2014allocating thinking budget to fine details while exploring interesting properties. \u2022SOTA Performance : Seed-Prover proves 5 out of 6 problems in IMO 2025, saturates MiniF2F [ 31] (shown in Figure 1), and outperforms prior work by up to 3\u00d7on multiple formal benchmarks. DuetothelackofsufficientgeometrysupportinLean, Seed-Proverincorporatesadedicatedgeometryreasoning engine Seed-Geometry . Similar to existing line of efforts in AlphaGeometry [2, 19] and TongGeometry [30], Seed-Geometry follows the forward-chaining design in the reasoning engine implementation, where the system derives all known facts by checking applicable rules until closure is reached. By backward-tracing fact dependencies, Seed-Geometry identifies the minimum dependency relations in a geometry problem\u2019s configuration, seperating the problem context from the auxiliary constructions necessary to prove a problem. Using statistics derived from more than past 20 years of math olympiad competitions, Seed-Geometry performs extensive search in the geometry space defined by its dedicated domain-specific language and establishes a repository of 230 million unique geometry problems requiring auxiliaries. A Seed model trained on such dedicated geometry data becomes an exceptionally effective neuro-symbolic geometry prover, where it fills in the missing auxiliary geometry elements and the geometry reasoning engine performs step-by-step forward- chaining, completing the final proof of a problem. In experiments, Seed-Geometry solves 43 of the IMO-AG-50 (vs.42 by AlphaGeometry 2), a benchmark that curates geometry problems of IMO from 2000 to 2024. It also sets a new state-of-the-art on the IMO shortlist geometry problems from 2000 to 2022, and notably solves the geometry problem of IMO 2025 under just 2 seconds. 2 Approach Here we introduce the two systems we used in IMO 2025,",
    "conclusion": "Zhao, Thomas Hanwen Zhu B LooKeng: An Easy-to-Use and Effective Python Interface for Lean Interacting with Lean poses significant challenges that limit the flexibility of Lean-based workflows. The most popular interface, LeanDojo [ 26], only supports earlier versions of Lean 4, restricting users from accessing Lean\u2019s newest updates. Furthermore, LeanDojo requires creating a Lean repository for interaction, which makes it impractical to use considering the massive scale of Lean interaction during model development and inference. To address these issues, we introduce LooKeng, a REPL4-based Python interface designed to simplify and accelerate the interaction process. LooKeng offers powerful features for developers while providing a user-friendly interface for end-users. The core functionality of LooKeng includes \u2018init_state\u2019, \u2018run_tac\u2019, and \u2018verify_proof\u2019. One can use LooKeng to interact with Lean step-by-step or verify an entire proof directly. The key features of LooKeng are summarized as follows: \u2022Stateless Design : A Lean state can be simultaneously processed using different LooKeng instances, enabling effortless scaling and sharing. \u2022Complex Tactics : Complex tactics such as apply?andall_goals are fully supported, with enhanced infotree integration to prevent false positive proofs. \u2022Version-Free : The LooKeng CLI allows users to manage and switch between different Lean versions. \u2022Memory Control : Users can easily track the memory consumption of the Lean backend, set custom thresholds, and automatically terminate processes when memory usage exceeds the limit. \u2022Proof Verification : LooKeng provides a straightforward method, \u2018verify_proof\u2019, to rigorously verify the final proof using the native Lean interface, ensuring correctness and reliability. \u2022Proof Simplification : LooKeng can remove useless tactics and hypothesis in the proof to obtain a simpler proof. \u2022Statement Negation : LooKeng is able to generate the negated statement of a statement. \u2022Multi-Concurrency Support : LooKeng can run as a service, handling thousands of concurrent requests via async architecture and resource isolation. 4https://github.com/leanprover-community/repl 12"
  }
]