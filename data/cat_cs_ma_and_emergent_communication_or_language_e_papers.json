[
  {
    "id": "2409.02645v2",
    "title": "Emergent Language: A Survey and Taxonomy",
    "authors": "Jannik Peters, Constantin Waubert de Puiseau, Hasan Tercan, Arya Gopikrishnan, Gustavo Adolpho Lucas De Carvalho, Christian Bitter, Tobias Meisen",
    "published": "2024-09-04",
    "category": "cs.MA",
    "arxiv_url": "http://arxiv.org/abs/2409.02645v2",
    "pdf_url": "https://arxiv.org/pdf/2409.02645v2",
    "abstract": "The field of emergent language represents a novel area of research within the domain of\nartificial intelligence, particularly within the context of multi-agent reinforcement learning.\nAlthough the concept of studying language emergence is not new, early approaches were pri-\nmarily concerned with explaining human language formation, with little consideration given\nto its potential utility for artificial agents. In contrast, studies based on reinforcement learning\naim to develop communicative capabilities in agents that are comparable to or even superior\nto human language. Thus, they extend beyond the learned statistical representations that are\ncommon in natural language processing research. This gives rise to a number of fundamen-\ntal questions, from the prerequisites for language emergence to the criteria for measuring its\nsuccess. This paper addresses these questions by providing a comprehensive review of 181 sci-\nentific publications on emergent language in artificial intelligence. Its objective is to serve as a\nreference for researchers interested in or proficient in the field. Consequently, the main contri-\nbutions are the definition and overview of the prevailing terminology, the analysis of existing\nevaluation methods and metrics, and the description of the identified research gaps.\nKeywords: emergent language, emergent communication, artificial intelligence, reinforcement\nlearning, multi-agent\n1arXiv:2409.02645v2  [cs.MA]  7 Mar 2025\n1",
    "introduction": "and overview of the EL field before 2021, however, it is mostly a summary\nof previous work and does not provide a taxonomy or review of existing metrics in the field as we\ndo.[58]focusesoncommoncharacteristicsinECresearchandthedevelopmentofemergenthuman-\nmachine communication strategies. They discuss distinctions and connections of EC research to\nlinguistics, cognitive science, computer science, and sociology, while we focus on emergent language\nand its analysis. We describe and discuss all relevant surveys in more detail in Section 3.\nBasedonthispreliminarywork,thecurrentstateofresearchonELmissesanoverarchingreview\nand a comprehensive compilation and alignment of proposed quantification and comparability",
    "conclusion": "In: Workshop on Reincarnating Reinforcement Learning at ICLR 2023 (2023). http://arxiv.org/pdf/2301.12050v2 [264] Sharma, A., Rao, Sudha and Brockett, Chris and Malhotra, Akanksha and Jojic, Nebojsa and Dolan, Bill: Investigating agency of llms in human-ai collaboration tasks. In: Graham, Y., Purver, M. (eds.) Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1968\u20131987. Associ- ation for Computational Linguistics, St. Julian\u2019s, Malta (2024). https://aclanthology.org/2 024.eacl-long.119 [265] Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y.,Zhao,W.X.,Wei,Z.,Wen,J.:Asurveyonlargelanguagemodelbasedautonomousagents. Frontiers of Computer Science 18(6) (2024) https://doi.org/10.1007/s11704-024-40231-1 [266] Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J.D., Chen, D., Arora, S.: Fine-tuning language models with just forward passes. In: Neural Information Processing Systems Foun- dation (ed.) Advances in Neural Information Processing Systems 36. Advances in neural information processing systems (2023). https://openreview.net/forum?id=Vota6rFhBQ [267] Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Chan, C.-M., Yu, H., Lu, Y., Hung, Y.-H., Qian, C., Qin, Y., Cong, X., Xie, R., Liu, Z., Sun, M., Zhou, J.: Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In: OpenReview.net (ed.) 12th International Conference on Learning Representations (2024). https://openreview.net/for um?id=EHg5GDnyq1 [268] Guo, T., Chen, X., Wang, Y., Chang, R., Pei, S., Chawla, N.V., Wiest, O., Zhang, X.: Large languagemodelbasedmulti-agents:Asurveyofprogressandchallenges.In:KateLarson(ed.) Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, pp. 8048\u20138057 (2024). https://doi.org/10.24963/ijcai.2024/890 [269] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, Thomas Griffiths: Cognitive architec- tures for language agents. Transactions on Machine Learning Research (2024) [270] Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., Yao, S.: Reflexion: Lan- guage agents with verbal reinforcement learning. In: Neural Information Processing Systems Foundation (ed.) Advances in Neural Information Processing Systems 36. Advances in neu- ral information processing systems (2023). https://papers.nips.cc/paper_files/paper/2023/ file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf 78"
  },
  {
    "id": "2406.07277v2",
    "title": "Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication",
    "authors": "Olaf Lipinski, Adam J. Sobey, Federico Cerutti, Timothy J. Norman",
    "published": "2024-06-11",
    "category": "cs.CL",
    "arxiv_url": "http://arxiv.org/abs/2406.07277v2",
    "pdf_url": "https://arxiv.org/pdf/2406.07277v2",
    "abstract": "Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication Olaf Lipinski1\u2217Adam J. Sobey2,1Federico Cerutti3Timothy J. Norman1 1University of Southampton2The Alan Turing Institute3University of Brescia {o.lipinski,t.j.norman}@soton.ac.uk asobey@turing.ac.uk federico.cerutti@unibs.it Abstract Effective communication requires the ability to refer to specific parts of an ob- servation in relation to others. While emergent communication literature shows success in developing various language properties, no research has shown the emergence of such positional references. This paper demonstrates how agents can communicate about spatial relationships within their observations. The results indicate that agents can develop a language capable of expressing the relationships between parts of their observation, achieving over 90% accuracy when trained in a referential game which requires such communication. Using a collocation measure, we demonstrate how the agents create such references. This analysis suggests that agents use a mixture of non-compositional and compositional messages to convey spatial relationships. We also show that the emergent language is interpretable by humans. The translation accuracy is tested by communicating with the receiver agent, where the receiver achieves over 78% accuracy using parts of this lexicon, confirming that the interpretation of the emergent language was successful. 1 Spatial referencing in emergent communication Emergent communication allows agents to develop bespoke languages for their environment. While there are many successful examples of efficient (Rita et al., 2020) and compositional (Chaabouni et al., 2020) languages, they often lack fundamental aspects seen in human language, such as syntax (Lazaridou and Baroni, 2020) or recursion (Baroni, 2020). It is argued that these aspects of communication are important to improve the efficiency and generalisability of emergent languages (Baroni, 2020; Boldt and Mortensen, 2024; Rita et al., 2024). However, the current architectures, environments, and reward schemes are yet to exhibit such fundamental properties. One such aspect is the development of deixis (Rita et al., 2024), which has",
    "introduction": "been described as a way of pointing through language. Examples of temporal deixis include words such as \u201cyesterday\u201d or \u201cbefore,\u201d and spatial deixis include words such as \u201chere\u201d or \u201cnext to\u201d (Lyons, 1977). In emergent communication, Lipinski et al. (2023) investigate how agents may refer to repeating observations, which could also be viewed from the linguistic perspective as investigating temporal deixis . However, while there are advocates to investigate how emergent languages can develop key concepts from human language (Rita et al., 2024), no work has demonstrated the emergence of relative references to specific locations within an observation, or spatial deixis . Spatial references would be valuable in establishing shared context between agents, increasing com- munication efficiency by reducing the need for detailed descriptions, and adaptability, by removing the need for unique references per object. For example, instead of describing a new, previously \u2217Corresponding author: o.lipinski@soton.ac.uk 38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2406.07277v2 [cs.CL] 28 Oct 2024 unseen object, such as \u201ca blue vase with intricate motifs on the table,\u201d one could simply use spatial relationships and say \u201cthe object left of the plate.\u201d Spatial referencing streamlines communication by leveraging the shared environment as a reference point. In dynamic environments where objects might change positions, spatial references enable agents to easily track and refer to objects without having to update their descriptions. This enhances communication efficiency and improves interaction and collaboration between agents. These elements may also help the evolved language become human interpretable, allowing the development of trustworthy emergent communication (Lazaridou and Baroni, 2020; Mu and Goodman, 2021). This paper therefore explores how agents can develop communication with spatial references. While Rita et al. (2024) posit that the emergence of these references might require complex settings, we show that even agents trained in a modified version of the simple referential game (Lazaridou et al., 2018; Lewis, 1969) can develop spatial references.2This resulting language is segmented and analysed using a collocation measure, Normalised Pointwise Mutual Information (NPMI) adapted from computational linguistics. NPMI allows us to measure the strength of associations between message parts and their context, making it a valuable tool for gaining insights into the underlying structure of the emergent language. Using NPMI, we show how the agents compose such spatial references, providing the first hint of a syntactic structure, and showing that the emergent language can be interpreted by humans. 2 Development of a spatial referential game Current emergent communication environments have not produced languages incorporating spatial references. To address this, we present a referential game (Lazaridou et al., 2018) environment where an effective language requires communication about spatial relationships. 2.1 Referential game environment In the referential game, there are two agents, a sender and a receiver. The sender observes a vector and transmits its compressed representation through a discrete channel to the receiver. The receiver observes a set of vectors and the sender\u2019s message. One of these vectors is the same as the one the sender has observed. The receiver\u2019s goal is to correctly identify",
    "conclusion": "Recent work in the field of emergent communication has advocated for better alignment of emergent\nlanguages with natural language (Boldt and Mortensen, 2024; Rita et al., 2024), such as through\nthe investigation of deixis (Rita et al., 2024). Aligned to this approach, we provide a first reported\nemergent language containing spatial references (Lyons, 1977), together with a method to interpret\nthe agents\u2019 messages in natural language. We show that agents can learn to communicate about spatial\nrelationships with over 90% accuracy. We identify both compositional and non-compositional spatial\nreferencing, showing that the agents use a mixture of both. We hypothesise why the agents choose\nnon-compositional representations of observation types which are sparse in the dataset, arguing\nthat this behaviour can be used to increase communicative efficiency. We show that, using the\nNPMI language analysis method, we can create a human interpretable dictionary, of the agents\u2019 own\nlanguage. We confirm that our method of language interpretation is accurate, achieving over 94%\naccuracy for certain dictionaries.\n10"
  },
  {
    "id": "2402.16247v1",
    "title": "Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition",
    "authors": "Dylan Cope, Peter McBurney",
    "published": "2024-02-26",
    "category": "cs.LG",
    "arxiv_url": "http://arxiv.org/abs/2402.16247v1",
    "pdf_url": "https://arxiv.org/pdf/2402.16247v1",
    "abstract": "In Emergent Communication (EC) agents learn to\ncommunicate with one another, but the protocols\nthat they develop are specialised to their training\ncommunity. This observation led to research into\nZero-Shot Coordination (ZSC) for learning com-\nmunication strategies that are robust to agents not\nencountered during training. However, ZSC typi-\ncally assumes that no prior data is available about\nthe agents that will be encountered in the zero-shot\nsetting. In many cases, this presents an unnecessar-\nily hard problem and rules out communication via\npreestablished conventions. We propose a novel AI\nchallenge called a Cooperative Language Acquisi-\ntion Problem (CLAP) in which the ZSC assump-\ntions are relaxed by allowing a \u2018joiner\u2019 agent to\nlearn from a dataset of interactions between agents\nin a target community. We propose and compare\ntwo methods for solving CLAPs: Imitation Learn-\ning (IL), and Emergent Communication pretrain-\ning and Translation Learning (ECTL), in which\nan agent is trained in self-play with EC and then\nlearns from the data to translate between the emer-\ngent protocol and the target community\u2019s protocol.\n1",
    "introduction": "of solving the task. As such, the mapping between meanings and messages is arbitrary, \u2217Correspondence: dylan.cope@kcl.ac.uk. DC is supported by the UKRI Centre for Doctoral Training in Safe and Trusted AI (EP- SRC Project EP/S023356/1). Preprint (under review).and any permutation of a learned protocol is equally likely to appear across different training runs (Bullard et al., 2021). The result is that the learned conventions established within a training community will be very unlikely to work with new agents, and by default, the EC trained agents will be incapable of adapting. In response to this, many researchers have become inter- ested in devising methods in which agents learn communica- tive strategies that can adapt to this Zero-Shot Coordination (ZSC) setting (Li et al., 2023; Hu et al., 2021, 2020; Os- senkopf, 2020; Cope and Schoots, 2020; Bullard et al., 2020). ZSC algorithms typically aim to successfully communicate with an unknown agent on the first encounter, without any prior information. But in many real-world settings, this is an unnecessarily challenging assumption. If someone is injured on a street in London, passing pedestrians can form an ad hoc team and aid the patient by speaking to each other in En- glish to coordinate a response. Indeed, language is arguably the most critical set of conventions that such teams can draw upon to efficiently work together. The study of artificial agents that can form ad hoc teams is known as Ad Hoc Teamwork (AHT) (Stone et al., 2010). Sim- ilarly to ZSC, most of these algorithms aim to make as few assumptions as possible about the players that an agent may form a team with. Notably, Sarratt and Jhala (2015) applied this minimalist approach to communication. Other work has relaxed this by assuming a prior known communication pro- tocol (Barrett et al., 2014; Mirsky et al., 2020). In this work, we present a novel AI challenge that we call aCooperative Language Acquisition Problem (CLAP). Here by \u2018language acquisition\u2019 we mean learning the syntax and semantics of a preexisting communication system used by a community. This class of problems is positioned between the challenges of ZSC and AHT. In a CLAP, we are given a dataset of communication events between speakers and lis- teners in a target community as they solve a problem. Our goal is to construct a joiner agent that can communicate and cooperate with agents from this community. This problem is also closely related to Imitation Learning (IL), however, most work in IL is confined to the single agent setting (Hussein et al., 2017). So to the best of our knowledge, this is the first attempt to pose an IL problem for multi-agent communication within a formal cooperative model. Alongside defining CLAP, we outline two baseline solu-arXiv:2402.16247v1 [cs.LG] 26 Feb 2024 tions to this problem. The first uses a simple imitation learn- ing method. The second is a novel algorithm called Emer- gent Communication pretraining and Translation Learning (ECTL). We introduce two environments and train target communities of agents that cooperate via",
    "conclusion": "agent was then evaluated for 500 episodes in the zero-shot CLAP-Replace setting. For Figure 5a, for each sample along the x-axis (dif- ferent Ncollect values), this entire process was run. Thus there was 9 Ncollect samples \u00d72 community permutations \u00d72 pos- sible replacement agents \u00d73 trials, totally 432 training runs. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7106\u221225\u221220\u221215\u221210\u22125Mean Reward 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7106567891011Mean Episode LengthTraining Statistics for 111b5 00000Figure 7: Training curves for one training run on the gridworld environment. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7106\u221225\u221220\u221215\u221210\u22125Mean Reward 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7106567891011Mean Episode LengthTraining Statistics for b20cd 00000 Figure 8: Training curves for one training run on the gridworld environment. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d71080200400600800100012001400Mean Reward 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7108150160170180190200Mean Episode LengthTraining Statistics for dc825 00000 Figure 9: Training curves for one training run on the driving environment, in the \u2018pit\u2019 scenario. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d71080200400600800100012001400Mean Reward 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7108150160170180190200Mean Episode LengthTraining Statistics for de02c 00000Figure 10: Training curves for one training run on the driving environment, in the \u2018pit\u2019 scenario. 0.0 0.2 0.4 0.6 0.8 1.0 Timesteps \u00d7108250500750100012501500Mean Reward 0.0 0.2 0.4 0.6 0.8 1.0 Timesteps \u00d7108150160170180190200Mean Episode LengthTraining Statistics for 3fba1 00000 Figure 11: Training curves for one training run on the driving environment, in the \u2018no pit\u2019 scenario. 0.0 0.2 0.4 0.6 0.8 1.0 Timesteps \u00d7108500100015002000Mean Reward 0.0 0.2 0.4 0.6 0.8 1.0 Timesteps \u00d7108150160170180190200Mean Episode LengthTraining Statistics for 4a79d 00000 Figure 12: Training curves for one training run on the driving environment, in the \u2018no pit\u2019 scenario."
  },
  {
    "id": "2310.06555v2",
    "title": "It's About Time: Temporal References in Emergent Communication",
    "authors": "Olaf Lipinski, Adam J. Sobey, Federico Cerutti, Timothy J. Norman",
    "published": "2023-10-10",
    "category": "cs.CL",
    "arxiv_url": "http://arxiv.org/abs/2310.06555v2",
    "pdf_url": "https://arxiv.org/pdf/2310.06555v2",
    "abstract": "Emergent communication studies the development of language between autonomous agents,\naiming to improve understanding of natural language evolution and increase communication\nefficiency. While temporal aspects of language have been considered in computational\nlinguistics, there has been no research on temporal references in emergent communication.\nThis paper addresses this gap, by exploring how agents communicate about temporal\nrelationships. We analyse three potential influences for the emergence of temporal references:\nenvironmental, external, and architectural changes. Our experiments demonstrate that\nalteringthelossfunctionisinsufficientfortemporalreferencestoemerge; rather, architectural\nchanges are necessary. However, a minimal change in agent architecture, using a different\nbatching method, allows the emergence of temporal references. This modified design is\ncompared with the standard architecture in a temporal referential games environment,\nwhich emphasises temporal relationships. The analysis indicates that over 95% of the agents\nwith the modified batching method develop temporal references, without changes to their\nloss function. We consider temporal referencing necessary for future improvements to the\nagents\u2019 communication efficiency, yielding a closer to optimal coding as compared to purely\ncompositional languages. Our readily transferable architectural insights provide the basis\nfor their incorporation into other emergent communication settings.\nKeywords: Emergent Communication, Emergent Language, Temporal Logic, Multiagent\nSystems, Representation Learning\n1",
    "introduction": "their vocabulary size and word length to their specific task, providing an advantage over a general communication protocol (Rita et al., 2020). Many aspects of emergent language have been explored (Lazaridou and Baroni, 2020; Boldt and Mortensen, 2024), with a particular focus on improving communication efficiency \u00a92024 Olaf Lipinski, Adam J. Sobey, Federico Cerutti, Timothy J. Norman. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ . Attribution requirements are provided at http://jmlr.org/papers/v25/25-0000.html .arXiv:2310.06555v2 [cs.CL] 3 May 2024 Lipinski, Sobey, Cerutti and Norman (Rita et al., 2020; Chaabouni et al., 2019; Kang et al., 2020). Kang et al. (2020) demonstrate how using the minimal deviation between subsequent time steps allows for more concise communication by reducing redundant information transfer. Investigation of the contextual information of the resulting language offered a further improvement in agent performance by using the time step similarity together with optimisation of the reconstruction of the speaker\u2019s state (Kang et al., 2020). There is, however, no existing research investigating or reporting on the emergence of temporal referencing strategies, where agents could communicate about relationships between different time steps. Such temporal references, together with the general characteristics of emergent languages, will enhance the agent\u2019s bandwidth efficiency and task performance in a variety of situations. As environmental complexity is being scaled in emergent communication research (Chaabouni et al., 2022), temporal references will benefit agents in settings where temporal relationships are embedded. One example is social deduction games (Brandizzi et al., 2021; Lipinski et al., 2022; Kopparapu et al., 2022), where referencing past events are expected to be key to winning strategies. Temporal references will allow agents to develop more efficient methods of communication by assigning shorter messages to events that happen more often. This is similar to Zipf\u2019s Law in human languages (Zipf, 1949), which states that the most commonly used words are the shortest. Temporal references would be particularly effective when the distribution of observations would be non-uniform, which means that certain objects appear more often than others. Specialised messages, used only for temporal references, would then also become more frequent than others. From information theory, we know that (adaptive) Huffman coding (Huffman, 1952; Knuth, 1985; Vitter, 1987) can assign shorter bit sequences to more frequent messages, thereby compressing them more efficiently than less common messages. Consequently, the incorporation of temporal references can enhance the efficiency of transmitting emergent language, optimizing communication. Our contribution lies in examining when temporal references emerge between agents. Three potential prerequisites are explored: environmental pressures, external pressures and architectural changes. The agents are trained in both the regular referential game (Lazaridou et al., 2017) and on an environment which encourages the development of temporal references through embedded environmental pressures (Section 2.3). The effect of an external pressure to develop temporal referencing is explored via an additional loss applied to the agents (Section 5). Three types of architecture are evaluated, (Section 3), analysing two novel architectures together with a reference architecture based on the commonly used EGG (Kharitonov et al., 2019) agents. The baseline Base(Section 3.1) agent, provides",
    "conclusion": "Emergent communication has been studied extensively, considering many aspects of emergent\nlanguages, such as efficiency (Rita et al., 2020; Chaabouni et al., 2019), compositionality\n(Auersperger and Pecina, 2022), generalisation (Chaabouni et al., 2020) and population\ndynamics (Chaabouni et al., 2022; Rita et al., 2022a). Yet, there has been no investigation\ninto learning and communicating temporal relationships. Discussing past observations is\nvital to communication, saving bandwidth by avoiding repeating information and allowing\nfor easier experience sharing.\nThis paper provides a first exploration of such emergent languages, including addressing\nthe fundamental questions of when they could develop and what is required for their\nemergence. We present a set of environments that are designed to facilitate investigation\ninto how agents might create such references. We use the conventional agent architecture for\nemergent communication (Kharitonov et al., 2019) as a baseline and explore both temporal\nloss and alternative architectures that may endow agents with the ability to learn temporal\nrelationships. We show that architectural change is necessary for temporal references to\nemerge, and demonstrate that temporal prediction loss is neither sufficient for their emergence,\nnor does it improve the emergent language.\n17\nLipinski, Sobey, Cerutti and Norman"
  },
  {
    "id": "2209.15342v2",
    "title": "Emergent Communication: Generalization and Overfitting in Lewis Games",
    "authors": "Mathieu Rita, Corentin Tallec, Paul Michel, Jean-Bastien Grill, Olivier Pietquin, Emmanuel Dupoux, Florian Strub",
    "published": "2022-09-30",
    "category": "cs.MA",
    "arxiv_url": "http://arxiv.org/abs/2209.15342v2",
    "pdf_url": "https://arxiv.org/pdf/2209.15342v2",
    "abstract": "Lewis signaling games are a class of simple communication games for simulating\nthe emergence of language. In these games, two agents must agree on a commu-\nnication protocol in order to solve a cooperative task. Previous work has shown\nthat agents trained to play this game with reinforcement learning tend to develop\nlanguages that display undesirable properties from a linguistic point of view (lack\nof generalization, lack of compositionality, etc). In this paper, we aim to provide\nbetter understanding of this phenomenon by analytically studying the learning\nproblem in Lewis games. As a core contribution, we demonstrate that the standard\nobjective in Lewis games can be decomposed in two components: a co-adaptation\nloss and an information loss. This decomposition enables us to surface two po-\ntential sources of over\ufb01tting, which we show may undermine the emergence of a\nstructured communication protocol. In particular, when we control for over\ufb01tting\non the co-adaptation loss, we recover desired properties in the emergent languages:\nthey are more compositional and generalize better.\n1",
    "introduction": "light on the prerequisites of language emergence. In their original form, Lewis signaling games involve two agents: a speaker and a listener. The speaker observes a random state from its environment, e.g. an image, and sends a signal to the listener. The listener then undertakes an action based on this signal. Finally, both agents are equally rewarded based on the outcome of the listener\u2019s action. The resolution of this cooperative two-player game requires the emergence of a shared protocol between the agents [ 55,17]. One way to model the emergence of such protocol is to give the agents the capacity to learn. The agents, and therefore, the communication protocol, are shaped by a sequence of trials and errors over multiple games [ 81,44,75,70]. This learning-centric approach allows for a \ufb01ne analysis of the language emergence dynamics [ 70,36]. It \u0003This work was performed when Paul Michel was af\ufb01liated with Ecole Normale Sup\u00e9rieure PSL. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2209.15342v2 [cs.MA] 15 Oct 2022 also raises challenging learning-speci\ufb01c questions: What are the inductive biases present in the agent architecture and loss function that shape the emergent language [ 43]? How do agents generalize from their training set? Is the resulting language compositional [ 8]? What is the impact of over\ufb01tting [ 53]? Recently, there has been a resurgence of interest for such learning-based approaches following advances in machine learning [ 51]. In these approaches, the speakers and listeners are modeled as deep reinforcement learning agents optimized to solve instances of the Lewis games [ 53,33,65,56,28]. The vast majority of these works explore Lewis games from an empirical perspective. However, some of the recent experimental results are at odds with experimental \ufb01ndings from the linguistics literature. For instance, the emergent protocols lack interpretability [ 48], generalization does not always correlate with language compositionality [ 10], successful strategies are not naturally adopted in populations [ 68,12], and anti-ef\ufb01cient communication may even emerge [ 9]. It is unclear whether those empirical observations result from a learning failure, e.g. optimization problems, over\ufb01tting, or whether they are symptomatic of more fundamental limitations of Lewis games for modeling language emergence, e.g. lack of embodiment [ 31,4,63,37]. Overall, it is crucial to establish new analytical insight to analyze Lewis games in the learning setting. In this paper, we introduce such an analytical framework to diagnose the learning dynamics of deep reinforcement learning agents in Lewis signaling games. As a core contribution, we demonstrate under mild assumptions that the loss of the speaker and listener can be decomposed into two components when resolving Lewis signaling games: (i) an information loss that maximizes the mutual information between the observed states and speaker messages; (ii) a co-adaptation loss that aligns the speaker and listener\u2019s interpretation of the messages (Section 2). Based on this decomposition, we empirically examine the evolution of these two losses during the learning process (Section 5). In particular, we identify an over\ufb01tting problem in the co-adaptation loss between the agents which undermines the emergence of",
    "conclusion": "In this paper, we propose a methodological approach to better understand the dynamics in Lewis\nsignaling games for language emergence. It allows us to surface two components of the training:\n(i) an information loss, (ii) a co-adaptation loss. We shed light that the agents tend to over\ufb01t this\nco-adaptation term during training, which hinders the learning dynamic and degrades the resulting\nlanguage. As soon as this over\ufb01tting is controlled, agents develop compositional languages that better\ngeneralize. Remarkably, this emergent compositionality does not result from environmental factors,\ne.g. communication bottleneck [ 43], under-parametrization [ 48,26], population dynamics [ 12,68],\nmemory restriction [ 15,16] or inductive biases [ 67], but only through a trial-and-error process.\nTherefore, we advocate for a better comprehension of the optimization and machine learning issues.\nAs illustrated in this paper, such understanding may unveil contradictions between computational\nmodels and language empirical observations and better expose the existing synergies between learning\ndynamics and environmental factors [27, 83, 64, 14, 18, 22]."
  },
  {
    "id": "2206.11302v1",
    "title": "Recommendations for Systematic Research on Emergent Language",
    "authors": "Brendon Boldt, David Mortensen",
    "published": "2022-06-22",
    "category": "cs.MA",
    "arxiv_url": "http://arxiv.org/abs/2206.11302v1",
    "pdf_url": "https://arxiv.org/pdf/2206.11302v1",
    "abstract": "Emergent language is unique among \ufb01elds\nwithin the discipline of machine learning for its\nopen-endedness, not obviously presenting well-\nde\ufb01ned problems to be solved. As a result, the\ncurrent research in the \ufb01eld has largely been ex-\nploratory: focusing on establishing new prob-\nlems, techniques, and phenomena. Yet after\nthese problems have been established, subse-\nquent progress requires research which can mea-\nsurably demonstrate how it improves on prior\napproaches. This type of research is what we\ncallsystematic research ; in this paper, we illus-\ntrate this mode of research speci\ufb01cally for emer-\ngent language. We \ufb01rst identify the overarching\ngoals of emergent language research, categoriz-\ning them as either science or engineering. Us-\ning this distinction, we present core methodolog-\nical elements of science and engineering, analyze\ntheir role in current emergent language research,\nand recommend how to apply these elements.\n1.",
    "introduction": "of research is 1Language Technologies Institute, Carnegie Mellon Uni- versity, Pittsburgh, PA 15213, USA. Correspondence to: Brendon Boldt <bboldt@cs.cmu.edu>, David Mortensen <dmortens@cs.cmu.edu>.what we call systematic research . This systematicity is nec- essary for research to be effectively applied to the real pro b- lems and questions which motivate it in the \ufb01rst place. The goal of this paper is to provide concrete recommendations for moving work on emergent language toward systematic research. In order to develop these recommendations, we start in Section 2by clarifying the goals of emergent language re- search; drawing attention to how they \ufb01t into the categories of science and engineering. In Section 3we illustrate the prototypical methods of science and engineering research which facilitate systematic research. Finally, in Section 4 we analyze to what extent current emergent language re- search uses these methods and recommend how they could be more fully employed. The speci\ufb01c contributions of this work are: 1. Recommending methods which move emergent lan- guage research toward systematic research, working towards overarching goals via measurable progress. 2. Distinguishing between the capacities in which emer- gent language research can be science or engineering via the goals of a given project. 1.1. Exploratory and Systematic Research Exploratory research is research which focuses on innova- tive approaches and problems rather than pursuing a well- established goal. Exploratory research can take place ei- ther in a new \ufb01eld, such as emergent language, or on the frontier of a more established research area. The primary purpose of exploratory research is to build up a baseline level of knowledge of new approaches which differ signi\ufb01- cantly from well-studied approaches. Once a critical mass of knowledge is acquired, a more structured and systematic research program is pursued in order to advance the knowl- edge and expertise of the \ufb01eld on that topic. Systematic research is characterized by research contribu - tions which measurably demonstrates how they improve on prior approaches; this entails adhering to established r e- search problems. The long-term success of any given re- search \ufb01eld is, in fact, predicated on systematic research; the direct comparison between two approaches to the same problem is what allows for signi\ufb01cant progress to be made Recommendations for Systematic Research on Emergent Langu age in an incremental way. At times, \u201cincremental\u201d is consid- ered a negative attribute, but this is far from the case in our own usage as the incremental nature of science and engi- neering are integral to their success as disciplines. In the end, both exploratory and systematic research have a time and place where they are appropriate. The ideas of exploratory and systematic research parallel the concepts of \u201cpre-paradigmatic science\u201d and \u201cnormal science\u201d from Kuhn (1962 ). Our account, though, is in- tended to apply to science and engineering and is targeted towards practitioners of these \ufb01elds rather philosophers o f science. 2. Goals of Emergent Language Research The goals of a pursuit dictate the methods employed to achieve it. Accordingly, we begin by enumerating the over- arching goals",
    "conclusion": "The above methodological recommendations offer a way\nto move work on emergent language toward systematic re-\nsearch, where research projects build off each other in a\nway that shows measurable improvement. The exploratory\nresearch in the \ufb01eld thus far aids in recognizing the unique\nchallenges of emergent language research which is nec-\nessary to implement these recommendations. The dis-\ntinction we have made between the capacities in which\nemergent language research can be science or engineering\nis critical\u2014while the \ufb01elds are closely related and inter-\ntwined, their paths to comparable, iterative research diff er\nsigni\ufb01cantly. Moving towards systematic research on emer-\ngent language will, then, facilitate the achievement of the\nunique and important goals of this \ufb01eld."
  },
  {
    "id": "2510.18221v3",
    "title": "The Emergence of Complex Behavior in Large-Scale Ecological Environments",
    "authors": "Joseph Bejjani, Chase Van Amburg, Chengrui Wang, Chloe Huangyuan Su, Sarah M. Pratt, Yasin Mazloumi, Naeem Khoshnevis, Sham M. Kakade, Kiant\u00e9 Brantley, Aaron Walsman",
    "published": "2025-10-21",
    "category": "cs.MA",
    "arxiv_url": "http://arxiv.org/abs/2510.18221v3",
    "pdf_url": "https://arxiv.org/pdf/2510.18221v3",
    "abstract": "We explore how physical scale and population size shape the emergence of com-\nplex behaviors in open-ended ecological environments. In our setting, agents\nare unsupervised and have no explicit rewards or learning objectives but instead\nevolve over time according to reproduction, mutation, and selection. As they act,\nagents also shape their environment and the population around them in an ongoing\ndynamic ecology. Our goal is not to optimize a single high-performance policy,\nbut instead to examine how behaviors emerge and evolve across large populations\ndue to natural competition and environmental pressures. We use modern hardware\nalong with a new multi-agent simulator to scale the environment and population to\nsizes much larger than previously attempted, reaching populations of over 60,000\nagents, each with their own evolved neural network policy. We identify various\nemergent behaviors such as long-range resource extraction, vision-based foraging,\nand predation that arise under competitive and survival pressures. We examine\nhow sensing modalities and environmental scale affect the emergence of these be-\nhaviors and find that some of them appear only in sufficiently large environments\nand populations, and that larger scales increase the stability and consistency of\nthese emergent behaviors. While there is a rich history of research in evolutionary\nsettings, our scaling results on modern hardware provide promising new directions\nto explore ecology as an instrument of machine learning in an era of increasingly\nabundant computational resources and efficient machine frameworks. Experi-\nmental code is available at https://github.com/jbejjani2022/ecological-emergent-\nbehavior.\n1",
    "introduction": "across a wide variety of biological disciplines, including evolutionary theory (Darwin, 1859; Simp- son, 1944), ecology (MacArthur & Wilson, 2001), and genetics (Dobzhansky, 1937). Models of these emergent dynamics range from theories of speciation and adaptive radiation (Coyne & Orr, 2004; Schluter, 2000) to the mathematical analyses of population genetics (Fisher, 1930; Wright, 1931). In these fields, decades of mathematical modeling, laboratory experiments, and field work have tremendously improved our understanding of the mechanisms of evolution in nature. Unfortunately, there are many obstacles to studying emergent behavior in the natural world. As ecosystems become larger and more complex, they are more difficult to control and measure. Even where possible, running controlled experiments in large-scale natural settings risks damaging or displacing wild populations. \u2217Correspondence to: jbejjani@college.harvard.edu / aaronwalsman@fas.harvard.edu 1arXiv:2510.18221v3 [cs.MA] 12 Dec 2025 In an effort to better understand the emergence of complex behavior due to competitive and envi- ronmental pressures, we study the open-ended evolution of neural network policies in large-scale ecological simulations. In this setting, agents do not have a specified objective or reward signal but instead collect resources to survive and reproduce according to the dynamics of the environment. Policy changes occur only via mutation as parents pass their policies on to their children. With the rapid growth in computational resources available to researchers, our goal is to study open-ended evolutionary dynamics in populations of these simple digital organisms at scales not previously pos- sible, in the hopes of providing new tools for investigating the effects of environmental scale and population size on behavioral emergence. To study this, we use a new JAX-based simulation environment that allows rapid evaluation of large grid worlds. Our larger experiments can contain over 60,000 individual agents with a physical area of over 1,000,000 grid cells. Agents in this environment must navigate to find food and other resources in order to survive. Agents can also reproduce by collecting enough resources to make a mutated copy of themselves. A 3D visualization of this environment can be found in Figure 1. Our goal is not only to better understand ecological phenomena through simulation, but to also study ecology as a mechanism for producing machine intelligence. Recent years have demonstrated that in AI, scale doesn\u2019t merely improve models\u2014it fundamentally transforms their capabilities (Wei et al., 2022a). Yet despite this insight, many attempts at embodied learning have been often been confined to relatively small environments with low populations. Our work explores what capabilities emerge when we scale not just a single model but an entire ecosystem. Just as reasoning emerges in indi- vidual models only beyond a certain size threshold (Wei et al., 2022b), we seek to understand what strategies evolve within populations only when environmental size and complexity reach sufficient richness. Through extensive experiments, we identify multiple instances where sensor configurations and our large environmental scales strongly impact the behaviors that emerge from simulation, which can lead to long-term ecological consequences. For example, we show that agents with simple com- pass sensors can adapt to conduct",
    "conclusion": "of each attack. However when the lethality is reduced, attacking behavior becomes less useful and therefore less likely to be selected for in the agent population. Agents with lower attack strengths therefore do not become as skillful at attacking and spend more time foraging for food. Figure 22: Population and action frequency data over 2M steps in a 256\u00d7256Oceanworld for (RCV+A)agents across Full Attack, Half Attack, and Quarter Attack strength settings. Four seeds are plotted per attack strength setting. F.8 VARYINGAGENTHP We investigate the effect of reducing the starting HP of child agents and lowering the healing rate to apply pressure for agents to \u2018grow\u2019 over their lifetime in order to survive. The healing rate parameter determines how much HP an agent can regenerate per healing step, which converts available energy to HP. Agents can influence healing only by maintaining enough energy. We study two settings: 1) Full HP: child HP starts at 5 and healing rate is 1 HP per 0.1 energy 2) Low HP: child HP starts at 1, with healing rate 0.5. Max agent HP is 10 in all settings. The main experiments (Section 4) were run with the Full HP setting. Across the two settings, there is no significant difference in homicides per attack, eat action fre- quency, or biomass utilization (Figure 23). In the Full HP setting, populations grow slightly larger, while in the Low HP setting, agents attack less frequently and move slightly more. We hypothesize that agents born with lower HP must focus on foraging early on in their lifetime in order to grow to full size, which would explain this difference. 32 Figure 23: Population and action frequency data over 2M steps in the 256\u00d7256Oceanworld for (RCV+A)agents across Full HP and Low HP settings. Four seeds are plotted per HP setting. 33"
  },
  {
    "id": "2211.02412v2",
    "title": "Emergent Quantized Communication",
    "authors": "Boaz Carmeli, Ron Meir, Yonatan Belinkov",
    "published": "2022-11-04",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2211.02412v2",
    "pdf_url": "https://arxiv.org/pdf/2211.02412v2",
    "abstract": "The \ufb01eld of emergent communication aims to understand the\ncharacteristics of communication as it emerges from arti\ufb01-\ncial agents solving tasks that require information exchange.\nCommunication with discrete messages is considered a de-\nsired characteristic, for both scienti\ufb01c and applied reasons.\nHowever, training a multi-agent system with discrete commu-\nnication is not straightforward, requiring either reinforcement\nlearning algorithms or relaxing the discreteness requirement\nvia a continuous approximation such as the Gumbel-softmax.\nBoth these solutions result in poor performance compared to\nfully continuous communication. In this work, we propose\nan alternative approach to achieve discrete communication\n\u2013 quantization of communicated messages. Using message\nquantization allows us to train the model end-to-end, achiev-\ning superior performance in multiple setups. Moreover, quan-\ntization is a natural framework that runs the gamut from con-\ntinuous to discrete communication. Thus, it sets the ground\nfor a broader view of multi-agent communication in the deep\nlearning era.\n1",
    "introduction": "agent systems, which are typically trained with gradient- based optimization. Two main approaches have been pro- posed in the literature for overcoming this challenge, namely using reinforcement learning (RL) algorithms (Williams *Supported by the Viterbi Fellowship in the Center for Com- puter Engineering at the Technion. 0.10-0.230.690.72-0.54-0.210011004.02.07.07.01.02.00.10-0.230.694.01.07.0Gumbel-softmaxContinuousQuantizedWordMessageN/ASymbol0011.0Single symbolFigure 1: Top: Symbol, word, and message elements for continuous, Gumbel-softmax, and quantized communica- tion modes. Bottom : Accuracy (Y-axis) achieved by the three communication modes vs. number of candidates (X- axis), in the Object game. Continuous communication leads to good performance on the end task but does not use sym- bols. Gumbel-softmax sends one word per symbol, but re- quires a recurrent channel and does not work well in prac- tice. Quantized communication enables discrete and suc- cessful communication. Detailed channel parameters are provided in section 5. 1992; Lazaridou, Peysakhovich, and Baroni 2016) or relax- ing the discrete communication with continuous approxi- mations such as the Gumbel-softmax (Jang, Gu, and Poole 2016; Havrylov and Titov 2017). The RL approach main- tains discreteness, but systems optimized with the Gumbel- softmax typically perform better in this setting. However, Gumbel-softmax training is effectively done with continu- ous communication. Both discretization approaches perform far worse than a system with fully continuous communica- tion. In short, the more discrete the channel, the worse the system\u2019s performance. In this work, we propose a new framework for discrete communication in multi-agent systems, based on quantiza- tion (Figure 1, top). Drawing inspiration from work on ef-arXiv:2211.02412v2 [cs.AI] 19 Jan 2023 \ufb01cient neural network quantization during training and in- ference (Banner et al. 2018; Wang et al. 2018; Choi et al. 2018)), we quantize the message delivered between the agents. We investigate two learning setups: First, training is done with continuous communication, while inference is discretized by quantization, similar to the common sce- nario when using continuous approximations like Gumbel- softmax. Second, we investigate the effects of quantizing the messages during both training and inference. We experimentally validate our approach in multiple sce- narios. We consider three different games that fall into the well-known design of referential games, where a sender transmits information about a target object, which a receiver needs to identify (Lewis 2008; Lazaridou, Peysakhovich, and Baroni 2016; Choi, Lazaridou, and De Freitas 2018; Guo et al. 2019). Our objects include synthetic discrete ob- jects, images, and texts. We also experiment with a vari- ant, which we call the classi\ufb01cation game, where the re- ceiver needs to identify the class to which the object belongs. In all cases, we \ufb01nd our quantized communication to out- perform the standard approach using Gumbel-softmax by a large margin, often even approaching the performance with fully continuous communication (Figure 1, bottom). Finally, we investigate the quantized communication by varying the granularity of quantization. This allows us to cover a much wider range of discreteness levels than has previously been possible. We analyze which aspects of the communication channel are most important for accomplish- ing the agents\u2019 task and how they affect the resulting",
    "conclusion": "s\nResearch on emergent communication between arti\ufb01cial\nagents strives for discrete communication. However, com-\nmon methods such as continuous relaxations via Gumbel-\nsoftmax lag far behind continuous communication in terms\nof performance on the agents\u2019 task. In this work we propose\nan alternative approach that achieves discrete communica-\ntion via message quantization, while enabling simple end-\nto-end training. We show that our quantized communication\nallows us to run the gamut from continuous to discrete com-\nmunication by controlling the quantization level, namely, the\nsize of the used alphabet and the word length. When apply-\ning quantization we observe extremely good results, even for\nthe smallest possible alphabet size, given long enough word\nlength.\nFuture work may explore more elaborate quantization\nschemes for message discretization, during either training or\ninference. We believe that the quantization approach offers a\ngood test bed for investigating emergent communication in\nmulti-agent systems."
  },
  {
    "id": "2403.11958v1",
    "title": "Language Evolution with Deep Learning",
    "authors": "Mathieu Rita, Paul Michel, Rahma Chaabouni, Olivier Pietquin, Emmanuel Dupoux, Florian Strub",
    "published": "2024-03-18",
    "category": "cs.CL",
    "arxiv_url": "http://arxiv.org/abs/2403.11958v1",
    "pdf_url": "https://arxiv.org/pdf/2403.11958v1",
    "abstract": "Computational modeling plays an essential role in the study of language emergence. It aims\nto simulate the conditions and learning processes that could trigger the emergence of a struc-\ntured language within a simulated controlled environment. Several methods have been used to\ninvestigate the origin of our language, including agent-based systems, Bayesian agents, genetic\nalgorithms, and rule-based systems. This chapter explores another class of computational mod-\nels that have recently revolutionized the field of machine learning: deep learning models. The\nchapter introduces the basic concepts of deep and reinforcement learning methods and sum-\nmarizes their helpfulness for simulating language emergence. It also discusses the key findings,\nlimitations, and recent attempts to build realistic simulations. This chapter targets linguists\nand cognitive scientists seeking an",
    "introduction": "tempting idea has been to reproduce experimentally the process of language emergence in either humans or computational models (Steels, 1997; Myers-Scotton, 2002; Kirby, 2002). Experimental paradigms with humans (Kirby et al., 2008; Raviv et al., 2019; Motamedi et al., 2019) have produced significant insights into language evolution. Still, their scope is limited due to the inability to replicate key aspects of language evolution, such as communication within and across large populations and the study of long evolutionary timescales. Computer modeling can help overcome these limitations and has played a prominent role in studying language evolution for a long time (Lieberman and Crelin, 1971). In particular, agent-based modeling has been used from \u2217Corresponding Author: mathieu.rita@inria.fr 1arXiv:2403.11958v1 [cs.CL] 18 Mar 2024 the early days of the language evolution research \u201crenaissance\u201d (Hurford, 1989; Steels, 1995) and is still a very active and influential field (Reali and Griffiths, 2009; 2010; Smith et al., 2003; Vogt, 2009; Gong et al., 2014; Ke et al., 2008; Brace et al., 2015; Cuskley et al., 2017; Kirby et al., 2015). Meanwhile, inthelastdecade, thefieldofmachinelearninghasrapidlydevelopedwiththeadvent of deep learning. Deep neural networks have achieved human-level performance in various domains, including image recognition (He et al., 2016; Chen et al., 2020), natural language processing (Devlin et al., 2018; Brown et al., 2020), automatic translation (Bahdanau et al., 2014; Vaswani et al., 2017), and reinforcement learning (Silver et al., 2016). This chapter aims to introduce the technical and conceptual background required for using deep learning to simulate language evolution, that is, to simulate both the emergence of communication in evolutionary timescales and patterns of language change in historical timescales (Kottur et al., 2017; Lazaridou et al., 2018; Lazaridou and Baroni, 2020) First, we present how to implement a communication game (Sec. 2), including formalizing it as a machine learning problem (Sec. 2.1), designing neural network agents (Sec. 2.2) and making agents learn to solve the game (Sec. 2.3). Second, we examine the Visual Discrimination Game (Lewis, 1969) as a case study (Sec. 3), which has been widely explored in neural emergent communication research. Finally, we provide an overview of recent emergent communication simulations with neural networks, highlighting the successes, limitations, and future challenges (Sec. 4). 2 Designing communication games with Deep Learning Communication games (Lewis, 1969; Steels, 1995; Baronchelli et al., 2010) are a framework used to investigatehowperceptual,interactive,orenvironmentalpressuresshapetheemergenceofstructured communicationprotocols(Kirbyetal.,2008;Cuskleyetal.,2017;Ravivetal.,2019). Thisframework hasprimarilybeenstudiedoverthepast 50yearsandisstilloneoftheleadingsimulationframeworks in language evolution. See Chapter Communication games: Modelling language evolution through dyadic interaction for more details. This section presents how to simulate communication games using Deep Learning. First, we frame the communication game as a multi-agent problem, where each agent is represented by a deep neural network (Sec. 2.1). Second, we define communicative agents (Sec. 2.2). Third, we use machine learning optimization to train agents to solve the communication game (Sec. 2.3). 2.1 Framing communication games as a machine learning problem 2.1.1 Machine learning is well suited for simulating communication games Mitchell (1997) defines machine learning as follows: \u201cA computer program fis said to learn from an experience Ewith respect to",
    "conclusion": "2007. 21 John Lyons. Semantics: Volume 2 , volume 2. Cambridge university press, 1977. 21 Shunyu Yao, Mo Yu, Yang Zhang, Karthik R Narasimhan, Joshua B Tenenbaum, and Chuang Gan. Linking emergent and natural languages via corpus transfer. arXiv preprint arXiv:2203.13344 , 2022. 22 Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980 , 2020. 22 Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548 , 2022. 22 Ruth-Ann Armstrong, John Hewitt, and Christopher Manning. Jampatoisnli: A jamaican patois natural language inference dataset. arXiv preprint arXiv:2212.03419 , 2022. 22 Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In International Conference on Machine Learning , pages 6437\u20136447. PMLR, 2020. 22 Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train- ing compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022b. 22 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. 22 Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harm- lessness from ai feedback. arXiv preprint arXiv:2212.08073 , 2022. 22 Marco Baroni, Roberto Dess\u00ec, and Angeliki Lazaridou. Emergent language-based coordination in deep multi-agent systems. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts , pages 11\u201316, 2022. 22 30"
  },
  {
    "id": "2204.12982v1",
    "title": "On the role of population heterogeneity in emergent communication",
    "authors": "Mathieu Rita, Florian Strub, Jean-Bastien Grill, Olivier Pietquin, Emmanuel Dupoux",
    "published": "2022-04-27",
    "category": "cs.MA",
    "arxiv_url": "http://arxiv.org/abs/2204.12982v1",
    "pdf_url": "https://arxiv.org/pdf/2204.12982v1",
    "abstract": "Published as a conference paper at ICLR 2022 ON THE ROLE OF POPULATION HETEROGENEITY IN EMERGENT COMMUNICATION Mathieu Rita INRIA, Paris mathieu.rita@inria.frFlorian Strub DeepMind fstrub@deepmind.comJean-Bastien Grill DeepMind jbgrill@deepmind.com Olivier Pietquin Google Research, Brain Team pietquin@google.comEmmanuel Dupoux EHESS,ENS-PSL,CNRS,INRIA Meta AI Research emmanuel.dupoux@gmail.com ABSTRACT Populations have often been perceived as a structuring component for language to emerge and evolve: the larger the population, the more structured the language. While this observation is widespread in the sociolinguistic literature, it has not been consistently reproduced in computer simulations with neural agents. In this paper, we thus aim to clarify this apparent contradiction. We explore emergent language properties by varying agent population size in the speaker-listener Lewis Game. After reproducing the experimental difference, we challenge the simula- tion assumption that the agent community is homogeneous. We then investigate how speaker-listener asymmetry alters language structure through the analysis a potential diversity factor: learning speed. From then, we leverage this observa- tion to control population heterogeneity without introducing confounding factors. We \ufb01nally show that introducing such training speed heterogeneities naturally sort out the initial contradiction: larger simulated communities start developing more stable and structured languages. 1 I NTRODUCTION Language emergence has been explored in linguistics and arti\ufb01cial intelligence for two main rea- sons (Lazaridou & Baroni, 2020). On the one hand, arti\ufb01cially reproducing language emergence may help to understand the evolution of human languages (Steels, 1997; Briscoe, 2002; Wagner et al., 2003). On the other hand, language is known to be structured, and compositional (Bickerton, 2007), and imitating such properties would enhance machine learning representations. As a result, there exists a constant back and forth between cognitive sciences, linguistics, and arti\ufb01cial intel- ligence to retrieve the core ingredients of language emergence (Kirby et al., 2008). In this paper, we explore how the size of a population may",
    "introduction": "impact the structure of emerging languages by using neural reinforcement learning methods. Especially, we explore the following socio-linguistic hypothesis: larger communities create more systematic languages (Raviv et al., 2019a; 2020). This hypothesis has been supported by a number of ethnographic (Gary Lupyan, 2010) and socio-linguistics (Raviv et al., 2020) observations as well as behavioral studies mimicking language emergence in a controlled setup (Raviv et al., 2019a). Few neural language emergence papers have explored how community size impacts language structure so far, but the available evidence is mitigated at best. Tieleman et al. (2019) observed a small but consistent regularization effect when pairing auto-encoders within a population. Similarly, Cogswell et al. (2019) observed slight improvements in language compositionality with a large population but only reported them in few experimental settings. Finally, Graesser et al. (2019) studied the impact of contact-agents for different population sizes, but they did not observe a correlation between the population size and the convergence speed, success rate, or mutual agent intelligibility. The following question arises: why does community size not improve language properties in recent emergent communication literature, although it is a key structuring factor in broader linguistics lit- 1arXiv:2204.12982v1 [cs.MA] 27 Apr 2022 Published as a conference paper at ICLR 2022 erature? We argue that recent emergent communication models are limited as they ignore individual learning capacities by working only with homogeneous populations. Consequently, they miss cou- pling effects emerging from agents\u2019 asymmetries. As a result, we hypothesize that community size effects could occur as soon as local heterogeneities are introduced into populations. In this work, we explore the effects of population size with neural agents in the well-known Lewis referential game (Lewis, 1969). In this game, a speaker describes a hidden object to a listener, which must then reconstruct object properties. Both agents thus need to co-develop a communication pro- tocol to solve the task. The population-based variant of this game randomly pairs one speaker and one listener from their respective communities. The goal is to observe whether increasing the num- ber of agents enhances the communication protocol qualities, e.g. success rate, compositionality, generalization etc. (Kottur et al., 2017; Chaabouni et al., 2020; Lazaridou et al., 2018). Firstly, we reproduce Lewis reconstruction setting and con\ufb01rm the experimental difference: when increasing the number of agents, we do not observe improvements over various emergent language metrics. We thus question the current paradigm to model population in the language emergence literature. In particular, all agents are trained uniformly, i.e., their learning speed, capacity, sampling are identical (Tieleman et al., 2019; Cogswell et al., 2019; Fitzgerald, 2019). Secondly, we evaluate the impact of a potential source of model heterogeneity: agents learning speed. We observe that the absolute value of speaker-listener speed is not important, yet their relative value is crucial. We hence shed light on the strong correlation between language structures and agents relative training facilities. Thirdly, we push this reasoning further by distributing learning speeds across the pop- ulation thus creating heterogeneous populations. We there observe an improvement",
    "conclusion": "how heterogeneous populations may be leveraged to structure further the language (e.g.\nmore complex tasks, larger population). Finally, training speed is a natural controlling parameter for\ncomputational models, but it is unclear how it may relate to human behavior. Overall, we hope that\nthis paper provides new insights toward modeling populations in language emergence, and it is part\nof this constant back and forth between cognitive sciences, linguistics, and arti\ufb01cial intelligence.\n9\nPublished as a conference paper at ICLR 2022"
  },
  {
    "id": "2306.03830v1",
    "title": "Inductive Bias for Emergent Communication in a Continuous Setting",
    "authors": "John Isak Fjellvang Villanger, Troels Arnfred Bojesen",
    "published": "2023-06-06",
    "category": "cs.LG",
    "arxiv_url": "http://arxiv.org/abs/2306.03830v1",
    "pdf_url": "https://arxiv.org/pdf/2306.03830v1",
    "abstract": "We study emergent communication in a multi-agent reinforcement learning setting,\nwhere the agents solve cooperative tasks and have access to a communication\nchannel. The communication channel may consist of either discrete symbols or\ncontinuous variables. We introduce an inductive bias to aid with the emergence of\ngood communication protocols for continuous messages, and we look at the effect\nthis type of inductive bias has for continuous and discrete messages in itself or\nwhen used in combination with reinforcement learning. We demonstrate that this\ntype of inductive bias has a beneficial effect on the communication protocols learnt\nin two toy environments, Negotiation and Sequence Guess.\n1",
    "introduction": "In the case of positive signaling, a speaker is incentivized to produce different messages from different observations. While for positive listening a listener is incentivized to produce different actions from different messages. Overall, this ensures an improved use of the communication bandwidth and a reduced chance of not acting based upon messages. In the works by Eccles et al. [3], this scheme is explored for when the messages passed between the agents are built from discrete symbols. An orthogonal approach to improving the stability of learning to communicate is to allow for the gradient signal to flow through the communication channel. Doing so shifts the problem from a decentralized towards a centralized training paradigm, which helps at alleviating some of the issues plaguing the former[ 5]. The cost is that the gradient information needs to be available and passed between the agents while training, which may or may not be viable in a given setting. In this work, we look at how MARL agents learn to form a \u201cshared language\u201d in order to solve cooperative tasks. Using two toy examples, a variant of Negotiation [ 2,1,9,6], and a new game we call Sequence Guess, we demonstrate how the positive signaling ideas can be extended to continuous Preprint. Under review.arXiv:2306.03830v1 [cs.LG] 6 Jun 2023 communication protocols, where the discrete symbols are replaced with real numbers. We estimate the effect of continuous positive signaling on differentiable communication protocols [ 19,15,10,23], reinforced communication protocols [ 4], and a combination of both. The effect of a continuous communication protocol is also compared to the effect of a discrete one, in the otherwise discrete game of Sequence Guess. 2 Positive Signaling We write the total loss function for a communicating MARL agent as L=Lrest+Lcomm, where the latter term is associated with communication and the former with other actions. The communication loss may be further subdivided into Lcomm=LRC+\u03bbIBLIB (1) whereLRCis the loss associated with the communication policy, LIBis an inductive bias, and \u03bbIB>0 is a weighting factor. We will focus on Lcomm from here on. Positive signaling is equivalent to maximizing the entropy of the average message policy, while at the same time minimizing the entropy of the message policy when conditioned upon a trajectory. We denote the policy for selecting message mgiven a trajectory xas\u03c0(m|x), and the average message policy, weighted by frequency, as\u03c0(m) =Ex\u223c\u03c0[\u03c0(m|x)].\u03c0can be estimated from a mini-batch of Btrajectories by \u03c0(m)\u22481 BBX b\u03c0(m|xb), (2) where the subscript blabels the mini-batch members. Then, a natural inductive biaswhich encourages positive signaling would be LIB(\u03c0, x) =\u2212H(\u03c0) +\u03bbPSH(\u03c0(\u00b7|x)) (3) =X m\u2208M\u03c0(m) ln(\u03c0(m))\u2212\u03bbPS\u03c0(m|x) ln(\u03c0(m|x)), where Mis the set of all possible messages and \u03bbPS>0is a weighting factor. In practice, however, minimizing the entropy when it is conditioned upon the current trajectory does not work well. One reason for this may be that for any c <log 2 the space of policies with entropy at mostcis disconnected[ 3], in that the minimal possible entropy during a gradual message policy shift from an old to a new most",
    "conclusion": "the choice of hyperparameters is they are of the same magnitude as the ones used by Cao et al. [1]. Note that weight decay with DM fails to produce any convergence, the most likely reason beings that the gradient signal in the initial correlations giving rise to communication is too weak compared to the gradient signal of weight decay. C.2.2 Continuous messages For continuous messages the architecture is the same except for that mastermind employs only the encoder part, where the final hidden state is used in a fully connected layer of size 100, then a ReLU activation and another fully connected layer, the output from this layer is used in the same manner as in Negotiation in order to generate the message. The guesser employs only the decoder part for continuous messages, as messages are of the same type as those used for Negotiation. Figure 5: The encoder-decoder architecture used for DM Sequence Guess. In the case of the mastermind input xtwill contain symbol number tfrom the guess sequence and target sequence, T1will be the length of the target sequence and T2will be the length of the message sequence. The output ytis used in a fully connected layer with a Softmax activation function in order to find message symbol number t.In the case of the guesser input xtwill contain symbol number tfrom the message, T1will be the length of the message sequence and T2will be the length of the target sequence. yt is used in a fully connected layer with a Softmax activation function in order to find guess symbol number t. In both cases a one-hot encoding of the current turn is appended to the final hidden state of the Encoder in order to produce the context vector. Table 3 shows the hyperparameters used in Sequence Guess. 12"
  },
  {
    "id": "2108.01828v3",
    "title": "Emergent Discrete Communication in Semantic Spaces",
    "authors": "Mycal Tucker, Huao Li, Siddharth Agrawal, Dana Hughes, Katia Sycara, Michael Lewis, Julie Shah",
    "published": "2021-08-04",
    "category": "cs.LG",
    "arxiv_url": "http://arxiv.org/abs/2108.01828v3",
    "pdf_url": "https://arxiv.org/pdf/2108.01828v3",
    "abstract": "Neural agents trained in reinforcement learning settings can learn to communicate\namong themselves via discrete tokens, accomplishing as a team what agents would\nbe unable to do alone. However, the current standard of using one-hot vectors\nas discrete communication tokens prevents agents from acquiring more desirable\naspects of communication such as zero-shot understanding. Inspired by word\nembedding techniques from natural language processing, we propose neural agent\narchitectures that enables them to communicate via discrete tokens derived from a\nlearned, continuous space. We show in a decision theoretic framework that our tech-\nnique optimizes communication over a wide range of scenarios, whereas one-hot\ntokens are only optimal under restrictive assumptions. In self-play experiments, we\nvalidate that our trained agents learn to cluster tokens in semantically-meaningful\nways, allowing them communicate in noisy environments where other techniques\nfail. Lastly, we demonstrate both that agents using our method can effectively\nrespond to novel human communication and that humans can understand unlabeled\nemergent agent communication, outperforming the use of one-hot communication.\n1",
    "introduction": "by forcing agents to broadcast one-hot vectors [ 9,20]. These tokens in effect become a lexicon used by agents. Studying when these tokens are emitted allows researchers to uncover their meanings, as well as to study the broader questions of what environment or agent factors contribute to desirable aspects of learned communication (e.g., compositionality or, in continuous communication settings, zero-shot understanding) [15, 19, 5, 4]. We claim that discretizing messages by constraining them to conform to one-hot vectors fundamentally precludes agents from learning some desirable properties of language. One-hot vectors establish no relationships between tokens because each one-hot vector is orthogonal to and equally far away from all other vectors. Conversely, research from natural language processing and word embeddings has long established the importance of learning representations of discrete words within a continuous, semantic space [29, 32].arXiv:2108.01828v3 [cs.LG] 4 Nov 2021 In this work, we demonstrate the bene\ufb01t of agents that employ a discrete set of tokens within a continuous space over agents that use the standard practice of communicating via one-hot vectors in discrete emergent communication settings. We present a novel architecture and implementation for learning such communication and provide decision-theoretic analysis of the value of such an approach - the congruence of meaning and form of communications. Simulation experiments con\ufb01rmed these results: our agents learned an arrangement of tokens that clustered in human-understandable patterns. The arrangement of discrete tokens within the learned communication space produced team performance that was robust to environment noise and enabled agents to effectively utilize novel communication vectors. In human-agent experiments, agents aligned their tokens with natural language embeddings and responded appropriately to novel English phrases. Lastly, we showed that humans capably interpreted unlabeled emergent communication tokens in a reference game.1 2 Related Work We propose a technique within emergent communication literature, drawing inspiration from work on word embeddings in natural language processing (NLP) and zero-shot classi\ufb01cation. 2.1 Emergent Communication Researchers of emergent communication study techniques to enable agents to learn to communicate among themselves, enabling high task performance in reinforcement learning settings (see [ 36,10,18], among others). These settings, such as reference games or Lewis signalling games [ 25], are designed such that agents must communicate to perform the task successfully; in \u201ccheap-talk\u201d scenarios, agents often learn successful communication strategies by sending real-valued vectors to each other. We focus on discrete communication emerging among decentralized agents that may communicate by sending one of a \ufb01nite set of vectors to each other, but may not access other agents\u2019 weights or gradients during training or execution [ 8]. Previously, such discrete communication has often taken the form of one-hot messages: Foerster et al. [ 9] proposed binary discrete messages, but subsequent works seem to have reverted to one-hot vectors [15, 20, 11, 23, 6]. Even when agents learn to communicate, they often fail to learn a protocol that humans or separately- trained agents can understand; that is, they fail at the \u201czero-shot\u201d learning problem [12]. To address this gap, several recent works have found properties",
    "conclusion": "on 80 examples of each of 4 animal-vehicle image pairs. After removing tasks that were completed in extremely short or long time (out of 3 standard deviations), we were left with 2093 valid samples from 253 unique workers. Participants were paid $0:03for each task; the average completion time was 58:9seconds, equivalent to a$1:84hourly wage. The instruction and user interface of this experiment is shown in Fig. 8. Communications were presented in a 2D plane with 8 labeled nodes and one unlabeled communication node. (Labels were generated by evaluating agents in self-play and selecting the most likely token for each class.) Participants were asked to select one out of two images that the communication node most likely referred to. Both images are from the held-out classes, meaning there were no labeled nodes for the image classes, in order to test zero-shot understanding in human-agent teams. As shown in Table 8, 4 out of the 5 trained models using our prototype-based method outperformed the BERT-based embeddings. The difference between two methods is marginally signi\ufb01cant ( 2(1;N= 2093) = 3:21;p=:073). Further inspection of Proto-1, the \ufb01rst trained model that exhibits random-chance performance, revealed that the model failed to converge to high reward in training, and that the tokens for vehicles and animals failed to separate. Training instability is a chronic problem in MARL [ 8], so we consider this failure as a symptom of general dif\ufb01culties with reinforcement learning rather than our technique speci\ufb01cally. Nevertheless, in four of our \ufb01ve trials, models did converge to high performance, and if one discards results from Proto-1 as outliers, we signi\ufb01cantly outperformed BERT embeddings ( 2(1;N= 1799) = 16 :15;p<: 0001 ). 21 Figure 7: Instructions and user interface of AMT data-gathering experiment. 22 Figure 8: Instructions and user interface of AMT communication interpretation experiment. 23"
  }
]