[
  {
    "id": "2409.02645v2",
    "title": "Emergent Language: A Survey and Taxonomy",
    "authors": "Jannik Peters, Constantin Waubert de Puiseau, Hasan Tercan, Arya Gopikrishnan, Gustavo Adolpho Lucas De Carvalho, Christian Bitter, Tobias Meisen",
    "published": "2024-09-04",
    "category": "cs.MA",
    "arxiv_url": "http://arxiv.org/abs/2409.02645v2",
    "pdf_url": "https://arxiv.org/pdf/2409.02645v2",
    "abstract": "The field of emergent language represents a novel area of research within the domain of\nartificial intelligence, particularly within the context of multi-agent reinforcement learning.\nAlthough the concept of studying language emergence is not new, early approaches were pri-\nmarily concerned with explaining human language formation, with little consideration given\nto its potential utility for artificial agents. In contrast, studies based on reinforcement learning\naim to develop communicative capabilities in agents that are comparable to or even superior\nto human language. Thus, they extend beyond the learned statistical representations that are\ncommon in natural language processing research. This gives rise to a number of fundamen-\ntal questions, from the prerequisites for language emergence to the criteria for measuring its\nsuccess. This paper addresses these questions by providing a comprehensive review of 181 sci-\nentific publications on emergent language in artificial intelligence. Its objective is to serve as a\nreference for researchers interested in or proficient in the field. Consequently, the main contri-\nbutions are the definition and overview of the prevailing terminology, the analysis of existing\nevaluation methods and metrics, and the description of the identified research gaps.\nKeywords: emergent language, emergent communication, artificial intelligence, reinforcement\nlearning, multi-agent\n1arXiv:2409.02645v2  [cs.MA]  7 Mar 2025\n1",
    "introduction": "and overview of the EL field before 2021, however, it is mostly a summary\nof previous work and does not provide a taxonomy or review of existing metrics in the field as we\ndo.[58]focusesoncommoncharacteristicsinECresearchandthedevelopmentofemergenthuman-\nmachine communication strategies. They discuss distinctions and connections of EC research to\nlinguistics, cognitive science, computer science, and sociology, while we focus on emergent language\nand its analysis. We describe and discuss all relevant surveys in more detail in Section 3.\nBasedonthispreliminarywork,thecurrentstateofresearchonELmissesanoverarchingreview\nand a comprehensive compilation and alignment of proposed quantification and comparability",
    "conclusion": "In: Workshop on Reincarnating Reinforcement Learning at ICLR 2023 (2023). http://arxiv.org/pdf/2301.12050v2 [264] Sharma, A., Rao, Sudha and Brockett, Chris and Malhotra, Akanksha and Jojic, Nebojsa and Dolan, Bill: Investigating agency of llms in human-ai collaboration tasks. In: Graham, Y., Purver, M. (eds.) Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1968\u20131987. Associ- ation for Computational Linguistics, St. Julian\u2019s, Malta (2024). https://aclanthology.org/2 024.eacl-long.119 [265] Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y.,Zhao,W.X.,Wei,Z.,Wen,J.:Asurveyonlargelanguagemodelbasedautonomousagents. Frontiers of Computer Science 18(6) (2024) https://doi.org/10.1007/s11704-024-40231-1 [266] Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J.D., Chen, D., Arora, S.: Fine-tuning language models with just forward passes. In: Neural Information Processing Systems Foun- dation (ed.) Advances in Neural Information Processing Systems 36. Advances in neural information processing systems (2023). https://openreview.net/forum?id=Vota6rFhBQ [267] Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Chan, C.-M., Yu, H., Lu, Y., Hung, Y.-H., Qian, C., Qin, Y., Cong, X., Xie, R., Liu, Z., Sun, M., Zhou, J.: Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In: OpenReview.net (ed.) 12th International Conference on Learning Representations (2024). https://openreview.net/for um?id=EHg5GDnyq1 [268] Guo, T., Chen, X., Wang, Y., Chang, R., Pei, S., Chawla, N.V., Wiest, O., Zhang, X.: Large languagemodelbasedmulti-agents:Asurveyofprogressandchallenges.In:KateLarson(ed.) Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, pp. 8048\u20138057 (2024). https://doi.org/10.24963/ijcai.2024/890 [269] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, Thomas Griffiths: Cognitive architec- tures for language agents. Transactions on Machine Learning Research (2024) [270] Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., Yao, S.: Reflexion: Lan- guage agents with verbal reinforcement learning. In: Neural Information Processing Systems Foundation (ed.) Advances in Neural Information Processing Systems 36. Advances in neu- ral information processing systems (2023). https://papers.nips.cc/paper_files/paper/2023/ file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf 78"
  },
  {
    "id": "2406.07277v2",
    "title": "Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication",
    "authors": "Olaf Lipinski, Adam J. Sobey, Federico Cerutti, Timothy J. Norman",
    "published": "2024-06-11",
    "category": "cs.CL",
    "arxiv_url": "http://arxiv.org/abs/2406.07277v2",
    "pdf_url": "https://arxiv.org/pdf/2406.07277v2",
    "abstract": "Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication Olaf Lipinski1\u2217Adam J. Sobey2,1Federico Cerutti3Timothy J. Norman1 1University of Southampton2The Alan Turing Institute3University of Brescia {o.lipinski,t.j.norman}@soton.ac.uk asobey@turing.ac.uk federico.cerutti@unibs.it Abstract Effective communication requires the ability to refer to specific parts of an ob- servation in relation to others. While emergent communication literature shows success in developing various language properties, no research has shown the emergence of such positional references. This paper demonstrates how agents can communicate about spatial relationships within their observations. The results indicate that agents can develop a language capable of expressing the relationships between parts of their observation, achieving over 90% accuracy when trained in a referential game which requires such communication. Using a collocation measure, we demonstrate how the agents create such references. This analysis suggests that agents use a mixture of non-compositional and compositional messages to convey spatial relationships. We also show that the emergent language is interpretable by humans. The translation accuracy is tested by communicating with the receiver agent, where the receiver achieves over 78% accuracy using parts of this lexicon, confirming that the interpretation of the emergent language was successful. 1 Spatial referencing in emergent communication Emergent communication allows agents to develop bespoke languages for their environment. While there are many successful examples of efficient (Rita et al., 2020) and compositional (Chaabouni et al., 2020) languages, they often lack fundamental aspects seen in human language, such as syntax (Lazaridou and Baroni, 2020) or recursion (Baroni, 2020). It is argued that these aspects of communication are important to improve the efficiency and generalisability of emergent languages (Baroni, 2020; Boldt and Mortensen, 2024; Rita et al., 2024). However, the current architectures, environments, and reward schemes are yet to exhibit such fundamental properties. One such aspect is the development of deixis (Rita et al., 2024), which has",
    "introduction": "been described as a way of pointing through language. Examples of temporal deixis include words such as \u201cyesterday\u201d or \u201cbefore,\u201d and spatial deixis include words such as \u201chere\u201d or \u201cnext to\u201d (Lyons, 1977). In emergent communication, Lipinski et al. (2023) investigate how agents may refer to repeating observations, which could also be viewed from the linguistic perspective as investigating temporal deixis . However, while there are advocates to investigate how emergent languages can develop key concepts from human language (Rita et al., 2024), no work has demonstrated the emergence of relative references to specific locations within an observation, or spatial deixis . Spatial references would be valuable in establishing shared context between agents, increasing com- munication efficiency by reducing the need for detailed descriptions, and adaptability, by removing the need for unique references per object. For example, instead of describing a new, previously \u2217Corresponding author: o.lipinski@soton.ac.uk 38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2406.07277v2 [cs.CL] 28 Oct 2024 unseen object, such as \u201ca blue vase with intricate motifs on the table,\u201d one could simply use spatial relationships and say \u201cthe object left of the plate.\u201d Spatial referencing streamlines communication by leveraging the shared environment as a reference point. In dynamic environments where objects might change positions, spatial references enable agents to easily track and refer to objects without having to update their descriptions. This enhances communication efficiency and improves interaction and collaboration between agents. These elements may also help the evolved language become human interpretable, allowing the development of trustworthy emergent communication (Lazaridou and Baroni, 2020; Mu and Goodman, 2021). This paper therefore explores how agents can develop communication with spatial references. While Rita et al. (2024) posit that the emergence of these references might require complex settings, we show that even agents trained in a modified version of the simple referential game (Lazaridou et al., 2018; Lewis, 1969) can develop spatial references.2This resulting language is segmented and analysed using a collocation measure, Normalised Pointwise Mutual Information (NPMI) adapted from computational linguistics. NPMI allows us to measure the strength of associations between message parts and their context, making it a valuable tool for gaining insights into the underlying structure of the emergent language. Using NPMI, we show how the agents compose such spatial references, providing the first hint of a syntactic structure, and showing that the emergent language can be interpreted by humans. 2 Development of a spatial referential game Current emergent communication environments have not produced languages incorporating spatial references. To address this, we present a referential game (Lazaridou et al., 2018) environment where an effective language requires communication about spatial relationships. 2.1 Referential game environment In the referential game, there are two agents, a sender and a receiver. The sender observes a vector and transmits its compressed representation through a discrete channel to the receiver. The receiver observes a set of vectors and the sender\u2019s message. One of these vectors is the same as the one the sender has observed. The receiver\u2019s goal is to correctly identify",
    "conclusion": "Recent work in the field of emergent communication has advocated for better alignment of emergent\nlanguages with natural language (Boldt and Mortensen, 2024; Rita et al., 2024), such as through\nthe investigation of deixis (Rita et al., 2024). Aligned to this approach, we provide a first reported\nemergent language containing spatial references (Lyons, 1977), together with a method to interpret\nthe agents\u2019 messages in natural language. We show that agents can learn to communicate about spatial\nrelationships with over 90% accuracy. We identify both compositional and non-compositional spatial\nreferencing, showing that the agents use a mixture of both. We hypothesise why the agents choose\nnon-compositional representations of observation types which are sparse in the dataset, arguing\nthat this behaviour can be used to increase communicative efficiency. We show that, using the\nNPMI language analysis method, we can create a human interpretable dictionary, of the agents\u2019 own\nlanguage. We confirm that our method of language interpretation is accurate, achieving over 94%\naccuracy for certain dictionaries.\n10"
  },
  {
    "id": "2402.16247v1",
    "title": "Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition",
    "authors": "Dylan Cope, Peter McBurney",
    "published": "2024-02-26",
    "category": "cs.LG",
    "arxiv_url": "http://arxiv.org/abs/2402.16247v1",
    "pdf_url": "https://arxiv.org/pdf/2402.16247v1",
    "abstract": "In Emergent Communication (EC) agents learn to\ncommunicate with one another, but the protocols\nthat they develop are specialised to their training\ncommunity. This observation led to research into\nZero-Shot Coordination (ZSC) for learning com-\nmunication strategies that are robust to agents not\nencountered during training. However, ZSC typi-\ncally assumes that no prior data is available about\nthe agents that will be encountered in the zero-shot\nsetting. In many cases, this presents an unnecessar-\nily hard problem and rules out communication via\npreestablished conventions. We propose a novel AI\nchallenge called a Cooperative Language Acquisi-\ntion Problem (CLAP) in which the ZSC assump-\ntions are relaxed by allowing a \u2018joiner\u2019 agent to\nlearn from a dataset of interactions between agents\nin a target community. We propose and compare\ntwo methods for solving CLAPs: Imitation Learn-\ning (IL), and Emergent Communication pretrain-\ning and Translation Learning (ECTL), in which\nan agent is trained in self-play with EC and then\nlearns from the data to translate between the emer-\ngent protocol and the target community\u2019s protocol.\n1",
    "introduction": "of solving the task. As such, the mapping between meanings and messages is arbitrary, \u2217Correspondence: dylan.cope@kcl.ac.uk. DC is supported by the UKRI Centre for Doctoral Training in Safe and Trusted AI (EP- SRC Project EP/S023356/1). Preprint (under review).and any permutation of a learned protocol is equally likely to appear across different training runs (Bullard et al., 2021). The result is that the learned conventions established within a training community will be very unlikely to work with new agents, and by default, the EC trained agents will be incapable of adapting. In response to this, many researchers have become inter- ested in devising methods in which agents learn communica- tive strategies that can adapt to this Zero-Shot Coordination (ZSC) setting (Li et al., 2023; Hu et al., 2021, 2020; Os- senkopf, 2020; Cope and Schoots, 2020; Bullard et al., 2020). ZSC algorithms typically aim to successfully communicate with an unknown agent on the first encounter, without any prior information. But in many real-world settings, this is an unnecessarily challenging assumption. If someone is injured on a street in London, passing pedestrians can form an ad hoc team and aid the patient by speaking to each other in En- glish to coordinate a response. Indeed, language is arguably the most critical set of conventions that such teams can draw upon to efficiently work together. The study of artificial agents that can form ad hoc teams is known as Ad Hoc Teamwork (AHT) (Stone et al., 2010). Sim- ilarly to ZSC, most of these algorithms aim to make as few assumptions as possible about the players that an agent may form a team with. Notably, Sarratt and Jhala (2015) applied this minimalist approach to communication. Other work has relaxed this by assuming a prior known communication pro- tocol (Barrett et al., 2014; Mirsky et al., 2020). In this work, we present a novel AI challenge that we call aCooperative Language Acquisition Problem (CLAP). Here by \u2018language acquisition\u2019 we mean learning the syntax and semantics of a preexisting communication system used by a community. This class of problems is positioned between the challenges of ZSC and AHT. In a CLAP, we are given a dataset of communication events between speakers and lis- teners in a target community as they solve a problem. Our goal is to construct a joiner agent that can communicate and cooperate with agents from this community. This problem is also closely related to Imitation Learning (IL), however, most work in IL is confined to the single agent setting (Hussein et al., 2017). So to the best of our knowledge, this is the first attempt to pose an IL problem for multi-agent communication within a formal cooperative model. Alongside defining CLAP, we outline two baseline solu-arXiv:2402.16247v1 [cs.LG] 26 Feb 2024 tions to this problem. The first uses a simple imitation learn- ing method. The second is a novel algorithm called Emer- gent Communication pretraining and Translation Learning (ECTL). We introduce two environments and train target communities of agents that cooperate via",
    "conclusion": "agent was then evaluated for 500 episodes in the zero-shot CLAP-Replace setting. For Figure 5a, for each sample along the x-axis (dif- ferent Ncollect values), this entire process was run. Thus there was 9 Ncollect samples \u00d72 community permutations \u00d72 pos- sible replacement agents \u00d73 trials, totally 432 training runs. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7106\u221225\u221220\u221215\u221210\u22125Mean Reward 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7106567891011Mean Episode LengthTraining Statistics for 111b5 00000Figure 7: Training curves for one training run on the gridworld environment. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7106\u221225\u221220\u221215\u221210\u22125Mean Reward 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7106567891011Mean Episode LengthTraining Statistics for b20cd 00000 Figure 8: Training curves for one training run on the gridworld environment. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d71080200400600800100012001400Mean Reward 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7108150160170180190200Mean Episode LengthTraining Statistics for dc825 00000 Figure 9: Training curves for one training run on the driving environment, in the \u2018pit\u2019 scenario. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d71080200400600800100012001400Mean Reward 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Timesteps \u00d7108150160170180190200Mean Episode LengthTraining Statistics for de02c 00000Figure 10: Training curves for one training run on the driving environment, in the \u2018pit\u2019 scenario. 0.0 0.2 0.4 0.6 0.8 1.0 Timesteps \u00d7108250500750100012501500Mean Reward 0.0 0.2 0.4 0.6 0.8 1.0 Timesteps \u00d7108150160170180190200Mean Episode LengthTraining Statistics for 3fba1 00000 Figure 11: Training curves for one training run on the driving environment, in the \u2018no pit\u2019 scenario. 0.0 0.2 0.4 0.6 0.8 1.0 Timesteps \u00d7108500100015002000Mean Reward 0.0 0.2 0.4 0.6 0.8 1.0 Timesteps \u00d7108150160170180190200Mean Episode LengthTraining Statistics for 4a79d 00000 Figure 12: Training curves for one training run on the driving environment, in the \u2018no pit\u2019 scenario."
  },
  {
    "id": "2310.06555v2",
    "title": "It's About Time: Temporal References in Emergent Communication",
    "authors": "Olaf Lipinski, Adam J. Sobey, Federico Cerutti, Timothy J. Norman",
    "published": "2023-10-10",
    "category": "cs.CL",
    "arxiv_url": "http://arxiv.org/abs/2310.06555v2",
    "pdf_url": "https://arxiv.org/pdf/2310.06555v2",
    "abstract": "Emergent communication studies the development of language between autonomous agents,\naiming to improve understanding of natural language evolution and increase communication\nefficiency. While temporal aspects of language have been considered in computational\nlinguistics, there has been no research on temporal references in emergent communication.\nThis paper addresses this gap, by exploring how agents communicate about temporal\nrelationships. We analyse three potential influences for the emergence of temporal references:\nenvironmental, external, and architectural changes. Our experiments demonstrate that\nalteringthelossfunctionisinsufficientfortemporalreferencestoemerge; rather, architectural\nchanges are necessary. However, a minimal change in agent architecture, using a different\nbatching method, allows the emergence of temporal references. This modified design is\ncompared with the standard architecture in a temporal referential games environment,\nwhich emphasises temporal relationships. The analysis indicates that over 95% of the agents\nwith the modified batching method develop temporal references, without changes to their\nloss function. We consider temporal referencing necessary for future improvements to the\nagents\u2019 communication efficiency, yielding a closer to optimal coding as compared to purely\ncompositional languages. Our readily transferable architectural insights provide the basis\nfor their incorporation into other emergent communication settings.\nKeywords: Emergent Communication, Emergent Language, Temporal Logic, Multiagent\nSystems, Representation Learning\n1",
    "introduction": "their vocabulary size and word length to their specific task, providing an advantage over a general communication protocol (Rita et al., 2020). Many aspects of emergent language have been explored (Lazaridou and Baroni, 2020; Boldt and Mortensen, 2024), with a particular focus on improving communication efficiency \u00a92024 Olaf Lipinski, Adam J. Sobey, Federico Cerutti, Timothy J. Norman. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ . Attribution requirements are provided at http://jmlr.org/papers/v25/25-0000.html .arXiv:2310.06555v2 [cs.CL] 3 May 2024 Lipinski, Sobey, Cerutti and Norman (Rita et al., 2020; Chaabouni et al., 2019; Kang et al., 2020). Kang et al. (2020) demonstrate how using the minimal deviation between subsequent time steps allows for more concise communication by reducing redundant information transfer. Investigation of the contextual information of the resulting language offered a further improvement in agent performance by using the time step similarity together with optimisation of the reconstruction of the speaker\u2019s state (Kang et al., 2020). There is, however, no existing research investigating or reporting on the emergence of temporal referencing strategies, where agents could communicate about relationships between different time steps. Such temporal references, together with the general characteristics of emergent languages, will enhance the agent\u2019s bandwidth efficiency and task performance in a variety of situations. As environmental complexity is being scaled in emergent communication research (Chaabouni et al., 2022), temporal references will benefit agents in settings where temporal relationships are embedded. One example is social deduction games (Brandizzi et al., 2021; Lipinski et al., 2022; Kopparapu et al., 2022), where referencing past events are expected to be key to winning strategies. Temporal references will allow agents to develop more efficient methods of communication by assigning shorter messages to events that happen more often. This is similar to Zipf\u2019s Law in human languages (Zipf, 1949), which states that the most commonly used words are the shortest. Temporal references would be particularly effective when the distribution of observations would be non-uniform, which means that certain objects appear more often than others. Specialised messages, used only for temporal references, would then also become more frequent than others. From information theory, we know that (adaptive) Huffman coding (Huffman, 1952; Knuth, 1985; Vitter, 1987) can assign shorter bit sequences to more frequent messages, thereby compressing them more efficiently than less common messages. Consequently, the incorporation of temporal references can enhance the efficiency of transmitting emergent language, optimizing communication. Our contribution lies in examining when temporal references emerge between agents. Three potential prerequisites are explored: environmental pressures, external pressures and architectural changes. The agents are trained in both the regular referential game (Lazaridou et al., 2017) and on an environment which encourages the development of temporal references through embedded environmental pressures (Section 2.3). The effect of an external pressure to develop temporal referencing is explored via an additional loss applied to the agents (Section 5). Three types of architecture are evaluated, (Section 3), analysing two novel architectures together with a reference architecture based on the commonly used EGG (Kharitonov et al., 2019) agents. The baseline Base(Section 3.1) agent, provides",
    "conclusion": "Emergent communication has been studied extensively, considering many aspects of emergent\nlanguages, such as efficiency (Rita et al., 2020; Chaabouni et al., 2019), compositionality\n(Auersperger and Pecina, 2022), generalisation (Chaabouni et al., 2020) and population\ndynamics (Chaabouni et al., 2022; Rita et al., 2022a). Yet, there has been no investigation\ninto learning and communicating temporal relationships. Discussing past observations is\nvital to communication, saving bandwidth by avoiding repeating information and allowing\nfor easier experience sharing.\nThis paper provides a first exploration of such emergent languages, including addressing\nthe fundamental questions of when they could develop and what is required for their\nemergence. We present a set of environments that are designed to facilitate investigation\ninto how agents might create such references. We use the conventional agent architecture for\nemergent communication (Kharitonov et al., 2019) as a baseline and explore both temporal\nloss and alternative architectures that may endow agents with the ability to learn temporal\nrelationships. We show that architectural change is necessary for temporal references to\nemerge, and demonstrate that temporal prediction loss is neither sufficient for their emergence,\nnor does it improve the emergent language.\n17\nLipinski, Sobey, Cerutti and Norman"
  },
  {
    "id": "2209.15342v2",
    "title": "Emergent Communication: Generalization and Overfitting in Lewis Games",
    "authors": "Mathieu Rita, Corentin Tallec, Paul Michel, Jean-Bastien Grill, Olivier Pietquin, Emmanuel Dupoux, Florian Strub",
    "published": "2022-09-30",
    "category": "cs.MA",
    "arxiv_url": "http://arxiv.org/abs/2209.15342v2",
    "pdf_url": "https://arxiv.org/pdf/2209.15342v2",
    "abstract": "Lewis signaling games are a class of simple communication games for simulating\nthe emergence of language. In these games, two agents must agree on a commu-\nnication protocol in order to solve a cooperative task. Previous work has shown\nthat agents trained to play this game with reinforcement learning tend to develop\nlanguages that display undesirable properties from a linguistic point of view (lack\nof generalization, lack of compositionality, etc). In this paper, we aim to provide\nbetter understanding of this phenomenon by analytically studying the learning\nproblem in Lewis games. As a core contribution, we demonstrate that the standard\nobjective in Lewis games can be decomposed in two components: a co-adaptation\nloss and an information loss. This decomposition enables us to surface two po-\ntential sources of over\ufb01tting, which we show may undermine the emergence of a\nstructured communication protocol. In particular, when we control for over\ufb01tting\non the co-adaptation loss, we recover desired properties in the emergent languages:\nthey are more compositional and generalize better.\n1",
    "introduction": "light on the prerequisites of language emergence. In their original form, Lewis signaling games involve two agents: a speaker and a listener. The speaker observes a random state from its environment, e.g. an image, and sends a signal to the listener. The listener then undertakes an action based on this signal. Finally, both agents are equally rewarded based on the outcome of the listener\u2019s action. The resolution of this cooperative two-player game requires the emergence of a shared protocol between the agents [ 55,17]. One way to model the emergence of such protocol is to give the agents the capacity to learn. The agents, and therefore, the communication protocol, are shaped by a sequence of trials and errors over multiple games [ 81,44,75,70]. This learning-centric approach allows for a \ufb01ne analysis of the language emergence dynamics [ 70,36]. It \u0003This work was performed when Paul Michel was af\ufb01liated with Ecole Normale Sup\u00e9rieure PSL. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2209.15342v2 [cs.MA] 15 Oct 2022 also raises challenging learning-speci\ufb01c questions: What are the inductive biases present in the agent architecture and loss function that shape the emergent language [ 43]? How do agents generalize from their training set? Is the resulting language compositional [ 8]? What is the impact of over\ufb01tting [ 53]? Recently, there has been a resurgence of interest for such learning-based approaches following advances in machine learning [ 51]. In these approaches, the speakers and listeners are modeled as deep reinforcement learning agents optimized to solve instances of the Lewis games [ 53,33,65,56,28]. The vast majority of these works explore Lewis games from an empirical perspective. However, some of the recent experimental results are at odds with experimental \ufb01ndings from the linguistics literature. For instance, the emergent protocols lack interpretability [ 48], generalization does not always correlate with language compositionality [ 10], successful strategies are not naturally adopted in populations [ 68,12], and anti-ef\ufb01cient communication may even emerge [ 9]. It is unclear whether those empirical observations result from a learning failure, e.g. optimization problems, over\ufb01tting, or whether they are symptomatic of more fundamental limitations of Lewis games for modeling language emergence, e.g. lack of embodiment [ 31,4,63,37]. Overall, it is crucial to establish new analytical insight to analyze Lewis games in the learning setting. In this paper, we introduce such an analytical framework to diagnose the learning dynamics of deep reinforcement learning agents in Lewis signaling games. As a core contribution, we demonstrate under mild assumptions that the loss of the speaker and listener can be decomposed into two components when resolving Lewis signaling games: (i) an information loss that maximizes the mutual information between the observed states and speaker messages; (ii) a co-adaptation loss that aligns the speaker and listener\u2019s interpretation of the messages (Section 2). Based on this decomposition, we empirically examine the evolution of these two losses during the learning process (Section 5). In particular, we identify an over\ufb01tting problem in the co-adaptation loss between the agents which undermines the emergence of",
    "conclusion": "In this paper, we propose a methodological approach to better understand the dynamics in Lewis\nsignaling games for language emergence. It allows us to surface two components of the training:\n(i) an information loss, (ii) a co-adaptation loss. We shed light that the agents tend to over\ufb01t this\nco-adaptation term during training, which hinders the learning dynamic and degrades the resulting\nlanguage. As soon as this over\ufb01tting is controlled, agents develop compositional languages that better\ngeneralize. Remarkably, this emergent compositionality does not result from environmental factors,\ne.g. communication bottleneck [ 43], under-parametrization [ 48,26], population dynamics [ 12,68],\nmemory restriction [ 15,16] or inductive biases [ 67], but only through a trial-and-error process.\nTherefore, we advocate for a better comprehension of the optimization and machine learning issues.\nAs illustrated in this paper, such understanding may unveil contradictions between computational\nmodels and language empirical observations and better expose the existing synergies between learning\ndynamics and environmental factors [27, 83, 64, 14, 18, 22]."
  },
  {
    "id": "2206.11302v1",
    "title": "Recommendations for Systematic Research on Emergent Language",
    "authors": "Brendon Boldt, David Mortensen",
    "published": "2022-06-22",
    "category": "cs.MA",
    "arxiv_url": "http://arxiv.org/abs/2206.11302v1",
    "pdf_url": "https://arxiv.org/pdf/2206.11302v1",
    "abstract": "Emergent language is unique among \ufb01elds\nwithin the discipline of machine learning for its\nopen-endedness, not obviously presenting well-\nde\ufb01ned problems to be solved. As a result, the\ncurrent research in the \ufb01eld has largely been ex-\nploratory: focusing on establishing new prob-\nlems, techniques, and phenomena. Yet after\nthese problems have been established, subse-\nquent progress requires research which can mea-\nsurably demonstrate how it improves on prior\napproaches. This type of research is what we\ncallsystematic research ; in this paper, we illus-\ntrate this mode of research speci\ufb01cally for emer-\ngent language. We \ufb01rst identify the overarching\ngoals of emergent language research, categoriz-\ning them as either science or engineering. Us-\ning this distinction, we present core methodolog-\nical elements of science and engineering, analyze\ntheir role in current emergent language research,\nand recommend how to apply these elements.\n1.",
    "introduction": "of research is 1Language Technologies Institute, Carnegie Mellon Uni- versity, Pittsburgh, PA 15213, USA. Correspondence to: Brendon Boldt <bboldt@cs.cmu.edu>, David Mortensen <dmortens@cs.cmu.edu>.what we call systematic research . This systematicity is nec- essary for research to be effectively applied to the real pro b- lems and questions which motivate it in the \ufb01rst place. The goal of this paper is to provide concrete recommendations for moving work on emergent language toward systematic research. In order to develop these recommendations, we start in Section 2by clarifying the goals of emergent language re- search; drawing attention to how they \ufb01t into the categories of science and engineering. In Section 3we illustrate the prototypical methods of science and engineering research which facilitate systematic research. Finally, in Section 4 we analyze to what extent current emergent language re- search uses these methods and recommend how they could be more fully employed. The speci\ufb01c contributions of this work are: 1. Recommending methods which move emergent lan- guage research toward systematic research, working towards overarching goals via measurable progress. 2. Distinguishing between the capacities in which emer- gent language research can be science or engineering via the goals of a given project. 1.1. Exploratory and Systematic Research Exploratory research is research which focuses on innova- tive approaches and problems rather than pursuing a well- established goal. Exploratory research can take place ei- ther in a new \ufb01eld, such as emergent language, or on the frontier of a more established research area. The primary purpose of exploratory research is to build up a baseline level of knowledge of new approaches which differ signi\ufb01- cantly from well-studied approaches. Once a critical mass of knowledge is acquired, a more structured and systematic research program is pursued in order to advance the knowl- edge and expertise of the \ufb01eld on that topic. Systematic research is characterized by research contribu - tions which measurably demonstrates how they improve on prior approaches; this entails adhering to established r e- search problems. The long-term success of any given re- search \ufb01eld is, in fact, predicated on systematic research; the direct comparison between two approaches to the same problem is what allows for signi\ufb01cant progress to be made Recommendations for Systematic Research on Emergent Langu age in an incremental way. At times, \u201cincremental\u201d is consid- ered a negative attribute, but this is far from the case in our own usage as the incremental nature of science and engi- neering are integral to their success as disciplines. In the end, both exploratory and systematic research have a time and place where they are appropriate. The ideas of exploratory and systematic research parallel the concepts of \u201cpre-paradigmatic science\u201d and \u201cnormal science\u201d from Kuhn (1962 ). Our account, though, is in- tended to apply to science and engineering and is targeted towards practitioners of these \ufb01elds rather philosophers o f science. 2. Goals of Emergent Language Research The goals of a pursuit dictate the methods employed to achieve it. Accordingly, we begin by enumerating the over- arching goals",
    "conclusion": "The above methodological recommendations offer a way\nto move work on emergent language toward systematic re-\nsearch, where research projects build off each other in a\nway that shows measurable improvement. The exploratory\nresearch in the \ufb01eld thus far aids in recognizing the unique\nchallenges of emergent language research which is nec-\nessary to implement these recommendations. The dis-\ntinction we have made between the capacities in which\nemergent language research can be science or engineering\nis critical\u2014while the \ufb01elds are closely related and inter-\ntwined, their paths to comparable, iterative research diff er\nsigni\ufb01cantly. Moving towards systematic research on emer-\ngent language will, then, facilitate the achievement of the\nunique and important goals of this \ufb01eld."
  },
  {
    "id": "2510.18221v3",
    "title": "The Emergence of Complex Behavior in Large-Scale Ecological Environments",
    "authors": "Joseph Bejjani, Chase Van Amburg, Chengrui Wang, Chloe Huangyuan Su, Sarah M. Pratt, Yasin Mazloumi, Naeem Khoshnevis, Sham M. Kakade, Kiant\u00e9 Brantley, Aaron Walsman",
    "published": "2025-10-21",
    "category": "cs.MA",
    "arxiv_url": "http://arxiv.org/abs/2510.18221v3",
    "pdf_url": "https://arxiv.org/pdf/2510.18221v3",
    "abstract": "We explore how physical scale and population size shape the emergence of com-\nplex behaviors in open-ended ecological environments. In our setting, agents\nare unsupervised and have no explicit rewards or learning objectives but instead\nevolve over time according to reproduction, mutation, and selection. As they act,\nagents also shape their environment and the population around them in an ongoing\ndynamic ecology. Our goal is not to optimize a single high-performance policy,\nbut instead to examine how behaviors emerge and evolve across large populations\ndue to natural competition and environmental pressures. We use modern hardware\nalong with a new multi-agent simulator to scale the environment and population to\nsizes much larger than previously attempted, reaching populations of over 60,000\nagents, each with their own evolved neural network policy. We identify various\nemergent behaviors such as long-range resource extraction, vision-based foraging,\nand predation that arise under competitive and survival pressures. We examine\nhow sensing modalities and environmental scale affect the emergence of these be-\nhaviors and find that some of them appear only in sufficiently large environments\nand populations, and that larger scales increase the stability and consistency of\nthese emergent behaviors. While there is a rich history of research in evolutionary\nsettings, our scaling results on modern hardware provide promising new directions\nto explore ecology as an instrument of machine learning in an era of increasingly\nabundant computational resources and efficient machine frameworks. Experi-\nmental code is available at https://github.com/jbejjani2022/ecological-emergent-\nbehavior.\n1",
    "introduction": "across a wide variety of biological disciplines, including evolutionary theory (Darwin, 1859; Simp- son, 1944), ecology (MacArthur & Wilson, 2001), and genetics (Dobzhansky, 1937). Models of these emergent dynamics range from theories of speciation and adaptive radiation (Coyne & Orr, 2004; Schluter, 2000) to the mathematical analyses of population genetics (Fisher, 1930; Wright, 1931). In these fields, decades of mathematical modeling, laboratory experiments, and field work have tremendously improved our understanding of the mechanisms of evolution in nature. Unfortunately, there are many obstacles to studying emergent behavior in the natural world. As ecosystems become larger and more complex, they are more difficult to control and measure. Even where possible, running controlled experiments in large-scale natural settings risks damaging or displacing wild populations. \u2217Correspondence to: jbejjani@college.harvard.edu / aaronwalsman@fas.harvard.edu 1arXiv:2510.18221v3 [cs.MA] 12 Dec 2025 In an effort to better understand the emergence of complex behavior due to competitive and envi- ronmental pressures, we study the open-ended evolution of neural network policies in large-scale ecological simulations. In this setting, agents do not have a specified objective or reward signal but instead collect resources to survive and reproduce according to the dynamics of the environment. Policy changes occur only via mutation as parents pass their policies on to their children. With the rapid growth in computational resources available to researchers, our goal is to study open-ended evolutionary dynamics in populations of these simple digital organisms at scales not previously pos- sible, in the hopes of providing new tools for investigating the effects of environmental scale and population size on behavioral emergence. To study this, we use a new JAX-based simulation environment that allows rapid evaluation of large grid worlds. Our larger experiments can contain over 60,000 individual agents with a physical area of over 1,000,000 grid cells. Agents in this environment must navigate to find food and other resources in order to survive. Agents can also reproduce by collecting enough resources to make a mutated copy of themselves. A 3D visualization of this environment can be found in Figure 1. Our goal is not only to better understand ecological phenomena through simulation, but to also study ecology as a mechanism for producing machine intelligence. Recent years have demonstrated that in AI, scale doesn\u2019t merely improve models\u2014it fundamentally transforms their capabilities (Wei et al., 2022a). Yet despite this insight, many attempts at embodied learning have been often been confined to relatively small environments with low populations. Our work explores what capabilities emerge when we scale not just a single model but an entire ecosystem. Just as reasoning emerges in indi- vidual models only beyond a certain size threshold (Wei et al., 2022b), we seek to understand what strategies evolve within populations only when environmental size and complexity reach sufficient richness. Through extensive experiments, we identify multiple instances where sensor configurations and our large environmental scales strongly impact the behaviors that emerge from simulation, which can lead to long-term ecological consequences. For example, we show that agents with simple com- pass sensors can adapt to conduct",
    "conclusion": "of each attack. However when the lethality is reduced, attacking behavior becomes less useful and therefore less likely to be selected for in the agent population. Agents with lower attack strengths therefore do not become as skillful at attacking and spend more time foraging for food. Figure 22: Population and action frequency data over 2M steps in a 256\u00d7256Oceanworld for (RCV+A)agents across Full Attack, Half Attack, and Quarter Attack strength settings. Four seeds are plotted per attack strength setting. F.8 VARYINGAGENTHP We investigate the effect of reducing the starting HP of child agents and lowering the healing rate to apply pressure for agents to \u2018grow\u2019 over their lifetime in order to survive. The healing rate parameter determines how much HP an agent can regenerate per healing step, which converts available energy to HP. Agents can influence healing only by maintaining enough energy. We study two settings: 1) Full HP: child HP starts at 5 and healing rate is 1 HP per 0.1 energy 2) Low HP: child HP starts at 1, with healing rate 0.5. Max agent HP is 10 in all settings. The main experiments (Section 4) were run with the Full HP setting. Across the two settings, there is no significant difference in homicides per attack, eat action fre- quency, or biomass utilization (Figure 23). In the Full HP setting, populations grow slightly larger, while in the Low HP setting, agents attack less frequently and move slightly more. We hypothesize that agents born with lower HP must focus on foraging early on in their lifetime in order to grow to full size, which would explain this difference. 32 Figure 23: Population and action frequency data over 2M steps in the 256\u00d7256Oceanworld for (RCV+A)agents across Full HP and Low HP settings. Four seeds are plotted per HP setting. 33"
  },
  {
    "id": "2211.02412v2",
    "title": "Emergent Quantized Communication",
    "authors": "Boaz Carmeli, Ron Meir, Yonatan Belinkov",
    "published": "2022-11-04",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2211.02412v2",
    "pdf_url": "https://arxiv.org/pdf/2211.02412v2",
    "abstract": "The \ufb01eld of emergent communication aims to understand the\ncharacteristics of communication as it emerges from arti\ufb01-\ncial agents solving tasks that require information exchange.\nCommunication with discrete messages is considered a de-\nsired characteristic, for both scienti\ufb01c and applied reasons.\nHowever, training a multi-agent system with discrete commu-\nnication is not straightforward, requiring either reinforcement\nlearning algorithms or relaxing the discreteness requirement\nvia a continuous approximation such as the Gumbel-softmax.\nBoth these solutions result in poor performance compared to\nfully continuous communication. In this work, we propose\nan alternative approach to achieve discrete communication\n\u2013 quantization of communicated messages. Using message\nquantization allows us to train the model end-to-end, achiev-\ning superior performance in multiple setups. Moreover, quan-\ntization is a natural framework that runs the gamut from con-\ntinuous to discrete communication. Thus, it sets the ground\nfor a broader view of multi-agent communication in the deep\nlearning era.\n1",
    "introduction": "agent systems, which are typically trained with gradient- based optimization. Two main approaches have been pro- posed in the literature for overcoming this challenge, namely using reinforcement learning (RL) algorithms (Williams *Supported by the Viterbi Fellowship in the Center for Com- puter Engineering at the Technion. 0.10-0.230.690.72-0.54-0.210011004.02.07.07.01.02.00.10-0.230.694.01.07.0Gumbel-softmaxContinuousQuantizedWordMessageN/ASymbol0011.0Single symbolFigure 1: Top: Symbol, word, and message elements for continuous, Gumbel-softmax, and quantized communica- tion modes. Bottom : Accuracy (Y-axis) achieved by the three communication modes vs. number of candidates (X- axis), in the Object game. Continuous communication leads to good performance on the end task but does not use sym- bols. Gumbel-softmax sends one word per symbol, but re- quires a recurrent channel and does not work well in prac- tice. Quantized communication enables discrete and suc- cessful communication. Detailed channel parameters are provided in section 5. 1992; Lazaridou, Peysakhovich, and Baroni 2016) or relax- ing the discrete communication with continuous approxi- mations such as the Gumbel-softmax (Jang, Gu, and Poole 2016; Havrylov and Titov 2017). The RL approach main- tains discreteness, but systems optimized with the Gumbel- softmax typically perform better in this setting. However, Gumbel-softmax training is effectively done with continu- ous communication. Both discretization approaches perform far worse than a system with fully continuous communica- tion. In short, the more discrete the channel, the worse the system\u2019s performance. In this work, we propose a new framework for discrete communication in multi-agent systems, based on quantiza- tion (Figure 1, top). Drawing inspiration from work on ef-arXiv:2211.02412v2 [cs.AI] 19 Jan 2023 \ufb01cient neural network quantization during training and in- ference (Banner et al. 2018; Wang et al. 2018; Choi et al. 2018)), we quantize the message delivered between the agents. We investigate two learning setups: First, training is done with continuous communication, while inference is discretized by quantization, similar to the common sce- nario when using continuous approximations like Gumbel- softmax. Second, we investigate the effects of quantizing the messages during both training and inference. We experimentally validate our approach in multiple sce- narios. We consider three different games that fall into the well-known design of referential games, where a sender transmits information about a target object, which a receiver needs to identify (Lewis 2008; Lazaridou, Peysakhovich, and Baroni 2016; Choi, Lazaridou, and De Freitas 2018; Guo et al. 2019). Our objects include synthetic discrete ob- jects, images, and texts. We also experiment with a vari- ant, which we call the classi\ufb01cation game, where the re- ceiver needs to identify the class to which the object belongs. In all cases, we \ufb01nd our quantized communication to out- perform the standard approach using Gumbel-softmax by a large margin, often even approaching the performance with fully continuous communication (Figure 1, bottom). Finally, we investigate the quantized communication by varying the granularity of quantization. This allows us to cover a much wider range of discreteness levels than has previously been possible. We analyze which aspects of the communication channel are most important for accomplish- ing the agents\u2019 task and how they affect the resulting",
    "conclusion": "s\nResearch on emergent communication between arti\ufb01cial\nagents strives for discrete communication. However, com-\nmon methods such as continuous relaxations via Gumbel-\nsoftmax lag far behind continuous communication in terms\nof performance on the agents\u2019 task. In this work we propose\nan alternative approach that achieves discrete communica-\ntion via message quantization, while enabling simple end-\nto-end training. We show that our quantized communication\nallows us to run the gamut from continuous to discrete com-\nmunication by controlling the quantization level, namely, the\nsize of the used alphabet and the word length. When apply-\ning quantization we observe extremely good results, even for\nthe smallest possible alphabet size, given long enough word\nlength.\nFuture work may explore more elaborate quantization\nschemes for message discretization, during either training or\ninference. We believe that the quantization approach offers a\ngood test bed for investigating emergent communication in\nmulti-agent systems."
  },
  {
    "id": "2403.11958v1",
    "title": "Language Evolution with Deep Learning",
    "authors": "Mathieu Rita, Paul Michel, Rahma Chaabouni, Olivier Pietquin, Emmanuel Dupoux, Florian Strub",
    "published": "2024-03-18",
    "category": "cs.CL",
    "arxiv_url": "http://arxiv.org/abs/2403.11958v1",
    "pdf_url": "https://arxiv.org/pdf/2403.11958v1",
    "abstract": "Computational modeling plays an essential role in the study of language emergence. It aims\nto simulate the conditions and learning processes that could trigger the emergence of a struc-\ntured language within a simulated controlled environment. Several methods have been used to\ninvestigate the origin of our language, including agent-based systems, Bayesian agents, genetic\nalgorithms, and rule-based systems. This chapter explores another class of computational mod-\nels that have recently revolutionized the field of machine learning: deep learning models. The\nchapter introduces the basic concepts of deep and reinforcement learning methods and sum-\nmarizes their helpfulness for simulating language emergence. It also discusses the key findings,\nlimitations, and recent attempts to build realistic simulations. This chapter targets linguists\nand cognitive scientists seeking an",
    "introduction": "tempting idea has been to reproduce experimentally the process of language emergence in either humans or computational models (Steels, 1997; Myers-Scotton, 2002; Kirby, 2002). Experimental paradigms with humans (Kirby et al., 2008; Raviv et al., 2019; Motamedi et al., 2019) have produced significant insights into language evolution. Still, their scope is limited due to the inability to replicate key aspects of language evolution, such as communication within and across large populations and the study of long evolutionary timescales. Computer modeling can help overcome these limitations and has played a prominent role in studying language evolution for a long time (Lieberman and Crelin, 1971). In particular, agent-based modeling has been used from \u2217Corresponding Author: mathieu.rita@inria.fr 1arXiv:2403.11958v1 [cs.CL] 18 Mar 2024 the early days of the language evolution research \u201crenaissance\u201d (Hurford, 1989; Steels, 1995) and is still a very active and influential field (Reali and Griffiths, 2009; 2010; Smith et al., 2003; Vogt, 2009; Gong et al., 2014; Ke et al., 2008; Brace et al., 2015; Cuskley et al., 2017; Kirby et al., 2015). Meanwhile, inthelastdecade, thefieldofmachinelearninghasrapidlydevelopedwiththeadvent of deep learning. Deep neural networks have achieved human-level performance in various domains, including image recognition (He et al., 2016; Chen et al., 2020), natural language processing (Devlin et al., 2018; Brown et al., 2020), automatic translation (Bahdanau et al., 2014; Vaswani et al., 2017), and reinforcement learning (Silver et al., 2016). This chapter aims to introduce the technical and conceptual background required for using deep learning to simulate language evolution, that is, to simulate both the emergence of communication in evolutionary timescales and patterns of language change in historical timescales (Kottur et al., 2017; Lazaridou et al., 2018; Lazaridou and Baroni, 2020) First, we present how to implement a communication game (Sec. 2), including formalizing it as a machine learning problem (Sec. 2.1), designing neural network agents (Sec. 2.2) and making agents learn to solve the game (Sec. 2.3). Second, we examine the Visual Discrimination Game (Lewis, 1969) as a case study (Sec. 3), which has been widely explored in neural emergent communication research. Finally, we provide an overview of recent emergent communication simulations with neural networks, highlighting the successes, limitations, and future challenges (Sec. 4). 2 Designing communication games with Deep Learning Communication games (Lewis, 1969; Steels, 1995; Baronchelli et al., 2010) are a framework used to investigatehowperceptual,interactive,orenvironmentalpressuresshapetheemergenceofstructured communicationprotocols(Kirbyetal.,2008;Cuskleyetal.,2017;Ravivetal.,2019). Thisframework hasprimarilybeenstudiedoverthepast 50yearsandisstilloneoftheleadingsimulationframeworks in language evolution. See Chapter Communication games: Modelling language evolution through dyadic interaction for more details. This section presents how to simulate communication games using Deep Learning. First, we frame the communication game as a multi-agent problem, where each agent is represented by a deep neural network (Sec. 2.1). Second, we define communicative agents (Sec. 2.2). Third, we use machine learning optimization to train agents to solve the communication game (Sec. 2.3). 2.1 Framing communication games as a machine learning problem 2.1.1 Machine learning is well suited for simulating communication games Mitchell (1997) defines machine learning as follows: \u201cA computer program fis said to learn from an experience Ewith respect to",
    "conclusion": "2007. 21 John Lyons. Semantics: Volume 2 , volume 2. Cambridge university press, 1977. 21 Shunyu Yao, Mo Yu, Yang Zhang, Karthik R Narasimhan, Joshua B Tenenbaum, and Chuang Gan. Linking emergent and natural languages via corpus transfer. arXiv preprint arXiv:2203.13344 , 2022. 22 Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980 , 2020. 22 Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548 , 2022. 22 Ruth-Ann Armstrong, John Hewitt, and Christopher Manning. Jampatoisnli: A jamaican patois natural language inference dataset. arXiv preprint arXiv:2212.03419 , 2022. 22 Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In International Conference on Machine Learning , pages 6437\u20136447. PMLR, 2020. 22 Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train- ing compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022b. 22 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. 22 Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harm- lessness from ai feedback. arXiv preprint arXiv:2212.08073 , 2022. 22 Marco Baroni, Roberto Dess\u00ec, and Angeliki Lazaridou. Emergent language-based coordination in deep multi-agent systems. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts , pages 11\u201316, 2022. 22 30"
  },
  {
    "id": "2204.12982v1",
    "title": "On the role of population heterogeneity in emergent communication",
    "authors": "Mathieu Rita, Florian Strub, Jean-Bastien Grill, Olivier Pietquin, Emmanuel Dupoux",
    "published": "2022-04-27",
    "category": "cs.MA",
    "arxiv_url": "http://arxiv.org/abs/2204.12982v1",
    "pdf_url": "https://arxiv.org/pdf/2204.12982v1",
    "abstract": "Published as a conference paper at ICLR 2022 ON THE ROLE OF POPULATION HETEROGENEITY IN EMERGENT COMMUNICATION Mathieu Rita INRIA, Paris mathieu.rita@inria.frFlorian Strub DeepMind fstrub@deepmind.comJean-Bastien Grill DeepMind jbgrill@deepmind.com Olivier Pietquin Google Research, Brain Team pietquin@google.comEmmanuel Dupoux EHESS,ENS-PSL,CNRS,INRIA Meta AI Research emmanuel.dupoux@gmail.com ABSTRACT Populations have often been perceived as a structuring component for language to emerge and evolve: the larger the population, the more structured the language. While this observation is widespread in the sociolinguistic literature, it has not been consistently reproduced in computer simulations with neural agents. In this paper, we thus aim to clarify this apparent contradiction. We explore emergent language properties by varying agent population size in the speaker-listener Lewis Game. After reproducing the experimental difference, we challenge the simula- tion assumption that the agent community is homogeneous. We then investigate how speaker-listener asymmetry alters language structure through the analysis a potential diversity factor: learning speed. From then, we leverage this observa- tion to control population heterogeneity without introducing confounding factors. We \ufb01nally show that introducing such training speed heterogeneities naturally sort out the initial contradiction: larger simulated communities start developing more stable and structured languages. 1 I NTRODUCTION Language emergence has been explored in linguistics and arti\ufb01cial intelligence for two main rea- sons (Lazaridou & Baroni, 2020). On the one hand, arti\ufb01cially reproducing language emergence may help to understand the evolution of human languages (Steels, 1997; Briscoe, 2002; Wagner et al., 2003). On the other hand, language is known to be structured, and compositional (Bickerton, 2007), and imitating such properties would enhance machine learning representations. As a result, there exists a constant back and forth between cognitive sciences, linguistics, and arti\ufb01cial intel- ligence to retrieve the core ingredients of language emergence (Kirby et al., 2008). In this paper, we explore how the size of a population may",
    "introduction": "impact the structure of emerging languages by using neural reinforcement learning methods. Especially, we explore the following socio-linguistic hypothesis: larger communities create more systematic languages (Raviv et al., 2019a; 2020). This hypothesis has been supported by a number of ethnographic (Gary Lupyan, 2010) and socio-linguistics (Raviv et al., 2020) observations as well as behavioral studies mimicking language emergence in a controlled setup (Raviv et al., 2019a). Few neural language emergence papers have explored how community size impacts language structure so far, but the available evidence is mitigated at best. Tieleman et al. (2019) observed a small but consistent regularization effect when pairing auto-encoders within a population. Similarly, Cogswell et al. (2019) observed slight improvements in language compositionality with a large population but only reported them in few experimental settings. Finally, Graesser et al. (2019) studied the impact of contact-agents for different population sizes, but they did not observe a correlation between the population size and the convergence speed, success rate, or mutual agent intelligibility. The following question arises: why does community size not improve language properties in recent emergent communication literature, although it is a key structuring factor in broader linguistics lit- 1arXiv:2204.12982v1 [cs.MA] 27 Apr 2022 Published as a conference paper at ICLR 2022 erature? We argue that recent emergent communication models are limited as they ignore individual learning capacities by working only with homogeneous populations. Consequently, they miss cou- pling effects emerging from agents\u2019 asymmetries. As a result, we hypothesize that community size effects could occur as soon as local heterogeneities are introduced into populations. In this work, we explore the effects of population size with neural agents in the well-known Lewis referential game (Lewis, 1969). In this game, a speaker describes a hidden object to a listener, which must then reconstruct object properties. Both agents thus need to co-develop a communication pro- tocol to solve the task. The population-based variant of this game randomly pairs one speaker and one listener from their respective communities. The goal is to observe whether increasing the num- ber of agents enhances the communication protocol qualities, e.g. success rate, compositionality, generalization etc. (Kottur et al., 2017; Chaabouni et al., 2020; Lazaridou et al., 2018). Firstly, we reproduce Lewis reconstruction setting and con\ufb01rm the experimental difference: when increasing the number of agents, we do not observe improvements over various emergent language metrics. We thus question the current paradigm to model population in the language emergence literature. In particular, all agents are trained uniformly, i.e., their learning speed, capacity, sampling are identical (Tieleman et al., 2019; Cogswell et al., 2019; Fitzgerald, 2019). Secondly, we evaluate the impact of a potential source of model heterogeneity: agents learning speed. We observe that the absolute value of speaker-listener speed is not important, yet their relative value is crucial. We hence shed light on the strong correlation between language structures and agents relative training facilities. Thirdly, we push this reasoning further by distributing learning speeds across the pop- ulation thus creating heterogeneous populations. We there observe an improvement",
    "conclusion": "how heterogeneous populations may be leveraged to structure further the language (e.g.\nmore complex tasks, larger population). Finally, training speed is a natural controlling parameter for\ncomputational models, but it is unclear how it may relate to human behavior. Overall, we hope that\nthis paper provides new insights toward modeling populations in language emergence, and it is part\nof this constant back and forth between cognitive sciences, linguistics, and arti\ufb01cial intelligence.\n9\nPublished as a conference paper at ICLR 2022"
  },
  {
    "id": "2306.03830v1",
    "title": "Inductive Bias for Emergent Communication in a Continuous Setting",
    "authors": "John Isak Fjellvang Villanger, Troels Arnfred Bojesen",
    "published": "2023-06-06",
    "category": "cs.LG",
    "arxiv_url": "http://arxiv.org/abs/2306.03830v1",
    "pdf_url": "https://arxiv.org/pdf/2306.03830v1",
    "abstract": "We study emergent communication in a multi-agent reinforcement learning setting,\nwhere the agents solve cooperative tasks and have access to a communication\nchannel. The communication channel may consist of either discrete symbols or\ncontinuous variables. We introduce an inductive bias to aid with the emergence of\ngood communication protocols for continuous messages, and we look at the effect\nthis type of inductive bias has for continuous and discrete messages in itself or\nwhen used in combination with reinforcement learning. We demonstrate that this\ntype of inductive bias has a beneficial effect on the communication protocols learnt\nin two toy environments, Negotiation and Sequence Guess.\n1",
    "introduction": "In the case of positive signaling, a speaker is incentivized to produce different messages from different observations. While for positive listening a listener is incentivized to produce different actions from different messages. Overall, this ensures an improved use of the communication bandwidth and a reduced chance of not acting based upon messages. In the works by Eccles et al. [3], this scheme is explored for when the messages passed between the agents are built from discrete symbols. An orthogonal approach to improving the stability of learning to communicate is to allow for the gradient signal to flow through the communication channel. Doing so shifts the problem from a decentralized towards a centralized training paradigm, which helps at alleviating some of the issues plaguing the former[ 5]. The cost is that the gradient information needs to be available and passed between the agents while training, which may or may not be viable in a given setting. In this work, we look at how MARL agents learn to form a \u201cshared language\u201d in order to solve cooperative tasks. Using two toy examples, a variant of Negotiation [ 2,1,9,6], and a new game we call Sequence Guess, we demonstrate how the positive signaling ideas can be extended to continuous Preprint. Under review.arXiv:2306.03830v1 [cs.LG] 6 Jun 2023 communication protocols, where the discrete symbols are replaced with real numbers. We estimate the effect of continuous positive signaling on differentiable communication protocols [ 19,15,10,23], reinforced communication protocols [ 4], and a combination of both. The effect of a continuous communication protocol is also compared to the effect of a discrete one, in the otherwise discrete game of Sequence Guess. 2 Positive Signaling We write the total loss function for a communicating MARL agent as L=Lrest+Lcomm, where the latter term is associated with communication and the former with other actions. The communication loss may be further subdivided into Lcomm=LRC+\u03bbIBLIB (1) whereLRCis the loss associated with the communication policy, LIBis an inductive bias, and \u03bbIB>0 is a weighting factor. We will focus on Lcomm from here on. Positive signaling is equivalent to maximizing the entropy of the average message policy, while at the same time minimizing the entropy of the message policy when conditioned upon a trajectory. We denote the policy for selecting message mgiven a trajectory xas\u03c0(m|x), and the average message policy, weighted by frequency, as\u03c0(m) =Ex\u223c\u03c0[\u03c0(m|x)].\u03c0can be estimated from a mini-batch of Btrajectories by \u03c0(m)\u22481 BBX b\u03c0(m|xb), (2) where the subscript blabels the mini-batch members. Then, a natural inductive biaswhich encourages positive signaling would be LIB(\u03c0, x) =\u2212H(\u03c0) +\u03bbPSH(\u03c0(\u00b7|x)) (3) =X m\u2208M\u03c0(m) ln(\u03c0(m))\u2212\u03bbPS\u03c0(m|x) ln(\u03c0(m|x)), where Mis the set of all possible messages and \u03bbPS>0is a weighting factor. In practice, however, minimizing the entropy when it is conditioned upon the current trajectory does not work well. One reason for this may be that for any c <log 2 the space of policies with entropy at mostcis disconnected[ 3], in that the minimal possible entropy during a gradual message policy shift from an old to a new most",
    "conclusion": "the choice of hyperparameters is they are of the same magnitude as the ones used by Cao et al. [1]. Note that weight decay with DM fails to produce any convergence, the most likely reason beings that the gradient signal in the initial correlations giving rise to communication is too weak compared to the gradient signal of weight decay. C.2.2 Continuous messages For continuous messages the architecture is the same except for that mastermind employs only the encoder part, where the final hidden state is used in a fully connected layer of size 100, then a ReLU activation and another fully connected layer, the output from this layer is used in the same manner as in Negotiation in order to generate the message. The guesser employs only the decoder part for continuous messages, as messages are of the same type as those used for Negotiation. Figure 5: The encoder-decoder architecture used for DM Sequence Guess. In the case of the mastermind input xtwill contain symbol number tfrom the guess sequence and target sequence, T1will be the length of the target sequence and T2will be the length of the message sequence. The output ytis used in a fully connected layer with a Softmax activation function in order to find message symbol number t.In the case of the guesser input xtwill contain symbol number tfrom the message, T1will be the length of the message sequence and T2will be the length of the target sequence. yt is used in a fully connected layer with a Softmax activation function in order to find guess symbol number t. In both cases a one-hot encoding of the current turn is appended to the final hidden state of the Encoder in order to produce the context vector. Table 3 shows the hyperparameters used in Sequence Guess. 12"
  },
  {
    "id": "2108.01828v3",
    "title": "Emergent Discrete Communication in Semantic Spaces",
    "authors": "Mycal Tucker, Huao Li, Siddharth Agrawal, Dana Hughes, Katia Sycara, Michael Lewis, Julie Shah",
    "published": "2021-08-04",
    "category": "cs.LG",
    "arxiv_url": "http://arxiv.org/abs/2108.01828v3",
    "pdf_url": "https://arxiv.org/pdf/2108.01828v3",
    "abstract": "Neural agents trained in reinforcement learning settings can learn to communicate\namong themselves via discrete tokens, accomplishing as a team what agents would\nbe unable to do alone. However, the current standard of using one-hot vectors\nas discrete communication tokens prevents agents from acquiring more desirable\naspects of communication such as zero-shot understanding. Inspired by word\nembedding techniques from natural language processing, we propose neural agent\narchitectures that enables them to communicate via discrete tokens derived from a\nlearned, continuous space. We show in a decision theoretic framework that our tech-\nnique optimizes communication over a wide range of scenarios, whereas one-hot\ntokens are only optimal under restrictive assumptions. In self-play experiments, we\nvalidate that our trained agents learn to cluster tokens in semantically-meaningful\nways, allowing them communicate in noisy environments where other techniques\nfail. Lastly, we demonstrate both that agents using our method can effectively\nrespond to novel human communication and that humans can understand unlabeled\nemergent agent communication, outperforming the use of one-hot communication.\n1",
    "introduction": "by forcing agents to broadcast one-hot vectors [ 9,20]. These tokens in effect become a lexicon used by agents. Studying when these tokens are emitted allows researchers to uncover their meanings, as well as to study the broader questions of what environment or agent factors contribute to desirable aspects of learned communication (e.g., compositionality or, in continuous communication settings, zero-shot understanding) [15, 19, 5, 4]. We claim that discretizing messages by constraining them to conform to one-hot vectors fundamentally precludes agents from learning some desirable properties of language. One-hot vectors establish no relationships between tokens because each one-hot vector is orthogonal to and equally far away from all other vectors. Conversely, research from natural language processing and word embeddings has long established the importance of learning representations of discrete words within a continuous, semantic space [29, 32].arXiv:2108.01828v3 [cs.LG] 4 Nov 2021 In this work, we demonstrate the bene\ufb01t of agents that employ a discrete set of tokens within a continuous space over agents that use the standard practice of communicating via one-hot vectors in discrete emergent communication settings. We present a novel architecture and implementation for learning such communication and provide decision-theoretic analysis of the value of such an approach - the congruence of meaning and form of communications. Simulation experiments con\ufb01rmed these results: our agents learned an arrangement of tokens that clustered in human-understandable patterns. The arrangement of discrete tokens within the learned communication space produced team performance that was robust to environment noise and enabled agents to effectively utilize novel communication vectors. In human-agent experiments, agents aligned their tokens with natural language embeddings and responded appropriately to novel English phrases. Lastly, we showed that humans capably interpreted unlabeled emergent communication tokens in a reference game.1 2 Related Work We propose a technique within emergent communication literature, drawing inspiration from work on word embeddings in natural language processing (NLP) and zero-shot classi\ufb01cation. 2.1 Emergent Communication Researchers of emergent communication study techniques to enable agents to learn to communicate among themselves, enabling high task performance in reinforcement learning settings (see [ 36,10,18], among others). These settings, such as reference games or Lewis signalling games [ 25], are designed such that agents must communicate to perform the task successfully; in \u201ccheap-talk\u201d scenarios, agents often learn successful communication strategies by sending real-valued vectors to each other. We focus on discrete communication emerging among decentralized agents that may communicate by sending one of a \ufb01nite set of vectors to each other, but may not access other agents\u2019 weights or gradients during training or execution [ 8]. Previously, such discrete communication has often taken the form of one-hot messages: Foerster et al. [ 9] proposed binary discrete messages, but subsequent works seem to have reverted to one-hot vectors [15, 20, 11, 23, 6]. Even when agents learn to communicate, they often fail to learn a protocol that humans or separately- trained agents can understand; that is, they fail at the \u201czero-shot\u201d learning problem [12]. To address this gap, several recent works have found properties",
    "conclusion": "on 80 examples of each of 4 animal-vehicle image pairs. After removing tasks that were completed in extremely short or long time (out of 3 standard deviations), we were left with 2093 valid samples from 253 unique workers. Participants were paid $0:03for each task; the average completion time was 58:9seconds, equivalent to a$1:84hourly wage. The instruction and user interface of this experiment is shown in Fig. 8. Communications were presented in a 2D plane with 8 labeled nodes and one unlabeled communication node. (Labels were generated by evaluating agents in self-play and selecting the most likely token for each class.) Participants were asked to select one out of two images that the communication node most likely referred to. Both images are from the held-out classes, meaning there were no labeled nodes for the image classes, in order to test zero-shot understanding in human-agent teams. As shown in Table 8, 4 out of the 5 trained models using our prototype-based method outperformed the BERT-based embeddings. The difference between two methods is marginally signi\ufb01cant ( 2(1;N= 2093) = 3:21;p=:073). Further inspection of Proto-1, the \ufb01rst trained model that exhibits random-chance performance, revealed that the model failed to converge to high reward in training, and that the tokens for vehicles and animals failed to separate. Training instability is a chronic problem in MARL [ 8], so we consider this failure as a symptom of general dif\ufb01culties with reinforcement learning rather than our technique speci\ufb01cally. Nevertheless, in four of our \ufb01ve trials, models did converge to high performance, and if one discards results from Proto-1 as outliers, we signi\ufb01cantly outperformed BERT embeddings ( 2(1;N= 1799) = 16 :15;p<: 0001 ). 21 Figure 7: Instructions and user interface of AMT data-gathering experiment. 22 Figure 8: Instructions and user interface of AMT communication interpretation experiment. 23"
  },
  {
    "id": "2504.16021v1",
    "title": "Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support",
    "authors": "Dinithi Dissanayake, Suranga Nanayakkara",
    "published": "2025-04-22",
    "category": "cs.HC",
    "arxiv_url": "http://arxiv.org/abs/2504.16021v1",
    "pdf_url": "https://arxiv.org/pdf/2504.16021v1",
    "abstract": "Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support DINITHI DISSANAYAKE, Augmented Human Lab, National University of Singapore, Singapore SURANGA NANAYAKKARA, Augmented Human Lab, National University of Singapore, Singapore Flow Theory describes an optimal cognitive state where individuals experience deep focus and intrinsic motivation when a task\u2019s difficulty aligns with their skill level. In AI-augmented reasoning, interventions that disrupt the state of cognitive flow can hinder rather than enhance decision-making. This paper proposes a context-aware cognitive augmentation framework that adapts interventions based on three key contextual factors: type, timing, and scale. By leveraging multimodal behavioral cues (e.g., gaze behavior, typing hesitation, interaction speed), AI can dynamically adjust cognitive support to maintain or restore flow. We introduce the concept of cognitive flow, an extension of flow theory in AI-augmented reasoning, where interventions are personalized, adaptive, and minimally intrusive. By shifting from static interventions to context-aware augmentation, our approach ensures that AI systems support deep engagement in complex decision-making and reasoning without disrupting cognitive immersion. 1 Introduction The theory of flow, proposed by Csikszentmihalyi [ 3], defines an optimal psychological state in which individuals experience deep focus and intrinsic motivation when the challenge of a task is perfectly matched to their skill level. When a task is too easy, users become bored, while excessive difficulty leads to frustration. For example, in video games, the enjoyment of a game depends on several factors, including the player\u2019s skill level and the challenge the game presents. A game that appropriately matches a player\u2019s skill level fosters a sense of engagement, encouraging continued play. Similarly, any new intervention or augmentation should support the user in either attaining or maintaining the state of flow of whatever the current task the user is engaged in. In the domain of AI-augmented reasoning systems, the goal",
    "introduction": "is to enhance human decision-making by providing intelligent feedback on logical reasoning, bias detection, and argumentation [ 4,5,7]. The effectiveness of such systems depends on several factors, with the type, timing, and scale of the intervention playing crucial roles in determining the quality of the augmentation. For example, poorly timed or intrusive interventions can disrupt a user\u2019s state of cognitive flow (whereby we define the optimal cognitive state one would prefer), leading to disengagement rather than improving reasoning. Current AI reasoning assistants often adopt a static or one-size-fits-all approach, assuming that interventions should be uniformly applied across all users [ 4]. However, individuals respond to cognitive support in diverse ways [10]. Some may prefer direct interventions, such as explicit fact-checking and counterarguments, while others may benefit from Socratic questioning, which promotes self-guided reflection. The timing of the intervention is equally significant\u2014interrupting a user mid-thought with a suggestion can break their focus, whereas the ability to understand subtle nudges, such as gaze-based or gesture-based cues, might be able to guide the intervention to present their reasoning seamlessly without causing disruption. Thereby, we identify three key factors that impact the effectiveness of these interventions: This paper was presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning (AIREASONING-2025-01). This is the authors\u2019 version for arXiv. Authors\u2019 Contact Information: Dinithi Dissanayake, dinithi@ahlab.org, Augmented Human Lab, National University of Singapore, Singapore; Suranga Nanayakkara, suranga@ahlab.org, Augmented Human Lab, National University of Singapore, Singapore. 1arXiv:2504.16021v1 [cs.HC] 22 Apr 2025 2 Dinithi Dissanayake and Suranga Nanayakkara \u2022Type of intervention : Some individuals may prefer direct feedback, while others may respond better to Socratic questioning. \u2022Timing of intervention : Understanding when users are \"stuck\" through subtle cues, such as gaze or gesture- based signals, is critical. \u2022Scale of intervention : The magnitude of the intervention\u2014whether subtle or more direct\u2014also determines its impact. These three factors; type, timing, and scale, determine the effectiveness of interventions, which can be broadly categorized under the concept of context . This paper proposes that context-aware reasoning interventions should be designed to dynamically adapt based on real-time user engagement signals, ensuring that interventions enhance rather than disrupt the cognitive flow. Contextual awareness plays a pivotal role in identifying the appropriate type, timing, and scale of interventions, thus enabling personalized cognitive assistance. By maintaining this balance, the intervention fosters a sense of autonomy in task completion, ensuring that users feel they have accomplished the task themselves, rather than perceiving it as something imposed by an external system, thus supporting the notion of assistive augmentation [9]. With recent developments in multimodal AI models [ 1,6,11], deriving contextual awareness becomes a matter of mapping input modalities to cognitive assistance and understanding when, what, and how to intervene. By leveraging multimodal cues, such as gaze behavior, typing hesitation, and interaction patterns, AI systems can infer cognitive load and deliver interventions at moments that sustain deep reasoning rather than hinder it. Furthermore, longitudinal engagement data can be utilized to cluster users based on preferred reasoning styles and",
    "conclusion": "an individual\u2019s cognitive state. In the context of cognitive flow, AI systems must intelligently map multimodal inputs to determine whether an individual is in a state of deep engagement, facing cognitive overload, or experiencing disengagement due to insufficient challenge. Learned representations derived from behavioral data provide a means to infer cognitive load and engagement levels, enabling real-time adjustments to interventions. By balancing challenge and support based on real-time multimodal inputs, the system ensures that users neither disengage due to excessive difficulty nor lose interest due to low complexity. By integrating context-aware AI-driven augmentation with flow theory, we move toward systems that do more than assist with reasoning\u2014they optimize the conditions under which reasoning and critical thinking can thrive. Rather than disrupting engagement, AI enhances 4 Dinithi Dissanayake and Suranga Nanayakkara cognition in ways that feel seamless and natural, preserving a sense of agency while fostering deeper intellectual engagement. 4 Call to Action This paper presents a case for the importance of context awareness in AI-driven cognitive interventions. We introduce the concept of Cognitive Flow Alignment, emphasizing that interventions are most effective when they dynamically adjust to an individual\u2019s cognitive state, neither disrupting engagement nor allowing stagnation. Using insights from flow theory, we argue that AI systems should infer behavioral cues from multimodal data to sustain an optimal cognitive state. While current AI-based augmentation focuses on task performance and engagement metrics, we propose that future systems should be evaluated on their ability to contextually regulate cognitive flow. This requires a shift from static intervention strategies to adaptive models that personalize intervention type, intensity, and timing based on real-time user states. We advocate for a mixed-method evaluation framework combining behavioral analytics with subjective feedback to refine personalized intervention strategies that optimize cognitive augmentation while respecting individual differences in engagement and cognitive load."
  },
  {
    "id": "2512.20745v2",
    "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent",
    "authors": "Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang",
    "published": "2025-12-23",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2512.20745v2",
    "pdf_url": "https://arxiv.org/pdf/2512.20745v2",
    "abstract": "Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved re-\nmarkable progress in natural language reasoning with long chain-of-thought.\nHowever, they remain computationally inefficient and struggle with accuracy\nwhen solving problems requiring complex mathematical operations. In this work,\nwe present AgentMath, an agent framework that seamlessly integrates language\nmodels\u2019 reasoning capabilities with code interpreters\u2019 computational precision to\nefficiently tackle complex mathematical problems. Our approach introduces three\nkey innovations: (1) An automated method that converts natural language chain-\nof-thought into structured tool-augmented trajectories, generating high-quality su-\npervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic rein-\nforcement learning (RL) paradigm that dynamically interleaves natural language\ngeneration with real-time code execution. This enables models to autonomously\nlearn optimal tool-use strategies through multi-round interactive feedback, while\nfostering emergent capabilities in code refinement and error correction; (3) An\nefficient training system incorporating innovative techniques, including request-\nlevel asynchronous rollout scheduling, agentic partial rollout, and prefix-aware\nweighted load balancing, achieving 4-5\u00d7 speedup and making efficient RL train-\ning feasible on ultra-long sequences with scenarios with massive tool calls. Ex-\ntensive evaluations show that AgentMath achieves state-of-the-art performance on\nchallenging mathematical competition benchmarks including AIME24, AIME25,\nand HMMT25, substantially outperforming frontier open-source models of com-\nparable size. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8%\naccuracy respectively, achieving advanced capabilities. These results validate the\neffectiveness of our approach and pave the way for building more sophisticated\nand scalable mathematical reasoning agents.\n1",
    "introduction": "Large Reasoning Models (LRMs) such as o3 and DeepSeek-R1 have made remarkable progress\nin natural language reasoning with long chain-of-thought (CoT)(OpenAI et al., 2024; Team et al.,\n2025a; DeepSeek-AI et al., 2025; xAI, 2023; Claude, 2025; Team et al., 2023; Wei et al., 2022).\nHowever, when tackling mathematical problems that demand precise computation or intricate sym-\nbolic manipulation, including large-number arithmetic, complex equation solving, and geometric\nreasoning, pure text-based reasoning still has limitations: frequent computational errors necessitate\nredundant corrections, which in turn leads to inefficiency and erroneous results.\n\u2020Corresponding authors. This work was conducted during Luo\u2019s internship at Tencent and was supported\nby the CIE-Tencent Ph.D. Student Research Incentive Program (Tencent Hunyuan Special Fund).\n1arXiv:2512.20745v2  [cs.AI]  27 Dec 2025\nPreprint\nTo enhance computational efficiency and accuracy, recent work has explored incorporating external\ntools (i.e., code interpreters), delegating complex and error-prone computational steps to external\nenvironments(Li et al., 2025e; Zhou et al., 2025; Lin & Xu, 2025; Zhang et al., 2025b; Chen et al.,\n2023; Gao et al., 2023; Gou et al., 2023b). For instance, models like o3 and o4-mini have signif-\nicantly improved mathematical reasoning accuracy through tool invocation. Nevertheless, existing",
    "conclusion": "complex doesn\\'t define __round__ method \\n \\n </interpreter> \\n \\n Oops, the code above appears to be throwing an error. I need to fix this to ensure it runs successfully.< The error occurs because the `round()` function cannot handle complex numbers directly. I will modify the code to handle complex numbers correctly. \\n \\n <code> \\n ```python \\n import cmath \\n import math \\n \\n # Let omega be the primitive 5th root of unity, omega = exp(2*pi*i / 5) \\n omega = cmath.exp(2 * cmath.pi * 1j / 5) \\n \\n # Calculate the five terms g(omega^k) \\n g_0 = (omega**0)**2 - 2 * (omega**0) + 2 \\n g_1 = omega**2 - 2 * omega + 2 \\n g_2 = omega**4 - 2 * omega**2 + 2 \\n g_3 = omega**6 - 2 * omega**3 + 2 \\n g_4 = omega**8 - 2 * omega**4 + 2 \\n \\n # Calculate the product of the five terms \\n total_product = g_0 * g_1 * g_2 * g_3 * g_4 \\n \\n # Print the individual terms and the final product \\n print(f\"g(1) = {g_0.real:.5f}{g_0.imag:+.5f}j\") \\n print(f\"g(omega) = {g_1.real:.5f}{g_1.imag:+.5f}j\") \\n print(f\"g(omega^2) = {g_2.real:.5f}{g_2.imag:+.5f}j\") \\n print(f\"g(omega^3) = {g_3.real:.5f}{g_3.imag:+.5f}j\") \\n print(f\"g(omega^4) = {g_4.real:.5f} {g_4.imag:+.5f}j\") \\n print(f\"Total product = {total_product.real:.5f} {total_product.imag:+.5f}j\") \\n ``` \\n </code> \\n <interpreter>\\ng(1) = 1.00000+0.00000j \\n g(omega) = 0.57295 - 1.31433j \\n g(omega^2) = 3.92705 - 2.12663j \\n g(omega^3) = 3.92705+2.12663j \\n g(omega^4) = 0.57295+1.31433j \\n Total product = 41.00000-0.00000j \\n \\n </interpreter> \\n\\n The numerical calculation shows the total product is 41. The small imaginary part is due to floating-point inaccuracies\u2026 So here f=g(x), so f2=1, f1=-2, f0=2 \\n\\n g=r(x), so g1=-64, g0=-1 \\n\\n ......Thus P=8321. \\n\\n Now, 8321 mod 1000 is 321. \\n\\n So the remainder is 321. Figure 9: AgentMath Case study For code self-correction 34"
  },
  {
    "id": "2510.17496v2",
    "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models",
    "authors": "Giacomo Camposampiero, Michael Hersche, Roger Wattenhofer, Abu Sebastian, Abbas Rahimi",
    "published": "2025-10-20",
    "category": "cs.LG",
    "arxiv_url": "http://arxiv.org/abs/2510.17496v2",
    "pdf_url": "https://arxiv.org/pdf/2510.17496v2",
    "abstract": "We introduce I-RA VEN-X, a symbolic benchmark designed to evaluate generaliza-\ntion and robustness in analogical and mathematical reasoning for Large Language\nModels (LLMs) and Large Reasoning Models (LRMs). I-RA VEN-X extends\nI-RA VEN by increasing operand complexity, attribute range, and introducing per-\nceptual uncertainty. Compared to LLMs, empirical results on I-RA VEN-X show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. For instance, LRMs experience a\nsignificantly smaller degradation on arithmetic accuracy ( 80.5%\u219263.0% ) com-\npared to LLMs ( 59.3%\u21924.4% ). However, LRMs are still significantly challenged\nby reasoning under uncertainty ( \u221261.8% in task accuracy) and cannot effectively\nexplore multiple probabilistic outcomes in superposition.\n1",
    "introduction": "benchmarks for reasoning proficiency in LLMs and LRMs. Firstly, most of the problems involve only a few operands, representing an overly simplistic subset of analogical and mathematical relations. Most importantly, the test problems and their corresponding solutions are openly available online, increasing the risk of potential data leakage from the model\u2019s pre- and post-training stages as previously observed in other settings [ 12]. Furthermore, assuming the availability of anoracle perceptionhas become a standard practice in their translation from visual to textual (symbolic) tasks (necessary to test language-only models) [ 13,14]. This assumption is reasonable when the scope of the investigation is limited to the reasoning component; however, it falls short when we zoom out to more complex, end-to-end systems, as it bypasses crucial steps of the original visual analogical reasoning, such as filtering irrelevant attributes and accounting for the uncertainty of the perception module. 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: MATH-AI.arXiv:2510.17496v2 [cs.LG] 31 Oct 2025 To tackle these problems, this paper makes the following contributions: 1.introduces I-RA VEN-X, an enhanced, symbolic version of the standard I-RA VEN benchmark that enables testing the generalization and robustness to simulated perceptual uncertainty in text-based language and reasoning models (see Figure 1), 2.highlights that LRMs consistently generalize better than LLMs in terms of productivity and systematicity, but significantly fail to reason under uncertainty. 2 Methods 2.1 I-RA VEN-X: testing generalization and robustness of reasoning in LLMs and LRMs We propose a fully-symbolic, parametrizable dataset to evaluate LLMs and LRMs, dubbed I-RA VEN- X. Some examples from the dataset are included in Figure 1. I-RA VEN-X enhances the original I-RA VEN (more extensively described in Appendix A) over different dimensions: 1.Productivity: we parametrize the number of operands in the reasoning relations (e.g., using 3\u00d710 matrices instead of 3\u00d73, in Figure 1); 2.Systematicity: we introduce larger dynamic ranges for the operand values (e.g., 1000 attribute values instead of 10, in Figure 1); 3.Robustness to confounding factors: we augment the set of original attributes in RPM with randomly sampled values, which do not contribute to the reasoning ( in Figure 1); 4.Robustness to non-degenerate value distributions: we smoothen the distributions of the input values corresponding to the generative factors ( in Figure 1). Practically speaking, 1. and 2. enable testing the generalization of LLMs and LRMs to longer reasoning chains and an increased number of concepts. On the other hand, 3. and 4. allow to loosen the strong assumption of an oracle perception, simulating an imperfect sensory front-end while operating with text-only language models, hence providing an additional focus on the robustness of reasoning under uncertainty. More details on the design of 1\u22124 are included in Appendix B. In addition, the original I-RA VEN was narrowed down to a single constellation ( center , containing a single object per panel), which was observed to be simultaneously a strong test for a wide range of logical and arithmetic skills and unexpectedly challenging. Figure 1: This figure highlights all the different axes of generalization and",
    "conclusion": "<0.02::888,0.95::889,0.03::890>, <0.01::874,0.54::875,0.45::876>); row 3: (<0.21::531,0.77::532,0.02::533>, <0.01::496,0.88::497,0.11::498>, <0.07::830,0.62::831,0.31::832>), (<0.20::24,0.79::25,0.01::26>, <0.19::495,0.62::496,0.19::497>, <0.06::830,0.92::831,0.02::832>), (<0.17::30,0.56::31,0.27::32>, <0.27::494,0.64::495,0.09::496>, <0.02::830,0.98::831,0.00::832>), (<0.00::42,0.98::43,0.02::44>, <0.38::493,0.58::494,0.04::495>, <0.19::830,0.53::831,0.28::832>), (<0.07::573,0.52::574,0.41::575>, <0.01::492,0.99::493,0.00::494>, <0.01::830,0.81::831,0.18::832>), (<0.26::760,0.55::761,0.19::762>, <0.13::491,0.83::492,0.04::493>, <0.05::830,0.82::831,0.13::832>), (<0.47::575,0.52::576,0.01::577>, <0.15::490,0.59::491,0.26::492>, <0.16::830,0.81::831,0.03::832>), (<0.03::290,0.82::291,0.15::292>, <0.29::489,0.52::490,0.19::491>, <0.03::830,0.85::831,0.12::832>), (<0.08::916,0.81::917,0.11::918>, <0.05::488,0.83::489,0.12::490>, <0.09::830,0.64::831,0.27::832>), Answer set: Answer #0: (<0.06::289,0.83::290,0.11::291>, <0.00::487,1.00::488,0.00::489>, <0.03::874,0.82::875,0.15::876>) Answer #1: (<0.01::850,0.78::851,0.21::852>, <0.00::487,0.99::488,0.01::489>, <0.08::874,0.85::875,0.07::876>) Answer #2: (<0.03::289,0.57::290,0.40::291>, <0.15::450,0.75::451,0.10::452>, <0.15::830,0.62::831,0.23::832>) Answer #3: (<0.06::289,0.52::290,0.42::291>, <0.03::487,0.92::488,0.05::489>, <0.31::830,0.61::831,0.08::832>) Answer #4: (<0.02::850,0.95::851,0.03::852>, <0.16::450,0.63::451,0.21::452>, <0.20::874,0.52::875,0.28::876>) Answer #5: (<0.02::850,0.86::851,0.12::852>, <0.18::487,0.80::488,0.02::489>, <0.14::830,0.79::831,0.07::832>) Answer #6: (<0.01::289,0.96::290,0.03::291>, <0.38::450,0.59::451,0.03::452>, <0.08::874,0.68::875,0.24::876>) Answer #7: (<0.08::850,0.62::851,0.30::852>, <0.15::450,0.82::451,0.03::452>, <0.09::830,0.87::831,0.04::832>) Table 6: Example prompt for the I-RA VEN-X task with smooth distributions. 12 D Comparison between OpenAI o3-mini and o1 This Appendix presents a small ablation study on two different closed-source LRMs, OpenAI o1 and OpenAI o3-mini. The goal of these experiments was to measure the difference, if any, in the reasoning capabilities of the o3-mini model compared to its bigger, more expensive predecessor. We restricted the size of the test set to 100 test examples for both I-RA VEN and I-RA VEN-X. The results, presented in Table 7, show that the two models achieve roughly comparable performance on both I-RA VEN and I-RA VEN-X, with o3-mini being consistently slightly less accurate than o1. However, o1 is also considerably more expensive compared to o3: o1 is priced at $15 and $60 per million input and output tokens, respectively, while o3-mini costs only $1.1 and $4.4 per million input and output tokens (approximately 14\u00d7 less expensive). Hence, we opt to use only o3-mini in the full evaluation. Model SettingI-RA VEN I-RA VEN-X Range 10 Range 100 Range 1000 Task Arithm. Task Arithm. Task Arithm. OpenAI o1 Entangled 88.0 79.7 86.0 68.2 86.0 68.2 OpenAI o3-mini Entangled 86.6 81.4 84.0 63.6 81.0 60.8 Table 7: Task and arithmetic accuracy (%) comparison of two different LRMs on a subset of 100 test examples of I-RA VEN and I-RA VEN-X. 13"
  },
  {
    "id": "2404.12534v3",
    "title": "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean",
    "authors": "Peiyang Song, Kaiyu Yang, Anima Anandkumar",
    "published": "2024-04-18",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2404.12534v3",
    "pdf_url": "https://arxiv.org/pdf/2404.12534v3",
    "abstract": "Neural theorem proving combines large language models (LLMs) with proof assistants such\nas Lean, where the correctness of formal proofs can be rigorously verified, leaving no room for\nhallucination. With existing neural theorem provers pretrained on a fixed collection of data and\noffering valuable suggestions at times, it is challenging for them to continually prove novel theorems\nin a fully autonomous mode, where human insights may be critical. In this paper, we explore LLMs\nas copilots that assist humans in proving theorems. We introduce Lean Copilot, a general framework\nfor running LLM inference natively in Lean. It enables programmers to build various LLM-based\nproof automation tools that integrate seamlessly into the workflow of Lean users. Lean users can\nuse our pretrained models or bring their own ones that run either locally (with or without GPUs) or\non the cloud. Using Lean Copilot, we build LLM-based tools that suggest proof steps, complete\nproof goals, and select relevant premises. Experimental results on the Mathematics in Lean textbook\ndemonstrate the effectiveness of our method compared to existing rule-based proof automation in\nLean ( AESOP ), confirming the significance of having LLMs integrated into the theorem proving\nworkflow in Lean. When assisting humans, Lean Copilot requires only 2.08 manually-entered proof\nsteps on average (3.86 required by AESOP ); when automating the theorem proving process, Lean\nCopilot automates 74.2% proof steps on average, 85% better than AESOP (40.1%). We open source\nall code and artifacts under a permissive MIT license to facilitate further research.\nKeywords: Neural Theorem Proving, Proof Automation, Large Language Models, Neuro-Symbolic\nReasoning, AI for Mathematics\n1.",
    "introduction": "85% better than AESOP (40.1%). We open source all code and artifacts under a permissive MIT license to facilitate further research. Keywords: Neural Theorem Proving, Proof Automation, Large Language Models, Neuro-Symbolic Reasoning, AI for Mathematics 1. Introduction Neural theorem proving (Li et al., 2024) combines the strong learning capability of neural networks with the rigor of symbolic proof checking. The neural component trains LLMs to effectively generate formal proofs, yet LLMs\u2019 tendency to hallucinate (Huang et al., 2025a) prevents the generated proofs from guaranteed correct. The symbolic component uses proof assistants to verify proof correctness, but the interactive nature of proof assistants require substantial amount of human efforts to formalize proofs. Note that each component\u2019s weakness is naturally complemented by the other. Combining both, neural theorem proving trains LLMs to generate candidate proofs and verifies them by proof assistants, forming a powerful neuro-symbolic system for formal reasoning (Yang et al., 2024). On the symbolic side, Lean (de Moura et al., 2015) has been a widely used proof assistant especially in mathematical theorem proving, thanks to its large math library Mathlib4 (mathlib \u00a9 2025 P. Song, K. Yang & A. Anandkumar.arXiv:2404.12534v3 [cs.AI] 11 May 2025 SONG YANG ANANDKUMAR SUGGEST_T ACTICS Proof Goal SELECT_PREMISES SEARCH_PROOF LLMs Locally with CTranslate2 Server OR Lean Copilot Figure 1: Lean Copilot provides a general framework for running LLM inference in Lean, either locally via CTranslate2 or on a server. This framework enables creating various proof automation tools, including those for tactic suggestion, proof search, and premise selection. Community, 2020) with over 189K theorems1from diverse domains, which is still expanding. On the neural side, LeanDojo (Yang et al., 2023) novelly augments tactic generation with premise retrieval, and pairs tactic (individual proof step in Lean) generation with proof search to form complete proofs in a GPT- f(Ayers et al., 2023) style. Later works propose methods to further improve both tactic generation (Lin et al., 2024) and proof search (Huang et al., 2025b). An alternative approach is whole- proof generation (First et al., 2023), where synthetic data is used to address the data scarcity problem (Frieder et al., 2024). By scaling up, this approach has given birth to powerful proof-generation LLMs, marked by DeepSeek-Prover-v1.5 (Xin et al., 2024) and Goedel-Prover (Lin et al., 2025). All these existing LLM-based provers aim to prove theorems fully autonomously without human intervention. They wrap the proof assistant (e.g., Lean) into a gym-like (Brockman et al., 2016) environment. The model interacts with the proof environment and is evaluated by the number of test theorems it proves. The interaction happens solely on the backend server, without any collaboration with humans. While an autonomous AI mathematician is desirable in the long run, current LLMs often fail to prove theorems that are truly novel or challenging, especially those from a different domain than the training data (Zheng et al., 2022). This is in part because each branch of mathematics uses different lemmas and requires distinct intuitions, limiting the generalization of LLMs. However, with the development in Lean",
    "conclusion": "We have introduced Lean Copilot: a framework for running LLM inference natively in Lean.\nUsing Lean Copilot, we have built LLM-based proof automation tools for generating tactic sug-\ngestions ( SUGGEST _TACTICS ), searching for proofs ( SEARCH _PROOF ), and selecting premises\n(SELECT _PREMISES ). Lean Copilot also provides general interfaces between LLMs and Lean,\nallowing users to bring their own models and/or build other proof automation tools. Experimental\nresults on the Mathematics in Lean textbook demonstrate the effectiveness of our method compared\nto existing rule-based proof automation in Lean ( AESOP ). When assisting humans, Lean Copilot\nrequires only 2.08 manually-entered proof steps on average (3.86 required by AESOP ); when automat-\ning the theorem proving process, Lean Copilot automates 74.2% proof steps on average, 85% better\nthan AESOP (40.1%). These results confirm the benefit of integrating LLMs into the theorem proving\npipeline in Lean. We open source all code and artifacts to facilitate future research, and we hope to\nsee more LLM-based proof automation built upon Lean Copilot to help create more high-quality\nformal data, which would in turn enhance LLMs\u2019 capability in formal math."
  },
  {
    "id": "2504.17017v2",
    "title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification",
    "authors": "Balaji Rao, William Eiers, Carlo Lipizzi",
    "published": "2025-04-23",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2504.17017v2",
    "pdf_url": "https://arxiv.org/pdf/2504.17017v2",
    "abstract": "Formally verifying properties of software code has been a highly desirable task, especially with\nthe emergence of LLM-generated code. In the same vein, they provide an interesting avenue for\nthe exploration of formal verification and mechanistic interpretability. Since the",
    "introduction": "of powerful training regimes like Direct Preference Optimization\n(DPO) (Rafailov et al., 2023), Proximal Policy Optimization (PPO) (Schulman et al., 2017),\nand GRPO (Shao et al., 2024). RL frameworks for theorem proving model interactions between\nLLMs and generated proofs as a Markov Decision Process (MDP). The reward function is\ntypically designed around binary proof completion: assigning a reward of 1 if the proof is\nverified as correct and 0 otherwise (Dong et al., 2024). This binary reward system provides\nclear feedback for optimizing performance. Other approaches incorporate search algorithms into\nRL frameworks, such as Best-First Search (BFS) which guides proof generation by prioritizing\npromising paths based on heuristic evaluations (Yang et al., 2023); and Monte Carlo Tree\nSearch (MCTS) which explores potential proof paths systematically by balancing exploration\nand exploitation (Lample et al., 2022). RL-based approaches aim to improve both single-\npass and stepwise proof generation models. In single-pass methods, RL optimizes full-proof\ngeneration by rewarding logical consistency using Chain-of-Thought tokens and verification\nsuccess. In stepwise methods, RL enhances tactic prediction by refining intermediate steps based\non feedback from symbolic verifiers. Recent works such as DeepSeekMath (Xin et al., 2024)\ndemonstrate that reinforcement learning can significantly enhance models\u2019 reasoning abilities\nby improving their capacity to generate coherent and verifiable proofs over time.\n3. Method\nBuilding on the work of the workflow described in DSP (Jiang et al., 2022a) and the proof\nconstruction method in ProofAug (Liu et al., 2025), in this section we present our framework\n3\nRao Eiers Lipizzi\nFigure 1: The two core components within theProofSeekframework: (a) the fine-tuning\nlanguage model module, (b) the proof generation and verification module\nProofSeekthat leverages the strengths of proof-step and whole-proof generation, as well as the\nnatural language generation paradigms of LLMs.ProofSeekconsists of two core components:\na component for fine-tuning a language model using SFT and RL, and a proof generation and\nverification component for generating and building the formal proofs. Both components of\nProofSeekare shown in Figure 1.Our framework is generalizable across domains\nwhere the input is a mathematical statement, policy code, or natural language\nstatement, and the output is a verified proof state or a failure state. The details of\nProofSeekis shown in Algorithm 1. We first fine-tune a whole-proof generation model using\nour two-stage approach. Then, we build a formal statement that represents the provided policy\ncode or mathematical statement. Finally, we leverage the fine-grained proof structure analysis",
    "conclusion": "24policy_allows ec2 _instance_policy RunInstances Images\u2227 25policy_allows ec2 _instance_policy RunInstances Instances\u2227 26policy_allows ec2 _instance_policy RunInstances Volumes\u2227 27policy_allows ec2 _instance_policy RunInstances NetworkInterfaces\u2227 28policy_allows ec2 _instance_policy RunInstances KeyPairs \" 29oops Listing 6: Initial Proof Attempt 1(* Proof of the theorem *) 2(* 3proof - 4have \" policy_allows ec2 _instance_policy RunInstances AllResources \" 5by ( simp add: ec2 _instance_policy_def ) 6moreover have \" policy_allows ec2 _instance_policy RunInstances Images \" 7by ( simp add: ec2 _instance_policy_def ) 8moreover have \" policy_allows ec2 _instance_policy RunInstances Instances \" 9by ( simp add: ec2 _instance_policy_def ) 10moreover have \" policy_allows ec2 _instance_policy RunInstances Volumes \" 11by ( simp add: ec2 _instance_policy_def ) 12moreover have \" policy_allows ec2 _instance_policy RunInstances NetworkInterfaces \" 13by ( simp add: ec2 _instance_policy_def ) 14moreover have \" policy_allows ec2 _instance_policy RunInstances KeyPairs \" 15by ( simp add: ec2 _instance_policy_def ) 16ultimately show ? thesis by simp 17qed 18*) Listing 7: Sorry Proof (Commented) 1(* Proof of the theorem *) 2(* 3proof - 4have \" policy_allows ec2 _instance_policy RunInstances AllResources \" 5by ( simp add: ec2 _instance_policy_def ) 6moreover have \" policy_allows ec2 _instance_policy RunInstances Images \" 7by ( simp add: ec2 _instance_policy_def ) 8moreover have \" policy_allows ec2 _instance_policy RunInstances Instances \" 9by ( simp add: ec2 _instance_policy_def ) 10moreover have \" policy_allows ec2 _instance_policy RunInstances Volumes \" 11by ( simp add: ec2 _instance_policy_def ) 12moreover have \" policy_allows ec2 _instance_policy RunInstances NetworkInterfaces \" 13by ( simp add: ec2 _instance_policy_def ) 14moreover have \" policy_allows ec2 _instance_policy RunInstances KeyPairs \" 15by ( simp add: ec2 _instance_policy_def ) 16ultimately show ? thesis by simp 17qed 15 Rao Eiers Lipizzi 18*) Listing 8: State Information 1{ 2\" success \": true , 3\" i_try \": 0, 4\" success_stage \": \" init_proof \", 5\" has_timeout \": false , 6\" extra_calls \": 0, 7\" has_sc \": false 8} 16"
  },
  {
    "id": "2505.05758v5",
    "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning",
    "authors": "Azim Ospanov, Farzan Farnia, Roozbeh Yousefzadeh",
    "published": "2025-05-09",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2505.05758v5",
    "pdf_url": "https://arxiv.org/pdf/2505.05758v5",
    "abstract": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning Azim Ospanov\u2217\u2020 aospanov9@cse.cuhk.edu.hkFarzan Farnia\u2020 farnia@cse.cuhk.edu.hkRoozbeh Yousefzadeh\u2217 roozbeh.yz@gmail.com Abstract Formal reasoning and automated theorem proving constitute a challenging subfield of machine learning, in which machines are tasked with proving mathematical theorems using formal languages like Lean. A formal verification system can check whether a formal proof is correct or not almost instantaneously, but generating a completely correct formal proof with large language models (LLMs) remains a formidable task. The usual approach in the literature is to prompt the LLM many times (up to several thousands) until one of the generated proofs passes the verifi- cation system. In this work, we present APOLLO (AutomatedPrOof repair via LLM andLean cOllaboration), a modular, model -agnostic agentic framework that combines the strengths of the Lean compiler with an LLM\u2019s reasoning abilities to achieve better proof-generation results at a low token and sampling budgets.Apollo directs a fully automated process in which the LLM generates proofs for theorems, a set of agents analyze the proofs, fix the syntax errors, identify the mistakes in the proofs using Lean, isolate failing sub -lemmas, utilize automated solvers, and invoke an LLM on each remaining goal with a low top -Kbudget. The repaired sub -proofs are recombined and reverified, iterating up to a user -controlled maximum number of attempts. On the miniF2F benchmark, we establish a new state -of-the-art accuracy of 84.9% among sub 8B -parameter models (as of August 2025) while keeping the sampling budget below one hundred. Moreover,Apolloraises the state -of-the-art accuracy for Goedel -Prover -SFT to 65.6% while cutting sample complexity from 25,600 to a few hundred. General -purpose models (o3 -mini, o4 -mini) jump from 3\u20137% to over 40% accuracy. Our results demonstrate that targeted, compiler -guided repair of LLM outputs yields dramatic gains in both efficiency",
    "introduction": "and correctness, sug- gesting a general paradigm for scalable automated theorem proving. The codebase is available athttps://github.com/aziksh-ospanov/APOLLO 1 Introduction Formal reasoning has emerged as one of the most challenging fields of AI with recent achievements such as AlphaProof and AlphaGeometry doing well at the International Math Olympiad competing with humans [ 1,2,3]. Formal reasoning relies both on AI models and a formal verification system that can automatically verify whether a mathematical proof is correct or not. In recent years, formal verification systems such as Lean [ 4] have facilitated a new form of doing mathematical research by enabling mathematicians to interact with the formal verification system and also with each other via the system, enabling larger numbers of mathematicians to collaborate with each other on a single project. As such, these formal verification systems are also called proof assistants as one can use them interactively to write a formal proof and instantaneously observe the current state of the proof and any possible errors or shortcomings in the proof generated by the compiler. Immediate access \u2217Huawei Hong Kong Research Center \u2020Department of Computer Science & Engineering, The Chinese University of Hong Kong 39th Conference on Neural Information Processing Systems (NeurIPS 2025).arXiv:2505.05758v5 [cs.AI] 4 Nov 2025 Common Approach: Whole-Proof Generation Pipeline repeat up to K times LLM Lean Server proof attemptexit loop continue LLM Lean Server proof attempt(s) sub-problem(s) to proveproof state compilation errors syntax errorsApollo Proof Repair Agent Subproof Extractor Auto Solverrepeat up to r times exit loop continueOur Proposed Apollo PipelineFigure 1: The summary of whole-proof generation pipeline vs. proposed Apollo agentic pipeline. LLM refers to a chosen formal theorem generator model. to the output of Lean compiler can help a mathematician to fix possible errors in the proof. At the same time, when a proof passes the compiler with no error, other collaborators do not need to verify the correctness of the proof. This type of collaboration is transforming the field of mathematics enabling large groups of collaborators to engage in large projects of mathematical research such as the Prime Number Theorem And More project [ 5]. Moreover, it has given rise to the digitization of mathematics [6]. In the AI front, large language models (LLMs) are improving at mathematical reasoning in natural language, and at the same time, they have shown a remarkable ability to learn the language of those formal verification systems and write mathematical proofs in formal language. This has led to the field of Automated Theorem Proving (ATP) where the standard approach is to prompt the LLM to generate a number of candidate proofs for a given theorem which will then be verified automatically using the formal verification system. Better models, better training sets, and better training methods has led to significant advances in this field [7, 8, 9, 10, 11]. Despite these advances, the LLMs do not really get the chance to interact with the verification system the way a human does. LLMs generate many possible proofs, sometimes as many as tens of thousands,",
    "conclusion": ".\nDependence on base model\u2019s proof sketch quality.Apollo\u2019s ability to produce a correct formal\nproof largely depends on the base model and whether its initial proof has a coherent proof sketch. As\n[44] observe, models often tend to \u201ccut corners,\u201d producing proofs that are much shorter than the\nrigorous, multi-step arguments required to generate a valid proof. When a base model fails to write a\ncorrect proof strategy (e.g. omitting critical lemmas or suggesting irrelevant tactics, or taking a wrong\napproach), the Apollo is less likely to repair such a proof. Enhancing the robustness of Apollo to very\npoor initial sketches remains an open challenge.\n6 Conclusion\nIn this work, we present Apollo, a novel, modular fully automated agentic system that combines syntax\ncleaning, automatic solvers, and LLM -driven sub -proof generation to transform an LLM\u2019s initial\nproof sketch into a fully verified Lean4 proof. This framework harnesses the full power of the Lean\ncompiler and integrated solvers, merging them with LLM systems. Applied across five whole -proof\ngeneration models, ranging from general -purpose LLMs (o3 -mini, o4 -mini) to specialized provers\n(Kimina -Prover -Preview -Distill -7B, Goedel -Prover -SFT, Goedel-V2), Apollo consistently establishes\nnew best accuracies on the miniF2F benchmark while reducing token and sampling budgets by one\nto two orders of magnitude. Our empirical analysis shows that Apollo not only raises overall proof\nsuccess rates but also guides models to generate longer, more structured proofs, as evidenced by a\ndrastic increase in average successful proof length. We further demonstrate how the recursion -depth\nparameter rtrades off sample complexity against accuracy, achieving robust gains with only a few\nhundred samples. We believe that Apollo\u2019s collaboration between LLMs, automatic solvers, and the\nLean compiler defines a paradigm of agentic systems that produce high -quality, verifiable proofs for\nincreasingly challenging theorems.\n10"
  },
  {
    "id": "2310.03731v1",
    "title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
    "authors": "Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li",
    "published": "2023-10-05",
    "category": "cs.CL",
    "arxiv_url": "http://arxiv.org/abs/2310.03731v1",
    "pdf_url": "https://arxiv.org/pdf/2310.03731v1",
    "abstract": "MATHCODER : S EAMLESS CODE INTEGRATION IN LLM S FOR ENHANCED MATHEMATICAL REASONING Ke Wang1\u2217Houxing Ren1\u2217Aojun Zhou1\u2217Zimu Lu1\u2217Sichun Luo3\u2217 Weikang Shi1\u2217Renrui Zhang1Linqi Song3Mingjie Zhan1\u2020Hongsheng Li1,2\u2021 1Multimedia Laboratory (MMLab), The Chinese University of Hong Kong 2Shanghai Artificial Intelligence Laboratory3City University of Hong Kong {wangk.gm, renhouxing, aojunzhou, zmjdll}@gmail.com hsli@ee.cuhk.edu.hk ABSTRACT The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language ,code , and execution results . We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/ MathCoder . 1 I NTRODUCTION Recently, closed-source large language models (LLMs) such as GPT-4 (OpenAI, 2023) and PaLM- 2 (Anil et al., 2023), paired with methods such as Chain-of-Thought (CoT) (Wei et al., 2022) and Program-Aided Language models (PAL) (Gao et al., 2023), have shown remarkable performance on mathematical reasoning tasks. In contrast, current open-source LLMs (Touvron et al., 2023; Penedo et al., 2023; Zhang et al.,",
    "introduction": "2022) still lag significantly behind in this area. Even Llama-2- 70B (Touvron et al., 2023), one of the most potent open-source models, only scores 56.8% and 13.5% respectively on GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets, remarkably lower than GPT-4 Code Interpreter1, which scores 97% and 69.7% (Zhou et al., 2023a). To narrow the gap between open-source and closed-source models in math problem solving, recent works, such as the WizardMath (Luo et al., 2023) and RFT (Yuan et al., 2023), have tried to tune open-source models with math problems and CoT solutions, achieving a significant gain in performance compared to their base model, Llama-2. On the other hand, methods such as PAL (Gao et al., 2023), PoT (Chen et al., 2022), and CSV (Zhou et al., 2023a) encourage code usage in solving \u2217Equal contribution. \u2020Project leader. \u2021Corresponding author. 1https://openai.com/blog/chatgpt-plugins#code-interpreter 1arXiv:2310.03731v1 [cs.CL] 5 Oct 2023 Table 1: Comparison with different Instruction-following datasets. The baseline datasets include recent RFT-u13b (Yuan et al., 2023) and WizardMath (Luo et al., 2023). Datasets Seed Annotation RFT-100k GSM8K Llama WizardMath-96k GSM8K+MATH GPT-4 Ours-49k GSM8K+MATH GPT-4 Ours-80k GSM8K+MATHGPT-4 + Self-distillation 7B 70B1020304050607080Accuracy 17.824.846.250.773.1MathCoder WizardMath (SoTA) Llama-1 RFTFigure 1: Performance comparison between MathCoder, WizardMath, and Llama-1 RFT models with different model sizes. math problems, showing promising improvements when paired with closed-source models like GPT- 3.5, GPT-4 and GPT-4 Code Interpreter. In particular, GPT-4 Code Interpreter surpasses the previous SOTA by a clear margin. Recent study (Zhou et al., 2023a) shows that this excellent performance can be attributed to its ability to generate and assess the execution results of a chain of code interlaced with natural language reasoning steps. However, existing open-source models fail to benefit from this sophisticated mechanism since they lag behind closed-source models in both code generation and natural language reasoning. Therefore, we still lack an effective recipe to deliver open-source models to solve math problems in a manner similar to GPT-4 Code Interpreter . In this paper, leveraging the strengths of GPT-4 Code Interpreter (Zhou et al., 2023a), we introduce a simple yet effective framework, MathCoder , designed to enhance the mathematical reasoning capabilities of open-source models. This framework can be categorized into two parts: (1) math instruction-following dataset construction and (2) customized supervised fine-tuning. Specifically , the instruction-following dataset, termed as MathCodeInstruct , consists exclusively of 80k math problems and their corresponding solutions. Each solution is interwoven with natural language for reasoning ,code for execution , and execution results . The comparison between MathCodeInstruct and other math instruction-tuning datasets is shown in Tab. 1. MathCodeInstruct is created in two steps. The first step is collecting GPT-4 Code Interpreter- style solutions for the GSM8K and MATH training sets. GSM8K and MATH are two important datasets of math problems for improving and evaluating models\u2019 mathematical abilities, which consist of grade school math word problems and challenging competition mathematics problems, respectively. Using this data, we trained our initial models, termed MathCoder-Initial . The second step is to augment more math problems by",
    "conclusion": "E m3, n3 = 2, 1 # ratio for point F # Using section formula to find coordinates of D, E, F D = ((m1 *B[0] + n1 *A[0]) / (m1 + n1), (m1 *B[1] + n1 *A[1]) / (m1 + n1)) E = ((m2 *C[0] + n2 *B[0]) / (m2 + n2), (m2 *C[1] + n2 *B[1]) / (m2 + n2)) F = ((m3 *A[0] + n3 *C[0]) / (m3 + n3), (m3 *A[1] + n3 *C[1]) / (m3 + n3)) D, E, F <|endofblock |> <|execution |> ((6.0, 4.0), (7.0, 10.0), (2.0, 4.0)) < |endofblock |> <|text|> The coordinates of the points are as follows: - Point Dis located at (6,4) - Point Eis located at (7,10) - Point Fis located at (2,4) Now, let\u2019s proceed to find the areas of triangles ABC andDEF using the formula for the area of a triangle, and then find the ratio of these areas. <|endofblock |> <|code|> # Function to find the area of a triangle given its vertices def triangle_area(v1, v2, v3): return 0.5 *abs(v1[0] *(v2[1] - v3[1]) + v2[0] *(v3[1] - v1[1]) + v3[0] *(v1[1] - v2[1])) # Areas of triangles ABC and DEF area_ABC = triangle_area(A, B, C) area_DEF = triangle_area(D, E, F) # Ratio of areas ratio_areas = area_DEF / area_ABC area_ABC, area_DEF, ratio_areas <|endofblock |> <|execution |> (36.0, 12.0, 0.3333333333333333) < |endofblock |> <|text|> So, the ratio of the area of triangle DEF to the area of triangle ABC is1 3\u2713or0.3333 . <|endofblock |><|endofmessage |> Figure 23: Example of LCE solution with special token. The problem is from the test set of MATH in geometry with id 500. In contrast to CoT, which consists solely of natural language, and PoT, which consists solely of code, our LCE solution intertwines natural language, code, and execution results. 32"
  },
  {
    "id": "2410.16429v2",
    "title": "Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4",
    "authors": "Leni Aniva, Chuyue Sun, Brando Miranda, Clark Barrett, Sanmi Koyejo",
    "published": "2024-10-21",
    "category": "cs.LO",
    "arxiv_url": "http://arxiv.org/abs/2410.16429v2",
    "pdf_url": "https://arxiv.org/pdf/2410.16429v2",
    "abstract": ". Machine-assisted theorem proving refers to the process of\nconducting structured reasoning to automatically generate proofs for\nmathematical theorems. Recently, there has been a surge of interest in\nusing machine learning models in conjunction with proof assistants to\nperform this task. In this paper, we introduce Pantograph, a tool that\nprovides a versatile interface to the Lean 4 proof assistant and enables\nefficient proof search via powerful search algorithms such as Monte Carlo\nTree Search. In addition, Pantograph enables high-level reasoning by\nenabling a more robust handling of Lean 4\u2019s inference steps. We provide\nan overview of Pantograph\u2019s architecture and features. We also report on\nan illustrative use case: using machine learning models and proof sketches\nto prove Lean 4 theorems. Pantograph\u2019s innovative features pave the way\nfor more advanced machine learning models to perform complex proof\nsearches and high-level reasoning, equipping future researchers to design\nmore versatile and powerful theorem provers.\n1",
    "introduction": "machine learning to automatically search for proofs in proof assistants (e.g., [ 21], [22], [5], [6], [12], [11], [8], [9], [20]). While these efforts have produced promising results, many proofs are still beyond the reach of machine learning-based automation. In order to continue to make progress in this area, several challenges need to be addressed. One of these challenges is the need for better interfaces betweenarXiv:2410.16429v2 [cs.LO] 31 Jan 2025 proof assistants and machine learning systems. In this paper, we introduce Pan- tograph ,1an API and Read-Eval-Print Loop (REPL) for Lean 4, whose primary goal is to provide a convenient interface for training and evaluating theorem proving agents. The name \u201cPantograph\u201d alludes to the process of recording a proof during proof search.2 The main motivation for creating Pantograph is to overcome the limitations of the interface provided by the Lean 4 Language Server Protocol (LSP), which is the standard interface provided for interactive use by a human user. Although the LSP provides interactive feedback for a human operator of Lean 4, it suffers from a number of problems as a machine interface. The LSP interface requires its user to keep track of positions of a cursor in text, and a machine user would be burdened with tracking these redundant data. Moreover, there is no straightforward way to extract tactic training data from the LSP interface or sketch out a proof to be finished by automation tactics. In contrast, Pantograph is designed from the ground up as an efficient and convenient interface for machine (and especially machine learning) agents. The main contributions of Pantograph are: 1.Unlike prior work, the user can decide to solve goals independently. This enables more powerful search algorithms such as Monte Carlo Tree Search (MCTS), which have been successful in other domains (e.g., AlphaGo and AlphaZero [ 17,18]), achieving superhuman performance on complex games like Go, Chess, and Shogi.3To do this, Pantograph handles metavariable coupling, which is a phenomenon that complicates tree search [13]. 2.In contrast to prior work in Lean 4 [ 23], Pantograph supports the use of the advanced reasoning steps (called tactics) have,let,conv, and calc. These tactics are crucial for supporting high-level reasoning strategies like proof sketching [8]. 3.Pantograph fully supports essential data extraction tasks (e.g., it can extract the before- and after-goal states of tactic executions, which are usually not available in raw Lean 4 scripts). In addition, Pantograph introduces several novel data extraction capabilities, including the ability to extract entire proof scripts with associated comments, which can be used for tasks like autoformalization, and the important ability to extract proof representations as programs, which allows for one-shot prediction of proofs. 4.Pantograph provides feedback from partially executed convand calctactics, which was not possible in preceding works. 5.Pantograph allows the user to resume an incomplete proof containing the sorrykeyword in Lean 4. This is useful for machine learning models which produce a proof draft before resolving the details in the proofs. 1https://github.com/stanford-centaur/PyPantograph 2A Pantograph is a mechanism for recording the movement of a pen",
    "conclusion": ".\n6 Conclusion\nIn this work, we introduce Pantograph, a Machine-to-Machine interaction library\nfor Lean 4. We compare its features against existing tools used for training\nmachine learning models for theorem proving, and we provide a list of its novel\nfeatures. We also illustrate an application by implementing the first Lean 4\nimplementation of the Draft-Sketch-Prove approach.\nIn future work, we plan to use Pantograph to build and train various machine\nlearning approaches for theorem proving. We also expect and hope that others\nwill use it in interesting and novel ways and that these use cases will provide\nfeedback for additional improvements and extensions of Pantograph.\nOur evaluation also demonstrates one way that formal tools like Lean can be\nused to address potential harm from Language Models such as the one used in\nthe evaluation section. Language models, though powerful, still face the problem\nof hallucination and generation of illogical results. These can be mitigated by\napplying formal techniques to the results produced of language models. The\ndraft-sketch-prove experiment is an instance of this general idea, where proof\nautomation formally checks the potentially incorrect result generated by an LLM.\nIn the future, Pantograph could be used for other hybrid reasoning approaches\ncombining generative AI and formal reasoning."
  },
  {
    "id": "2506.17104v1",
    "title": "Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving",
    "authors": "Chuxue Cao, Mengze Li, Juntao Dai, Jinluan Yang, Zijian Zhao, Shengyu Zhang, Weijie Shi, Chengzhong Liu, Sirui Han, Yike Guo",
    "published": "2025-06-20",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2506.17104v1",
    "pdf_url": "https://arxiv.org/pdf/2506.17104v1",
    "abstract": "Large language models (LLMs) have shown\npromising first-order logic (FOL) reasoning\ncapabilities with applications in various areas.\nHowever, their effectiveness in complex mathe-\nmatical reasoning involving multi-step FOL de-\nductions is still under-researched. While LLMs\nperform competitively on established mathe-\nmatical reasoning benchmarks, they struggle\nwith multi-step FOL tasks, as demonstrated by\nDeepseek-Prover-V2-7B\u2019s low accuracy (4.2%)\non our proposed theorem proving dataset. This\nissue arises from the limited exploration of di-\nverse proof strategies and the potential for early\nreasoning mistakes to undermine entire proofs.\nTo address these issues, we propose DREAM, a\nself-adaptive solution that enhances the Diver-\nsity and REAsonability of LLMs\u2019 generation\nstrategies. DREAM incorporates an Axiom-\nDriven Strategy Diversification mechanism to\npromote varied strategic outcomes and a Sub-\nProposition Error Feedback to help LLMs re-\nflect on and correct their proofs. Our contri-\nbutions include pioneering advancements in\nLLMs\u2019 mathematical reasoning through FOL\ntheorem proving, introducing a novel inference\nstage solution that improves performance by\n0.6% to 6.4%, and providing a curated dataset\nof 447 mathematical theorems in Lean 4 format\nfor evaluation.\n1",
    "introduction": "2: Sub-proposition Error Feedback Axiom 1 ... Strategy generation prompt Strategy Sub-pr oposition 1: Assume member A aI_bDa and derive a contradiction - Infer ence rules used: Proof by contradiction - Axioms used: member_of_intersection_is_member_of_set1,.. Conjectur e: theorem prove_aI_bDa_is_empty {A : \u03b1} (a b : \u03b1) : \u00ac HasMember .member A (aI_bDa a b) := sorry Axiom 2 Axiom m Axiom Set 1 Axiom Set n LLM invocations ... Randomly selected Axiom Set Prove prompt 1 Strategy Formal Pr oof theorem prove_aI_bDa_is_empty {A : \u03b1} (h_dif f : difference b a bDa) (h_no_inter : \u00ac intersection a bDa aI_bDa) : \u00ac member A aI_bDa := by by_contra h_mem -- Proof by contradiction... Error messages Sub-proposition identified Proof Sub-pr oposition Err or Feedback Sub-pr oposition 1 /- Show that assuming menmbership in aIbDa leads to a contradiction -/ ... /- Error messages -/ ... Analyze prompt Sub-proposition error feedback from previous attempts Error Analysis 1. Err or Patterns: - Common mistakes identified - Root causes of errors - Typical misunderstandings 2. Strategic Recommendations - Propose specific proof strategies - Suggest alternative tactics - Outline required corrections Formal compiler Step1: Strategy Generation Step3: Sub-proposition Error Feedback Step2: Theorem Proving Axiom 1 ... Strategy generation prompt Strategy Sub-proposition 1: Assume member A aI_bDa and derive a contradiction - Inference rules used: Proof by contradiction - Axioms used: member_of_intersection_is_member_of_set1,.. Conjecture: theorem prove_aI_bDa_is_empty {A : \u03b1 } (a b : \u03b1 ) : \u00ac HasMember.member A (aI_bDa a b) := sorry Axiom selection prompt Axiom 2 Axiom m Axiom Set 1 Axiom Set n LLM invocations ... Randomly selected Axiom Set Prove prompt Strategy Formal Proof theorem prove_aI_bDa_is_empty {A : \u03b1 } (h_di\ufb00 : di\ufb00erence b a bDa) (h_no_inter : \u00ac intersection a bDa aI_bDa) : \u00ac member A aI_bDa := by by_contra h_mem -- Proof by contradiction... Error messages error: unsolved goals\\n \u03b1 Type\\nb a bDa aI_bDa : \u03b1 \u2192 Prop\\nA : \u03b1 \\nh_di\ufb00 : di\ufb00erence b a bDa\\nh_no_inter : ... Sub-proposition identified Proof Error messages Sub-proposition Error Feedback Sub-proposition 1 /- Show that assuming menmbership in aIbDa leads to a contradiction -/ ... /- Error messages -/ ... Analyze prompt Sub-proposition error feedback from previous a\ue039empts Error Analysis 1. Error Pa\ue039erns: - Common mistakes identified - Root causes of errors - Typical misunderstandings 2. Strategic Recommendations - Propose specific proof strategies - Suggest alternative tactics - Outline required corrections Prove Prompt Alternative Strategy Formal compiler Prove prompt 2 Axiom selection prompt Sub-proposition labeling prompt Axioms and Conjectur e in TPTP Axioms and Conjectur e in Lean 4 Formal Compiler Wrong Step 2: Post-pr ocessing variable (Point : Type) (Line : Type) variable (distinct_points : Point \u2192 Point \u2192 Prop)... axiom apart1 : \u2200 (X : Point), \u00ac distinct_points X X axiom apart4 : \u2200 (X Y Z : Point), distinct_points X Y \u2192 (distinct_points X Z \u2228 distinct_points Y Z)... theorem conjecture : \u2200 (X Y Z : Point), distinct_points X Y \u2192 apart_point_and_line Z (line_connecting X Y) \u2192 apart_point_and_line X (line_connecting Z Y)",
    "conclusion": "multiplicative_identity '-/ --Sub-proposition 2: Multiplication is Well -Defined have h\u2082 := well_definedness_of_multiplication multiplicative_identity a have h\u2083 := well_definedness_of_multiplication multiplicative_identity b --Sub-proposition 3: Multiplication is Compatible with Equality have h\u2084 := compatibility_of_equality_and_multiplication a multiplicative_identity b have h\u2085 := compatibility_of_equality_and_multiplication d multiplicative_identity b --Sub-proposition 4: `a`is Defined have h\u2086 := a_equals_d have h\u2087 := a_is_defined have h\u2088 := b_is_defined have h\u2089 := d_is_defined intro h simp_all <;> aesopFigure 16: An illustrative example of cascading error. System prompt : Your task is to convert TPTP format axioms and conjectures into Lean 4 format . Follow these guidelines : 1. Type Declarations : - Declare all necessary types using `Type ` - Define type variables when needed using uppercase letters (e.g., `A`,`B`) 2. Axiom Conversion : - Convert each TPTP axiom into a complete Lean 4 definition - Use appropriate Lean 4 syntax for logical operators : - Do not use `sorry `in axiom definitions 3. Conjecture Conversion : - Convert the conjecture into a theorem statement - Use `theorem `for the declaration - End the theorem with `sorry ` - Do not provide the proof 4. Code Format : - Wrap all Lean 4 code with ```lean ``` markers - Use proper indentation - Include necessary imports - Add brief comments explaining complex translations 5. Variable Handling : - Declare all variables with appropriate types - Maintain consistent variable naming between axioms and conjecture - Use meaningful variable names when possible Please ensure each conversion preserves the original logical meaning while following Lean 4 's syntax and type system . User prompt : Input TPTP Format : Axioms : { axioms } Conjecture : { conjecture } Please provide the Lean 4 conversion following the guidelines above . Figure 17: Prompts for converting first-order axioms and conjectures from TPTP format to Lean4 format."
  },
  {
    "id": "2511.06522v1",
    "title": "FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis",
    "authors": "Jan Ondras, Marek \u0160uppa",
    "published": "2025-11-09",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2511.06522v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06522v1",
    "abstract": "Mathematical reasoning requires abstracting symbolic rules from visual patterns\u2014\ninferring the infinite from the finite. We investigate whether multimodal AI systems\npossess this capability throughFractalBench, a benchmark evaluating fractal pro-\ngram synthesis from images. Fractals provide ideal test cases: Iterated Function\nSystems with only a few contraction maps generate complex self-similar patterns\nthrough simple recursive rules, requiring models to bridge visual perception with\nmathematical abstraction. We evaluate four leading MLLMs\u2014GPT-4o, Claude 3.7\nSonnet, Gemini 2.5 Flash, and Qwen 2.5-VL\u2014on 12 canonical fractals. Models\nmust generate executable Python code reproducing the fractal, enabling objective\nevaluation. Results reveal a striking disconnect: 76% generate syntactically valid\ncode but only 4% capture mathematical structure. Success varies systematically\u2014\nmodels handle geometric transformations (Koch curves: 17-21%) but fail at branch-\ning recursion (trees: <2%), revealing fundamental gaps in mathematical abstraction.\nFractalBenchprovides a contamination-resistant diagnostic for visual-mathematical\nreasoning and is available at https://github.com/NaiveNeuron/FractalBench\nCantor Set Cantor Dust Koch Curve Koch Snowflake\nSierpi ski Gasket\n Sierpi ski Carpet\n Sierpi ski Pentagon\n Heighway Dragon\nL\u00e9vy Dragon McWorter Pentigree Pythagoras Tree Symmetric Binary Tree\nFigure 1: Twelve canonical fractals testing different mathematical reasoning capabilities: linear\nrecursion (Cantor), geometric transformations (Koch), multi-scale self-similarity (Sierpi \u00b4nski), space-\nfilling curves (dragons), and branching recursion (trees). All defined via Iterated Function Systems.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: MATH-AI.arXiv:2511.06522v1  [cs.AI]  9 Nov 2025\n1",
    "introduction": "compactly defined by an Iterated Function System (IFS) [ 1,2]\u2014typically 2-8 contraction mappings\u2014yet generates arbitrarily complex patterns through recursive self-similarity. Successfully synthesizing fractal code demands three interconnected capabil- ities: recognizing scale invariance across recursive levels, inferring precise geometric transformations from visual evidence, and achieving recursive abstraction\u2014understanding the generative process rather than enumerating visible patterns. We introduceFractalBench, comprising 12 canonical fractals spanning distinct challenges: Koch curves test geometric transformations, Sierpi \u00b4nski structures probe multi-scale self-similarity, dragon curves evaluate space-filling navigation, and tree fractals assess branching recursion. This diversity enables systematic diagnosis: which mathematical capabilities do current models possess, and where do they fail? Evaluating four leading MLLMs on 7,320 fractal images (610 unique test images across 12 model- prompt combinations) reveals a striking disconnect: 76% execution success but only 4% visual correctness. Koch curves achieve 17-21% accuracy, Sierpi \u00b4nski fractals 3-18%, while tree fractals fail catastrophically at <2%, revealing models can compose local operations but lack recursive abstraction. This work makes three contributions to understanding visual-mathematical reasoning in AI systems. First, we establish a diagnostic framework connecting fractal synthesis to specific mathematical reasoning requirements, enabling systematic capability assessment. Second, we provide empirical evidence that current MLLMs possess geometric capabilities but fundamentally lack recursive abstraction\u2014findings with implications for mathematical AI beyond fractals. Third, we demonstrate contamination-resistant evaluation through parameterizable complexity, offering a methodology applicable to future benchmarking efforts. 2 Related Work Existing benchmarks reveal gaps in visual-mathematical reasoning.TurtleBench[ 3] achieves only 19% accuracy on simple geometric shapes, testinggeometric perception.MathVista[ 4] andMATH- Vision[ 5] evaluate mathematical problem-solving with visual contexts, whileMATHGLANCE[ 6] reveals models\u201cdo not know where to look\u201din mathematical diagrams.GeoGramBench[ 7] targets geometric program reasoning, showing performance degradation with structural complexity. These benchmarks primarily testapplyingmathematical knowledge to solve visual problems. FractalBenchtestsmathematical abstraction\u2014inferring recursive generative rules from self-similar patterns. WhereTurtleBench[ 3] asks \u201ccan you draw what you see?\u201d,FractalBenchasks \u201ccan you infer the infinite process generating finite observations?\u201d This capability\u2014abstracting symbolic rules from visual examples\u2014is central to mathematical discovery and reasoning. Fractals uniquely target this gap through recursive self-similarity, precise geometric transformations, and objective pixel-perfect evaluation.FractalBenchis not a general vision or code benchmark, but rather a targeted diagnostic of visual-mathematical abstraction. Notably, the same difficulty patterns we observe (e.g., failures on branching recursion) appear in broader benchmarks such asGeoGramBench[ 7], MathVista[ 4], andMATHGLANCE[ 6], indicating that the limitations we expose are conceptual rather than API-specific. See App. A for a comprehensive survey. 3 FractalBench 3.1 Fractal Definitions via Iterated Function Systems Self-similar fractals are defined as attractors of contractive Iterated Function Systems (IFS). Given contraction mappings f1, . . . , f m:Rd\u2192Rd, the IFS attractor is the unique compact set K satisfying K=Sm i=1fi(K). We consider 12 classic fractals [ 1,2] (Fig. 1), spanning Cantor sets, Koch curves, Sierpi \u00b4nski structures, dragon curves, and tree fractals. Each has contraction ratio r\u2208(0,1) determining scale reduction per iteration. Complete definitions are provided in App. B, and parameters in App. C. 2 3.2 Benchmark Design and Mathematical Reasoning Requirements FractalBenchcomprises 610 images ( 1,024\u00d71,024 pixels, 4-12",
    "conclusion": "specific geometric or recursive properties models capture versus miss. Such metrics would distinguish, for example, between code that produces visually similar output through incorrect means (e.g., iterative approximation) versus code that implements the correct generative process with minor visual artifacts. K.2 Benchmark Scope Program Synthesis Baselines.FractalBenchis designed as a diagnostic benchmark for current multimodal large language models (MLLMs), focusing on vision-to-code reasoning rather than full program synthesis pipelines. We therefore do not include comparisons with traditional program synthesis baselines such as symbolic search, neurosymbolic inference, and constraint-based syn- thesis [ 15,16]. Such comparisons would, however, provide useful performance context, situating MLLM capabilities relative to specialized synthesis techniques. Model Coverage.Our evaluation covers four representative MLLMs available at the time of benchmark development (GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL). More recent reasoning-specialized models (e.g., OpenAI o1, DeepSeek-R1) or math-focused models are not included. Evaluating such models would help determine whetherFractalBenchdistinguishes genuinely stronger reasoning capabilities or primarily exposes universal failure modes in visual- mathematical abstraction. Expanding coverage to these systems would sharpen the benchmark\u2019s diagnostic scope. K.3 Analysis Limitations Observational Findings.The observed differences in prompting strategies\u2014where direct code generation unexpectedly outperforms reasoning-first prompts (Sec. 4.2)\u2014remain observational. We do not perform stepwise ablations to isolate whether the performance gap stems from prompt complexity overload, genuine incompatibility between verbal reasoning and geometric precision, or other factors. Controlled experiments varying prompt length, reasoning depth, and instruction complexity independently would enable causal interpretation beyond our current hypotheses. Model Improvement Feedback Loop.WhileFractalBenchsystematically identifies characteristic failure modes in recursive and branching reasoning, we do not demonstrate how these diagnostics could guide targeted model improvements through few-shot tuning, structured prompting refinements, or tool integration. Establishing such a feedback loop\u2014where benchmark insights lead to measurable capability gains\u2014would further validateFractalBenchas a tool for advancing visual\u2013mathematical reasoning research. 25"
  },
  {
    "id": "2412.16075v1",
    "title": "Formal Mathematical Reasoning: A New Frontier in AI",
    "authors": "Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, Dawn Song",
    "published": "2024-12-20",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2412.16075v1",
    "pdf_url": "https://arxiv.org/pdf/2412.16075v1",
    "abstract": "AI for Mathematics (AI4Math) is not only intriguing intellectually but also crucial\nfor AI-driven discovery in science, engineering, and beyond. Extensive efforts on\nAI4Math have mirrored techniques in NLP, in particular, training large language\nmodels on carefully curated math datasets in text form. As a complementary yet\nless explored avenue, formal mathematical reasoning is grounded in formal systems\nsuch as proof assistants, which can verify the correctness of reasoning and provide\nautomatic feedback. In this position paper, we advocate for formal mathematical\nreasoning and argue that it is indispensable for advancing AI4Math to the next\nlevel. In recent years, we have seen steady progress in using AI to perform formal\nreasoning, including core tasks such as theorem proving and autoformalization, as\nwell as emerging applications such as verifiable generation of code and hardware\ndesigns. However, significant challenges remain to be solved for AI to truly master\nmathematics and achieve broader impact. We summarize existing progress, discuss\nopen challenges, and envision critical milestones to measure future success. At\nthis inflection point for formal mathematical reasoning, we call on the research\ncommunity to come together to drive transformative advancements in this field.\n1",
    "introduction": "field is that mathematical problems are a proxy for a broad array of reasoning and planning tasks. Another attraction is that math plays a foundational role in quantitative disciplines, so AI4Math has the potential to revolutionize AI for science, engineering, and beyond. For these reasons, designers of large language models (LLMs) [ 3,4] have frequently highlighted LLMs\u2019 success in math problems, and there have also been efforts to build AI systems that outperform humans at math competitions [5\u20137]. Given the importance of AI4Math, substantial research has been dedicated to developing math LLMs, using techniques borrowed from natural language processing (NLP). A common approach is to continue pretraining LLMs on math data, such as arXiv papers and web pages from MathOverflow, and then finetune the model on curated datasets of math problems with detailed, step-by-step solutions. We call this the \u201cinformal\u201d approach to distinguish it from the formal approach that will be introduced later (Sec. 2). Just like LLMs in general, math LLMs have a simple recipe, but the secret sauce is often data curation [ 8\u201311]. Carefully curated training data plus inference-time techniques, including chain-of-thought prompting [ 12], self-consistency [ 13], and tool use [ 14], have led to remarkable success on widely used benchmarks such as GSM8K [ 15] and MATH [ 16], as well as in the AIMO Progress Prize [ 6]. However, at the time of writing, the success of the informal approach has been Preprint. Under review.arXiv:2412.16075v1 [cs.AI] 20 Dec 2024 mostly limited to high school math not exceeding the AIME level.1This raises a key question: How far can we go by scaling up the informal approach? Will it enable math LLMs to solve more challenging competition problems (e.g., IMO, International Mathematical Olympiad) or even problems in mathematical research? Moving from high school to more advanced mathematics, the informal approach faces challenges that are hard to resolve by merely scaling up the training. First, training math LLMs requires high-quality data, which is scarce in advanced mathematics. For novel research math problems, it is infeasible to find solutions to similar problems on the Internet or manually annotate the data on a large scale. Without scaling up the data, we cannot fully benefit from the scaling laws for LLMs [ 18,19]. Second, solutions to many advanced problems are not numbers that can be evaluated by comparing them with the ground truth. Instead, they carry out a chain of intricate reasoning steps, e.g., a proof. LLMs are notorious for hallucinating seemingly valid reasoning steps, making it challenging to evaluate the correctness of model output or collect useful feedback for learning. These challenges are difficult to address by scaling up the informal approach during training. If training-time scaling is not enough, what else do we need? One emerging direction, exemplified by OpenAI o1 [ 17], is to scale up the informal approach during inference, potentially combining search with neural verifiers to mitigate hallucinated reasoning [ 15]. While this approach has gained traction, its effectiveness on advanced mathematical problems is an",
    "conclusion": ", community building, and a clear roadmap for the future.\n26\nThe narrative in this paper is rooted in the approach of AI and machine learning researchers, empha-\nsizing general-purpose learning algorithms applied to well-defined tasks that can be automatically\nevaluated using benchmarks. While this paradigm has dominated AI research in recent decades,\nit has limitations. Mathematicians have explored many ways of using AI in their work, including\nbrainstorming ideas and inspirations, writing assistance, and organizing or searching mathematical\nliterature. Many of these use cases, however, resist straightforward evaluation through benchmarks or\nautomated metrics. Even in areas like theorem proving\u2014where automated evaluation is feasible\u2014\nperformance on benchmarks may not fully capture what human users find meaningful or helpful\n(Sec. 4.3). For example, benchmarks such as LeanDojo [ 35] and PutnamBench [ 209] often fail to\nmeasure how the prover performs on new and evolving formalization projects, such as formalizing the\nproof of Fermat\u2019s Last Theorem [ 288]. This gap underscores the need for human-centered evaluation\napproaches that draw on insights from human-computer interaction and cognitive science [ 289,290]."
  },
  {
    "id": "2507.23726v2",
    "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
    "authors": "Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, Cheng Ren, Jiawei Shen, Wenlei Shi, Tong Sun, He Sun, Jiahui Wang, Siran Wang, Zhihong Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Yuchen Wu, Yihang Xia, Huajian Xin, Fan Yang, Huaiyuan Ying, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang, Ge Zhang, Tianyun Zhao, Jianqiu Zhao, Yichi Zhou, Thomas Hanwen Zhu",
    "published": "2025-07-31",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2507.23726v2",
    "pdf_url": "https://arxiv.org/pdf/2507.23726v2",
    "abstract": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement\nlearning with long chain-of-thought, yet they continue to struggle with theorem proving due\nto the lack of clear supervision signals when solely using natural language. Dedicated domain-\nspecific languages like Lean provide clear supervision via formal verification of proofs, enabling\neffective training through reinforcement learning. In this work, we propose Seed-Prover , a lemma-\nstyle whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean\nfeedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design\nthree test-time inference strategies that enable both deep and broad reasoning. Seed-Prover\nproves 78.1%of formalized past IMO problems, saturates MiniF2F, and achieves over 50% on\nPutnamBench, outperforming the previous state-of-the-art by a large margin. To address the\nlack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry ,\nwhich outperforms previous formal geometry engines. We use these two systems to participate in\nIMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in\nautomated mathematical reasoning, demonstrating the effectiveness of formal verification with\nlong chain-of-thought reasoning.\nProject Page: https://github.com/ByteDance-Seed/Seed-Prover\n2021 2022 2023 2024 2025\nPublication Date30405060708090100Pass Rate on miniF2F-test (%)\nProof Artifact\nCo-trainingCurriculum LearningHypertree Proof\nSearch\nReProverCOPRADeepSeek-Prover-V1\nInternLM2-Math-Plus\nTheoremLlamaLean-ST aRInternLM2-StepProverDeepSeek-Prover-V1.5\nABEL\nAlchemyInternLM2.5-StepProver\n3D-ProverHunyuanProver\nSTPBFS-Prover\nGoedel-ProverLeanabell-ProverKimina-Prover-PreviewDeepSeek-Prover-V2\nDSP+Kimina-ProverGoedel-Prover-V2 Delta-ProverSeed-Prover\nFigure 1 Growth in MiniF2F-Test performance over time.\n1arXiv:2507.23726v2  [cs.AI]  1 Aug 2025\n1",
    "introduction": "reinforcement learning to the training of large language models to prove mathematical statements. Unlike natural language, formal languages such as Lean can provide a clear and automatic signal on the correctness of a formalized proof. A noteworthy work from AlphaProof [3] uses Lean to successfully solve 3 problems from the 2024 International Mathematical Olympaid (IMO). AlphaProof demonstrates that LLMs using formal language are capable of proving very challenging problems that LLMs using natural language fail to prove. There are two types of LLM formal provers, step-level provers [ 3,14,22,23,25] and whole-proof generation provers[5,24]. Step-level provers incrementally generate Lean code line-by-line. While this enables close interaction with the Lean environment, it requires special scaffolding to generate a complete Lean proof, and the interaction is often too granular to allow high-level reasoning. In contrast, whole-proof models generate an entire Lean proof at once, but typically lack interaction with the Lean compiler. Recent work has shown that combining whole-proof models with long chain-of-thought reasoning [ 9,15,21] substantially outperforms step-level provers. In this work, we propose Seed-Prover , a whole-proof model with following features: \u2022Lemma-Style Proving : Seed-Prover tries to generate useful intermediate lemmas before proving the main statement. These lemmas serve as shared knowledge across different inference paths. \u2022Iterative Proof Refinement : Seed-Prover iteratively refines its proof based on Lean compiler feedback, previous proved lemmas, and self-summarization. \u2022Test-Time Scaling : We implement a three-tiered inference strategy that enables Seed-Prover to think both deeply and broadly\u2014allocating thinking budget to fine details while exploring interesting properties. \u2022SOTA Performance : Seed-Prover proves 5 out of 6 problems in IMO 2025, saturates MiniF2F [ 31] (shown in Figure 1), and outperforms prior work by up to 3\u00d7on multiple formal benchmarks. DuetothelackofsufficientgeometrysupportinLean, Seed-Proverincorporatesadedicatedgeometryreasoning engine Seed-Geometry . Similar to existing line of efforts in AlphaGeometry [2, 19] and TongGeometry [30], Seed-Geometry follows the forward-chaining design in the reasoning engine implementation, where the system derives all known facts by checking applicable rules until closure is reached. By backward-tracing fact dependencies, Seed-Geometry identifies the minimum dependency relations in a geometry problem\u2019s configuration, seperating the problem context from the auxiliary constructions necessary to prove a problem. Using statistics derived from more than past 20 years of math olympiad competitions, Seed-Geometry performs extensive search in the geometry space defined by its dedicated domain-specific language and establishes a repository of 230 million unique geometry problems requiring auxiliaries. A Seed model trained on such dedicated geometry data becomes an exceptionally effective neuro-symbolic geometry prover, where it fills in the missing auxiliary geometry elements and the geometry reasoning engine performs step-by-step forward- chaining, completing the final proof of a problem. In experiments, Seed-Geometry solves 43 of the IMO-AG-50 (vs.42 by AlphaGeometry 2), a benchmark that curates geometry problems of IMO from 2000 to 2024. It also sets a new state-of-the-art on the IMO shortlist geometry problems from 2000 to 2022, and notably solves the geometry problem of IMO 2025 under just 2 seconds. 2 Approach Here we introduce the two systems we used in IMO 2025,",
    "conclusion": "Zhao, Thomas Hanwen Zhu B LooKeng: An Easy-to-Use and Effective Python Interface for Lean Interacting with Lean poses significant challenges that limit the flexibility of Lean-based workflows. The most popular interface, LeanDojo [ 26], only supports earlier versions of Lean 4, restricting users from accessing Lean\u2019s newest updates. Furthermore, LeanDojo requires creating a Lean repository for interaction, which makes it impractical to use considering the massive scale of Lean interaction during model development and inference. To address these issues, we introduce LooKeng, a REPL4-based Python interface designed to simplify and accelerate the interaction process. LooKeng offers powerful features for developers while providing a user-friendly interface for end-users. The core functionality of LooKeng includes \u2018init_state\u2019, \u2018run_tac\u2019, and \u2018verify_proof\u2019. One can use LooKeng to interact with Lean step-by-step or verify an entire proof directly. The key features of LooKeng are summarized as follows: \u2022Stateless Design : A Lean state can be simultaneously processed using different LooKeng instances, enabling effortless scaling and sharing. \u2022Complex Tactics : Complex tactics such as apply?andall_goals are fully supported, with enhanced infotree integration to prevent false positive proofs. \u2022Version-Free : The LooKeng CLI allows users to manage and switch between different Lean versions. \u2022Memory Control : Users can easily track the memory consumption of the Lean backend, set custom thresholds, and automatically terminate processes when memory usage exceeds the limit. \u2022Proof Verification : LooKeng provides a straightforward method, \u2018verify_proof\u2019, to rigorously verify the final proof using the native Lean interface, ensuring correctness and reliability. \u2022Proof Simplification : LooKeng can remove useless tactics and hypothesis in the proof to obtain a simpler proof. \u2022Statement Negation : LooKeng is able to generate the negated statement of a statement. \u2022Multi-Concurrency Support : LooKeng can run as a service, handling thousands of concurrent requests via async architecture and resource isolation. 4https://github.com/leanprover-community/repl 12"
  },
  {
    "id": "2508.00271v2",
    "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning",
    "authors": "Hongjin Qian, Zheng Liu",
    "published": "2025-08-01",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2508.00271v2",
    "pdf_url": "https://arxiv.org/pdf/2508.00271v2",
    "abstract": "Technical Report METAAGENT : TOWARD SELF-EVOLVING AGENT VIA TOOL META-LEARNING Hongjin Qian, Zheng Liu\u2217 BAAI {chienqhj, zhengliu1026 }@gmail.com ABSTRACT In this work, we propose MetaAgent, an agentic paradigm inspired by the princi- ple of learning-by-doing, where expertise is developed through hands-on prac- tice and continual self-improvement. MetaAgent starts with a minimal work- flow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent au- tonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as meta tool learning , through which MetaAgent incrementally refines its reasoning and tool-use strate- gies, without changing model parameters or requiring further post-training. Eval- uated on challenging knowledge discovery benchmarks, including GAIA, Web- WalkerQA, and BrowseComp, MetaAgent consistently outperforms workflow- based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowl- edge discovery. We provide our source codes in this repository . 1 I NTRODUCTION Recent information-seeking systems powered by large language models (LLMs), such as ChatGPT, have achieved remarkable success (Ouyang et al., 2022; Google, 2023). These systems can provide instant answers to almost any type of information need, ranging from simple factual queries to complex information aggregation (Zhao et al., 2024a; Wang et al., 2024). However, when faced with more complex tasks that require synthesizing information across multi-step reasoning or interacting with external tools, most current LLMs struggle to deliver accurate solutions,",
    "introduction": "especially in deep knowledge discovery scenarios (Li et al., 2025c; Phan et al., 2025; Zhu et al., 2024). As illustrated in Figure 1, the query \u201cWhich ICLR 26 hotel is nearest to the venue within C275/night?\u201d exemplifies a simplified deep knowledge discovery scenario. To answer this ques- tion, an agent must first identify the list of conference hotels through web search, then check current currency exchange rates. By accurately converting prices to the target currency using computational tools and comparing the distances of eligible hotels to the venue, the agent can determine the correct answer. However, even when equipped with search tools, standard LLMs struggle with such queries because they cannot effectively manage the sequential reasoning and tool use required to integrate information from multiple sources and execute each step of the problem (Qian et al., 2025; Wei et al., 2025; Zhao et al., 2024a). To overcome such challenges, recent advances in agentic AI have focused on harnessing the rea- soning capabilities of LLMs not only for generating answers, but also for planning and executing multi-step solutions to complex tasks, enabling scalable deep reasoning even at test time (OpenAI, 2025; Snell et al., 2024). In this agentic paradigm, the LLM acts as a central coordinator: it de- composes a complex query into manageable sub-tasks, interacts with external tools, such as search \u2217Corresponding Author 1arXiv:2508.00271v2 [cs.AI] 1 Sep 2025 Technical Report Which ICLR 26 hotel is nearest to the venue within \u20ac275/night? Conference hotel price list: InterContinental 349 USD Toronto Marriott 379 USD \u2026 13% sale tax, 8.5%\u2026 USD to Euro exchange rate 379 USD x (1+TR) x CR = \u2026 349 USD x (1+TR) x CR = \u2026 \u2026 Compare eligible hotels by distance Vanilla LLM/RAGAgentic Solver Lacks dynamic interaction with external tools, making closed-loop reasoning impossible.Good AnswerIncorrect AnswerExternal Tools MetaAgent Dynamic Context Engineering Starting from Minimal Design Building In-House Tools Learning From Success & FailureSelf-Re%nementSelf-Expansion Learning Tools by Using Tools Figure 1: Illustration of a deep knowledge discovery task with sequential information dependen- cies, requiring multi-step reasoning and dynamic tool use to arrive at the correct answer. These tasks challenge standard LLMs and RAG approaches, which lack flexible tool interaction. Agentic systems address this limitation. MetaAgent demonstrates this approach by starting with a minimal, adaptable design and evolving through continual, data-driven task completion. engines or code compilers, and incrementally integrates the information obtained at each step into its reasoning trajectory (Li et al., 2025a; Jin et al., 2025b; Li et al., 2025b; Zheng et al., 2025; Dong et al., 2025). By following this paradigm, agentic systems can resolve intricate dependencies and deliver accurate solutions to deep knowledge discovery problems that static LLMs alone cannot handle (Li et al., 2025c). To implement agentic systems, there are currently two main approaches. The first approach is to manually design task-specific workflows, where human experts predefine how the agent should plan tasks and use tools (Li et al., 2025a; Wu et al., 2025c; Soni et al., 2025). Although this method can be effective",
    "conclusion": "Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572 , 2025b. Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the deep research. arXiv preprint arXiv:2502.04644 , 2025c. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR) , 2023. Weinan Zhang, Junwei Liao, Ning Li, Kounianhua Du, and Jianghao Lin. Agentic information retrieval. arXiv preprint arXiv:2410.09713 , 2024. Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao. Pro- cess vs. outcome reward: Which is better for agentic rag reinforcement learning, 2025. URL https://arxiv.org/abs/2505.14069 . Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna K. Qiu, and Lili Qiu. Retrieval aug- mented generation (rag) and beyond: A comprehensive survey on how to make your llms use external data more wisely, 2024a. URL https://arxiv.org/abs/2409.14924 . Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji- Rong Wen. A survey of large language models, 2024b. URL https://arxiv.org/abs/ 2303.18223 . Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environ- ments, 2025. URL https://arxiv.org/abs/2504.03160 . Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zhicheng Dou, and Ji-Rong Wen. Large language models for information retrieval: A survey, 2024. URL https://arxiv.org/abs/2308.07107 . 14"
  },
  {
    "id": "2501.11425v3",
    "title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training",
    "authors": "Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, Jiecao Chen",
    "published": "2025-01-20",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2501.11425v3",
    "pdf_url": "https://arxiv.org/pdf/2501.11425v3",
    "abstract": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Siyu Yuan1,2,\u2217,\u2020,Zehui Chen2,\u2217,\u2020,Zhiheng Xi1,\u2020,Junjie Ye1,2,\u2020,Zhengyin Du2,Jiecao Chen2 1Fudan University ,2ByteDance Seed \u2217Equal contributions ,\u2020Work done at ByteDance Seed Abstract Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive and agentic environments. Existing work primarily focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is notoriously difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent toReflect on the fly. Unlike traditional methods that reward or penalize actions solely based on correctness, our approach leverages Monte Carlo Tree Search (MCTS) to construct training samples that recover correct trajectories from erroneous ones. A key challenge of agent task reflection lies in the necessity for timely revision rather than waiting until the end of a rollout to revise errors. To address this, we introduce a model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in a failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that this approach continuously improves the model\u2019s ability to recover from errors and enables earlier/timely error correction. Extensive experiments on three representative interactive and agentic environments show that the proposed framework effectively equips agents",
    "introduction": "to identify and correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%). Date:March 25, 2025 Correspondence: Siyu Yuan at syyuan21@m.fudan.edu.cn Project Page: https://github.com/bytedance/Agent-R 1arXiv:2501.11425v3 [cs.AI] 24 Mar 2025 1 Introduction Large Language Models (LLMs) have become foundational tools in solving complex tasks across interactive and agentic environments [ 5,7,45,52,60]. These LLM-based agents are increasingly employed in scenarios requiring capabilities such as autonomous decision-making, error correction, and task optimization [ 22,31]. Despite the impressive performance of the existing methods, their reliance on behavior cloning from stronger experts poses significant limitations: due to the adoption of all-correct trajectories for training, it struggles to proactively self-correct errors, leading to cascading failures and suboptimal task performance [ 50,59]. This limitation arises from an inability to effectively detect errors or revise trajectories dynamically once errors occur, highlighting the need for methods emphasizing timely revision capabilities. Previous work has proposed methods relying on explicit error signals or reward functions for self- correction. However, these methods mainly focus on single-turn scenarios, such as code repair [ 4,15], tool use [ 26], and mathematical reasoning [ 12,18]. In contrast, tasks in interactive and agentic environments usually involve multi-turn interactions and do not reveal explicit error signals until reaching the terminal state. Additionally, unlike mathematical reasoning [ 49], designing high-quality reward functions to critique intermediate actions in long interactive trajectories remains difficult. Wang et al. [43]improves agents\u2019 self-reflection in embodied tasks, but it heavily relies on expert models for correction during training data construction. A critical bottleneck in enhancing error recovery in interactive and agentic environments is the lack of step-level reflection data. Traditional approaches to collecting these datasets involve labor-intensive annotation processes, which are both time-consuming and costly [ 20,57,63]. Without robust reflection data, models face challenges in identifying and correcting their own errors, limiting their utility as intelligent agents. Constructing reflection datasets is thus essential for building agents capable of self-reflection and better decision-making. However, how to automatically construct such training samples is non-trivial. A significant challenge of agent task reflection lies in the necessity for timely revision rather than waiting until the end of a rollout to revise errors. If corrections are applied only at the end of the trajectory, the delayed revisions prevent agents from learning to detect and address errors as they occur, undermining their capacity for real-time self-reflection. Furthermore, delayed revisions may leave catastrophic errors unaddressed, particularly those occurring early in the trajectory. To address these challenges, we propose Agent-R, a novel framework designed to enable LLM-based agentstoperformon-the-flyreflectionandself-improvement. Unlikepreviousreward-basedapproaches, which directly penalize or reward actions based solely on outcome correctness [ 32,37,40,51], Agent-R introduces a dynamic self-training framework that revises errors at the step level. By leveraging Monte Carlo Tree Search (MCTS) [ 16], Agent-R dynamically constructs training samples that recover correct trajectories from erroneous ones, effectively guiding the agent to navigate complex decision spaces. Specifically, Agent-R identifies the most suitable revision step (based on the current actor model) in an incorrect trajectory and connects it with the",
    "conclusion": "sleeve, classic fit, and color heather grey, as specified in the task description. Therefore, Judgment: Bad Judgment (Irrelevant Actions): The action is bad becauseitisnotrelevant tothetaskof finding men\u2019s shirts with specific characteristics. Therefore, Judgment: Bad Sciworld Judgment (Invalid Actions): There is no orange mentioned in the environment, so thisactionisinvalid. It\u2019s not a helpful action, but rather an invalid input. Therefore, Judgment: Bad Judgment (Mismatch Observation): The current task is to determine if the aluminum foil is electrically conductive. The agent has already connected the aluminum foil to the battery and the green light bulb, and observed that the light bulb turned on. This suggests that the aluminum foil is electrically conductive. Moving the aluminum foil to the yellow box would mean that the agent is giving up on the possibility of the aluminum foil being conductive, whichisnotinlinewiththecurrentobservation . Therefore, Judgment: Bad Judgment (Irrelevant Actions): The agent is not using the art studio\u2019s resources effectively, as thecup containing redpaintisnotrelatedtothetask of creating green paint. Therefore, Judgment: Bad TextCraft Judgment (Invalid Actions): The agent is stuck in an infinite loop of trying to execute an invalidaction, which is not helpful in solving the task. Therefore, Judgment: Bad Judgment (Mismatch Observation): The action is not good because it is not possible to craft a dried kelp block with 9 dried kelp. Theobservation suggests thattheagentlacksthenecessary itemstocraftthe block. This action is actually harmful to the task of crafting the block. Therefore, Judgment: Bad Judgment (Irrelevant Actions): The agent is trying to craft diamond boots, which requires 4 diamonds. The agent has already obtained 4 diamonds, which is sufficient to craft the boots. However, the agent is trying to craft a diamond block instead, which requires 9 diamonds. Thisactionisnothelpfulin achieving thegoalofcrafting diamond boots, as it will waste the 4 diamonds the agent already has. Therefore, Judgment: Bad 26"
  },
  {
    "id": "2601.11974v1",
    "title": "Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement",
    "authors": "Xinmeng Hou, Peiliang Gong, Bohao Qu, Wuqi Wang, Qing Guo, Yang Liu",
    "published": "2026-01-17",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2601.11974v1",
    "pdf_url": "https://arxiv.org/pdf/2601.11974v1",
    "abstract": "While Large Language Models (LLMs) en-\nable complex autonomous behavior, current\nagents remain constrained by static, human-\ndesigned prompts that limit adaptability. Ex-\nisting self-improving frameworks attempt to\nbridge this gap but typically rely on ineffi-\ncient, multi-turn recursive loops that incur\nhigh computational costs. To address this,\nwe propose Metacognitive Agent Reflective\nSelf-improvement (MARS), a framework that\nachieves efficient self-evolution within a sin-\ngle recurrence cycle. Inspired by educational\npsychology, MARS mimics human learning by\nintegrating principle-based reflection (abstract-\ning normative rules to avoid errors) and pro-\ncedural reflection (deriving step-by-step strate-\ngies for success). By synthesizing these in-\nsights into optimized instructions, MARS al-\nlows agents to systematically refine their rea-\nsoning logic without continuous online feed-\nback. Extensive experiments on six bench-\nmarks demonstrate that MARS outperforms\nstate-of-the-art self-evolving systems while sig-\nnificantly reducing computational overhead.\nCode are available at https://anonymous.\n4open.science/r/MARS-9F16\n1",
    "introduction": "Principles (Concise Enhancement) Injecting Procedure (Reasoning Enhancement)Human Metacognitive Re\ufb02ection Metacognitive Agent with Relective Self-improvementInspires ConstraintsGuides Step-wise ReasoningFigure 1: The Cognitive Inspiration behind MARS. This framework parallels human reflection with the MARS (Metacognitive Agent with Reflective Self- improvement) agent, converting baseline agent failures into principled-based and procedural instructions to syn- thesize enhanced prompts. G\u00f6del machines (Schmidhuber, 2007), which for- malized self-referential systems that rewrite their own code. Although formal proof requirements make the original framework impractical (Steune- brink and Schmidhuber, 2011), it has inspired mod- ern self-improving systems that rely on empirical validation instead. However, current self-improvement frameworks for LLM agents tend to be constrained by multi- turn recursiveness, which results in inefficient learn- ing and adaptation, as well as excessive computa- tional resource usage. Humans, by contrast, are able to resolve previous errors and adapt to new solutions more efficiently through structured learn- ing approaches. Research in education science has identified two complementary paradigms for guid- ing learners (Hiebert and Lefevre, 1986; Ander- son, 1983). The first isprinciple-based learning, which focuses on helping learners avoid mistakes by establishing conceptual categories of what is correct versus incorrect, and understanding the un- derlying rules that govern a domain (Hiebert and Lefevre, 1986; Rittle-Johnson et al., 2001). The second isprocedural learning, which emphasizesarXiv:2601.11974v1 [cs.AI] 17 Jan 2026 using prior experience and step-by-step reasoning to increase the likelihood of successful outcomes (Anderson, 1983; Kolb, 1984). Rather than learn- ing in isolation, humans benefit most when they integrate both approaches through systematic re- flection and summarization of their experiences. Studies in metacognition have shown that struc- tured reflection\u2014where learners explicitly analyze what worked, what failed, and why\u2014significantly improves learning efficiency and knowledge trans- fer (Flavell, 1979; Kaplan et al., 2013; Stanton et al., 2021). Furthermore, research on productive failure demonstrates that learning from one\u2019s own errors, when properly guided, leads to deeper con- ceptual understanding than direct instruction alone (Kapur, 2014, 2010). In this work, we proposeMARS(Metacognitive Agent withReflectiveSelf-improvement), a frame- work that enables multi-agent systems to achieve efficient self-improvement within a single recur- rence cycle by integrating both principle-based and procedural learning approaches. Inspired by human metacognitive learning, MARS allows agents to systematically reflect on their experiences, extract- ing general principles that help avoid past mistakes while simultaneously deriving procedural knowl- edge that replicates successful strategies. Unlike existing self-evolving agent frameworks that rely on multi-turn recursive improvement, which often leads to inefficient learning and excessive computa- tional costs, MARS consolidates the learning pro- cess through structured summarization, enabling agents to maximize adaptation efficiency in each improvement cycle. Our main contributions are as follows: \u2022We propose MARS, a self-improvement framework for multi-agent systems that inte- grates principle-based and procedural learning inspired by human meta-cognitive theory. \u2022We introduce a triple-pathway reflection mechanism that extracts: (1) normative princi- ples for error avoidance, (2) procedural strate- gies for success replication, and (3) a unified synthesis of both pathways. \u2022We design a structured summarization mod- ule that consolidates learning within a single cycle, reducing computational overhead from multi-turn recursive",
    "conclusion": "+= \"### Critical Warnings by Question Type:\\n\\n\" 15forenhinsorted_enh[:8]: # Top-weighted groups 16text += f\"**{enh.question_type} ({\u2019/\u2019.join(enh. topics)})** \" 17text += f\"({enh.num_questions} failures):\\n\" 18text += \"[!] \" + \" | \".join(enh.key_warnings[:3]) + \"\\n\" 19text += f\" -> {enh.enhanced_prompt_addition}\\n\\n\" 20all_prompts[\u2019concise\u2019] = base_prompt + text # P\u2019 = P + E 21 22# Reasoning enhancement E^(r): process-oriented hints 23if\u2019reasoning\u2019inself.enhancement_types: 24text = f\"\\n## GUIDANCE FOR {category.upper()}\\n\" 25text += \"### Key Considerations by Problem Type:\\n\\n\" 26forenhinsorted_enh[:6]: 27text += f\"* {enh.question_type} ({\u2019/\u2019.join(enh. topics)}): \" 28text += f\"{enh.enhanced_prompt_addition}\\n\" 29all_prompts[\u2019reasoning\u2019] = base_prompt + text 30 31# Specific enhancement E^(c+r): combines concise + reasoning 32# Includes: mistakes (concise) + verification (concise) + approach (reasoning) 33if\u2019specific\u2019inself.enhancement_types: 34text = f\"\\n## GUIDANCE FOR {category.upper()}\\n\" 35forenhinsorted_enh[:10]: 36text += f\"**{enh.question_type} - {\u2019 & \u2019.join(enh. topics)}**\\n\" 37# From concise: explicit warnings as mistake patterns 38text += \"Common Mistakes:\\n\" 39forminenh.common_mistakes[:3]: text += f\" x {m }\\n\" 40# From concise: action sequences as verification 41text += \"Verification Steps:\\n\" 42forsinenh.verification_steps[:4]: text += f\" + {s}\\n\" 43# From reasoning: process-oriented approach 44text += f\"Approach: {enh.type_specific_approach}\\n \\n\" 45all_prompts[\u2019specific\u2019] = base_prompt + text 46 47returnall_prompts Listing 4: Code for enhancement generation and prompt aggregation. G.5 Hybrid Selection Listing 5 implements the hybrid strategy (Equa- tion 6) that selects optimal enhancement per cate- gory. 1defselect_hybrid_enhancement(self, val_data: Dict[str, List], 2enhancements: Dict) -> Dict[str, str]: 3\"\"\"Select E*_c = argmax Acc(E, V_c) for each category c 4where E in {E^(c), E^(r), E^(c+r)} (concise, reasoning, specific)\"\"\" 5 6optimal = {} 7# specific = concise + reasoning (c+r) 8etypes = [\u2019concise\u2019, \u2019reasoning\u2019, \u2019specific\u2019] 9 10forcategory, val_questionsinval_data.items(): 11best_acc, best_type = 0, \u2019concise\u2019 12 13foretypeinetypes: 14enhanced_prompt = enhancements.get(f\"{category}_{ etype}\") 15if notenhanced_prompt:continue 16 17correct =sum(1forqinval_questions 18ifself.evaluate(enhanced_prompt, q) 19== q[\u2019correct_answer\u2019]) 20accuracy = correct /len(val_questions) 21 22ifaccuracy > best_acc: 23best_acc, best_type = accuracy, etype 24 25optimal[category] = best_type 26print(f\" {category}: \u2019{best_type}\u2019 (acc: {best_acc :.1%})\") 27 28returnoptimal Listing 5: Code for hybrid enhancement selection."
  },
  {
    "id": "2511.19436v1",
    "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection",
    "authors": "Qiang Wang, Xinyuan Gao, SongLin Dong, Jizhou Han, Jiangyang Li, Yuhang He, Yihong Gong",
    "published": "2025-11-24",
    "category": "cs.CV",
    "arxiv_url": "http://arxiv.org/abs/2511.19436v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19436v1",
    "abstract": "We presentVDC-Agent, a self-evolving framework for\nVideo Detailed Captioning that requires neither human an-\nnotations nor larger teacher models. The agent forms a\nclosed loop of caption generation, principle-guided scor-\ning (score and textual suggestions), and prompt refinement.\nWhen caption quality regresses, a self-reflection path lever-\nages the previous chain-of-thought to amend the update.\nRunning this process on unlabeled videos produces tra-\njectories of(caption,score)pairs. We convert the trajec-\ntories into preference tuples and filter out samples with\nJSON parsing errors, resulting in VDC-Agent-19K, which\ncontains 18,886 automatically constructed pairs. We then\nfine-tune the base MLLM on this dataset using an easy-\nto-hard curriculum direct preference optimization. Built\non Qwen2.5-VL-7B-Instruct, ourVDC-Agent-7Battains\nstate-of-the-art performance on the VDC benchmark with\n49.08%average accuracy and2.50score, surpassing spe-\ncialized video captioners and improving over the base\nmodel by+5.13%accuracy and+0.27score at similar in-\nference cost.\n1.",
    "introduction": "w/o human feedback w/o external modelsHigh-Quality CaptionVideo High-Quality CaptionHuman Annotations Train Caption Scorer Large-Scale Caption Generation + FilteringVideo High-Quality CaptionMLLM 1 More Powerful MLLM Scoring / Fusion FilteringMLLM 2 MLLM 3 (a) Human-Aligned Pipeline (b) Stronger Teacher Guidance (c) Agentic Self-Reflection (Proposed) PromptSuggestion ReflectionInitial PromptFigure 1.Comparison of video captioning paradigms.(a) Human-aligned pipelines rely on manual annotations to train cap- tion scorers. (b) Multi-MLLM-based pipelines depend on multiple or stronger MLLMs for scoring or fusion. (c) Our proposedVDC- Agentachieves self-improvement through agentic self-reflection, requiring neither human annotations nor larger models. caption datasets [29, 33, 38]. Although these methods have shown strong performance, they typicallyrely on ei- ther distilling caption generation capabilities from more powerful MLLMs (proprietary models like GPT-4V [1] or open-source alternatives like Qwen-72B [2]) or incor- porating extensive manual annotations for human prefer- ence alignment, as shown in Fig. 1 (a)(b). For instance, ShareGPT4Video [4] constructs datasets using GPT-4V , fol- lowed by manual verification and filtering to enhance cap- tion quality. Cockatiel [33] and OwlCap [49] rely on exten- sive human annotations to train caption scorers, or use more powerful captioning models to fuse captions from differ- 1arXiv:2511.19436v1 [cs.CV] 24 Nov 2025 ent MLLMs. A VC-DPO [38] and VideoCap-R1 [29] build upon datasets constructed with powerful captioning models and employ reinforcement learning to further improve per- formance. However, these approaches face several inherent limi- tations: prohibitive human annotation costs, access barri- ers to proprietary APIs, and substantial computational re- sources required for large-scale model inference. Conse- quently, enabling models to achieve autonomous reflection and iterative improvement in caption generation, without dependence on stronger MLLMs or extensive human anno- tations, has become critical for advancing beyond the cur- rent paradigm. To address this challenge,we propose to treat the captioner itself as an autonomous agent that can generate, evaluate, and refine its own captions through it- erative self-reflection. To this end, we proposeVDC-Agent, a self-evolving video captioning framework that enables MLLMs to im- prove themselves through iterative self-reflection without requiring stronger external supervision. As depicted in Fig. 1 (c), VDC-Agent forms a closed-loop system that con- tinuously refines its captioning ability by alternating be- tween caption generation, evaluation, and prompt refine- ment. Given a collection of unlabeled videos, the model first generates captions using an initial prompt. It then con- ducts self-assessment based on a set of principles describ- ing what constitutes a good caption (such as coverage of objects, actions, and temporal dynamics), assigns a quality score to the caption, and produces textual suggestions for improvement. These suggestions guide an internalprompt refiner, which updates the prompt in the next iteration. If the newly generated caption is even worse than the previ- ous one, the model triggers aself-reflectionmechanism that revisits the chain of thought used in the last prompt refine- ment, diagnosing why the previous update failed and avoid- ing the same mistake in subsequent steps. Through repeated cycles of caption\u2013evaluation\u2013refinement, our VDC-Agent can generate higher-quality video descriptions. To internalize VDC-Agent\u2019s self-reflection capability into the MLLM, enabling it to achieve",
    "conclusion": "2024. 2, 6 [44] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action.arXiv preprint arXiv:2303.11381, 2023. 3 [45] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. InThe eleventh international conference on learning representations, 2022. 3 [46] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appa- gent: Multimodal agents as smartphone users. InProceed- ings of the 2025 CHI Conference on Human Factors in Com- puting Systems, pages 1\u201320, 2025. 3 [47] Quan Zhang, Jinwei Fang, Rui Yuan, Xi Tang, Yuxin Qi, Ke Zhang, and Chun Yuan. Weakly supervised temporal action localization via dual-prior collaborative learning guided by multimodal large language models. InProceedings of the Computer Vision and Pattern Recognition Conference, pages 24139\u201324148, 2025. 1 [48] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Zi- wei Liu, and Chunyuan Li. Video instruction tuning with synthetic data.arXiv preprint arXiv:2410.02713, 2024. 2 [49] Chunlin Zhong, Qiuxia Hou, Zhangjun Zhou, Shuang Hao, Haonan Lu, Yanhao Zhang, He Tang, and Xiang Bai. Owlcap: Harmonizing motion-detail for video captioning 10 via hmd-270k and caption set equivalence reward.arXiv preprint arXiv:2508.18634, 2025. 1, 6 [50] Yaoyao Zhong, Wei Ji, Junbin Xiao, Yicong Li, Weihong Deng, and Tat-Seng Chua. Video question answering: Datasets, algorithms and challenges. InProceedings of the 2022 conference on empirical methods in natural language processing, pages 6439\u20136455, 2022. 1 [51] Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, and Cordelia Schmid. Streaming dense video captioning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 18243\u201318252, 2024. 1 11"
  },
  {
    "id": "2410.10934v2",
    "title": "Agent-as-a-Judge: Evaluate Agents with Agents",
    "authors": "Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, J\u00fcrgen Schmidhuber",
    "published": "2024-10-14",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2410.10934v2",
    "pdf_url": "https://arxiv.org/pdf/2410.10934v2",
    "abstract": "Agent-as-a-Judge: Evaluate Agents with Agents Mingchen Zhuge1,2,Changsheng Zhao1,Dylan R. Ashley2,Wenyi Wang2,Dmitrii Khizbullin2, Yunyang Xiong1,Zechun Liu1,Ernie Chang1,Raghuraman Krishnamoorthi1,Yuandong Tian1, Yangyang Shi1,Vikas Chandra1,J\u00a8 urgen Schmidhuber2 1Meta AI,2KAUST Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes\u2014ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI , a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems\u2014by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement. Date: October 18, 2024 Correspondence: mingchen.zhuge@kaust.edu.sa ,cszhao@meta.com Dataset: https://huggingface.co/devai-benchmark Project: https://github.com/metauto-ai/agent-as-a-judge Note: First four authors made core contributions. KAUST crafted the dataset. Work done while Mingchen was interning at Meta, with Changsheng leading. 1 Introduction Recent years have seen multimodal agentic systems move from occasionally being able to solve small toy problems to being regularly deployed for challenging real-world problems (the dream of most AI research). Yet, the current evaluation methods and the available benchmarks for agentic systems are struggling to keep up with these rapid advances, dramatically slowing true progress. We believe that the current issue with evaluating agentic systems stems from the lack of feedback during the",
    "introduction": "Recent years have seen multimodal agentic systems move from occasionally being able to solve small toy\nproblems to being regularly deployed for challenging real-world problems (the dream of most AI research).\nYet, the current evaluation methods and the available benchmarks for agentic systems are struggling to keep\nup with these rapid advances, dramatically slowing true progress.\nWe believe that the current issue with evaluating agentic systems stems from the lack of feedback during the\nintermediate task-solving stages for these nontraditional systems. Agentic systems think more like humans,\noften act step-by-step (Wooldridge, 1999) and often host very human-like symbolic communications internally\nto solve problems (Zhuge et al., 2023). And thus agentic systems should be evaluated like a human, with\nrich evaluative feedback which looks at the full thought and action trajectory; evaluating an agentic system\nin the traditional way is like evaluating a student using multiple-choice testing\u2014a comparatively unreliable\nestimator (Park, 2010). For example, while SWE-Bench (Yang et al., 2024a) is widespread, its evaluation",
    "conclusion": "In this work, we introduced the Agent-as-a-Judge method to use agentic systems to evaluate\nagentic systems. We simultaneously released DevAI: a new benchmark that evaluates the code-generating\nability of agentic systems on complete AI development tasks when used with Agent-as-a-Judge. We went on\nto show that Agent-as-a-Judge outperforms existing methods on this task and that it performs similarly to an\nensemble of expert human evaluators. Altogether, we believe that the above opens the door for scaling up\nagentic far more than before."
  },
  {
    "id": "2510.06711v1",
    "title": "Inefficiencies of Meta Agents for Agent Design",
    "authors": "Batu El, Mert Yuksekgonul, James Zou",
    "published": "2025-10-08",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2510.06711v1",
    "pdf_url": "https://arxiv.org/pdf/2510.06711v1",
    "abstract": "Recent works began to automate the design of\nagentic systems using meta-agents that propose\nand iteratively refine new agent architectures.\nIn this paper, we examine three key challenges\nin a common class of meta-agents.First, we\ninvestigate how a meta-agent learns across iter-\nations and find that simply expanding the con-\ntext with all previous agents, as proposed by\nprevious works, performs worse than ignoring\nprior designs entirely. We show that the perfor-\nmance improves with an evolutionary approach.\nSecond, although the meta-agent designs multi-\nple agents during training, it typically commits\nto a single agent at test time. We find that the\ndesigned agents have low behavioral diversity,\nlimiting the potential for their complementary\nuse.Third, we assess when automated design is\neconomically viable. We find that only in a few\ncases\u2014specifically, two datasets\u2014the overall\ncost of designing and deploying the agents is\nlower than that of human-designed agents when\ndeployed on over 15,000 examples. In contrast,\nthe performance gains for other datasets do not\njustify the design cost, regardless of scale.\n1",
    "introduction": "end, recent works have taken the firststeps in the direction of automating the design of agentic systems (Hu et al., 2024; Li et al., 2024; Saad-Falcon et al., 2024; Niu et al., 2025; Nie et al., 2025; Shang et al., 2025; Wang et al., 2025; Ye et al., 2025; Zhang et al., 2025b,a). Our work fo- cuses on a common class of meta-agents that follow thesample\u2013evaluate\u2013iteratepattern (see Figure 1, Algorithm 1) and highlights three challenges. Meta LearningWe begin by examining the as- sumption that the meta-agent effectively learns from previously discovered agents. Our analysis reveals that the meta-agent framework proposed by Hu et al. (2024) does not meaningfully leverage prior designs. In fact, it performs worse than a base- line that ignores prior designs entirely. In contrast, we demonstrate that an evolutionary context cura- tion strategy, where the generation of the next agent is conditioned on the previous best-performing agents (parents), yields improved performance. Diversity and ComplementarityWhile the meta-agent generates a set of candidate agents, typ- ically only one is deployed, neglecting potential synergies among them. If the designed agents were behaviorally diverse, where each specializes in par- ticular types of queries, this would enable dynamic selection of the most suitable agent per query. How- ever, we find that the designed agents often lack be- havioral diversity, which is even more pronounced when evolutionary strategies are used. Economic ViabilityFor a meta-agent to be eco- nomically viable, the fixed cost of designing a new agent must be justified by corresponding improve- ments in performance. We formalize this trade-off by defining the total cost of a meta-designed agent as the sum of a fixed design cost and a per-example inference cost. This raises the key question:How many test examples are needed before the cost per correct response becomes lower when using the designed agent?In our experiments, we find thisarXiv:2510.06711v1 [cs.AI] 8 Oct 2025 Figure 1:Overview of the meta-agent framework.The Meta-Agent iteratively samples and evaluates agents, refining its outputs through a feedback loop. We focus on three key dimensions: (1) learning from previously designed agents; (2) diversity and complementarity of generated agents; and (3) economic viability. break-even point occurs at approximately 15,000 examples for MMLU and DROP. In contrast, for other datasets, the performance gains do not justify the design cost, regardless of the scale of deploy- ment. 2 Related Works Our primary reference is ADAS (Hu et al., 2024), which has introduced meta-agent search with the idea of searching for agents in the code space. MAS-GPT (Ye et al., 2025) and ScoreFlow (Wang et al., 2025) develop meta-agents by training a model to dynamically generate multi-agent systems for a given query. AgentSquare (Shang et al., 2025) and Archon (Saad-Falcon et al., 2024) explore mod- ular agent architectures and use discrete module recombination to efficiently search design spaces. AutoFlow (Li et al., 2024), Weak-for-Strong (Nie et al., 2025), and ADAS (Hu et al., 2024) use a meta agent that follows the sample-evaluate-iterate paradigm (Algorithm 1). Other recent meta-agent approaches include Multi-agent Supernet (Zhang",
    "conclusion": "includes all 90 agents designed across 3 runs. Figure 5:Average inference cost per test query of the best agents.For best agent in the initial library F (Initial, see Appendix A.1), best agent designed by meta agent with \u03d5C(Cumulative), best agents designed by meta agent with \u03d5P(Parallel) , best agent designed by meta agent with \u03d5E(Evolutionary). Averaged across the single best agents from 3 runs. Best agent is selected based on the highest training performance. Figure 6: Score matrix S, where each row corresponds to an agent and each column to a dataset example. A cell is white if the agent answers correctly and black otherwise. For DROP, gray indicates intermediate F1 scores; for GPQA, gray denotes partial correctness across repeated attempts. The normalized rows, si, serve as agent embeddings, capturing performance across training questions. Figure 7: Cosine similarity matrixC, with agents reordered by descending average similarity to all other agents. Figure 8: Histograms of agent similarities (entries of C), excluding agents with zero performance (all black rows of Sin Figure 6, and corresponding dark blue rows and columns of Cin Figure 7). Only the upper triangular entries ofC(excluding the diagonal) are used, as Cis symmetric. Each subplot shows histograms of similarity scores (x-axis) and their frequency (y-axis). Figure 9: Figure 2 with (1 - Hamming distance) as the similarity metric. All nonzero entries ofSare set to1. Figure 10: Figure 8 with (1 - Hamming distance) as the similarity metric. All nonzero entries ofSare set to1. Figure 11: Training performance of designed agents across iterations. The dotted red line shows the performance of the best agent from the initial library. Figure 12: Design cost of the next agent across iterations. While costs remain stable with Parallel and Evolutionary context curation, they increase linearly with increasing context length in Cumulative context curation."
  },
  {
    "id": "2410.16128v1",
    "title": "SMART: Self-learning Meta-strategy Agent for Reasoning Tasks",
    "authors": "Rongxing Liu, Kumar Shridhar, Manish Prajapat, Patrick Xia, Mrinmaya Sachan",
    "published": "2024-10-21",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2410.16128v1",
    "pdf_url": "https://arxiv.org/pdf/2410.16128v1",
    "abstract": "SMART: Self-learning Meta-strategy Agent for Reasoning Tasks Rongxing Liu*1, Kumar Shridhar*1, Manish Prajapat1, 2, Patrick Xia3and Mrinmaya Sachan1 1ETH Zurich,2ETH AI Center,3Microsoft Tasks requiring deductive reasoning, especially those involving multiple steps, often demand adaptive strategies such as intermediate generation of rationales or programs, as no single approach is universally optimal. While Language Models (LMs) can enhance their outputs through iterative self-refinement and strategy adjustments, they frequently fail to apply the most effective strategy in their first attempt. This inefficiency raises the question: Can LMs learn to select the optimal strategy in the first attempt, without a need for refinement? To address this challenge, we introduce SMART(Self-learning Meta-strategy Agent forReasoning Tasks), a novel framework that enables LMs to autonomously learn and select the most effective strategies for various reasoning tasks. We model the strategy selection process as a Markov Decision Process and leverage reinforcement learning-driven continuous self-improvement to allow the model to find the suitable strategy to solve a given task. Unlike traditional self- refinement methods that rely on multiple inference passes or external feedback, SMARTallows an LM to internalize the outcomes of its own reasoning processes and adjust its strategy accordingly, aiming for correct solutions on the first attempt. Our experiments across various reasoning datasets and with different model architectures demonstrate that SMARTsignificantly enhances the ability of models to choose optimal strategies without external guidance (+15 points on the GSM8K dataset). By achieving higher accuracy with a single inference pass, SMART not only improves performance but also reduces computational costs for refinement-based strategies, paving the way for more efficient and intelligent reasoning in LMs. https://github.com/kumar-shridhar/SMART/ 1. Introduction When people first encounter complex reasoning tasks, such as solving mathematical problems, they often make mistakes or approach them inefficiently [ 3]. However, with experience, humans tend to improve their performance by replacing",
    "introduction": "ineffective or incorrect strategies with more effective ones, using a mix of strategies tailored to the specific task [1, 2, 15, 30, inter alia ]. Language Models (LMs) similarly struggle with reasoning tasks, sometimes producing incoherent results [11,16,25]. Acommonremedyistoresampletheoutput, aprocessknownas refinement . Thisrefinement may involve reusing the same reasoning approach [ 16] or adopting an entirely new one [ 25]. In addition, providing feedback on initial results has proven beneficial during resampling [ 11,13,24,34,inter alia ]. This raises a critical question: Can LMs be taught to optimize their choice of reasoning strategy for specific tasks overtime on the first trial, much like humans do? To address this question, we propose a novel framework called SMART(Self-learning Meta-strategy Agent forReasoning Tasks), which allows LMs to learn optimal strategy selection through a continuous self-learning approach. We model the task of identifying the optimal strategy as a Markov Decision Process (MDP) [ 21,28], where the agent (LM) starts with its pre-trained knowledge and iteratively improves its performance by learning from its own outputs and strategy choices. By integrating the LM\u2019s reasoning abilities with reinforcement learning-driven self-improvement, the agent can simulate different reasoning (*)equal contribution. Correspondence at: { rongxingtianxia611@163.com, shkumar@ethz.ch }arXiv:2410.16128v1 [cs.AI] 21 Oct 2024 SMART:Self-learning Meta-strategy Agent forReasoning Tasks LMt-1Question : John has 10 apples, Jack has 20 apples, .....L2M: How many apples are .. LMt-1Question : John has 10 apples, Jack has 20 apples, .....<incorrect strategy > L2MLMt-1Question : John has 10 apples, Jack has 20 apples, .....PoT: def num_apples(): CoT: There are in total 10+ ..LMtLabelTraining Label RefinementPoT: def .. CoT: There ..Initial Sampling Iterative Refinement1 2 Figure1:Our proposed methodology : In the first step (initial sampling), an agent (LM) chooses a strategy and solves the given task with it. If it is correct, the process ends successfully. If an incorrect strategy is chosen, the agent iteratively refines its strategy, taking previous strategies into account. The process stops when a correct strategy is chosen to solve a task, or when a stopping criterion such as the number of attempts is reached. All correct strategies are used to further refine the model, and the process is repeated. During testing, we sample once from LM \ud835\udc61without refinement. strategies, evaluate their effectiveness based on past outcomes, and adjust its strategy choice accordingly. Our approach differs from traditional methods by focusing on iterative reward-based learning, which encourages the agent to produce the correct inference on the first attempt without resampling. This not only improves cost efficiency - only one sampling step is required during inference - but also results in a more generalizable model capable of adapting its strategy selection based on the specific task. We validate SMARTon a variety of reasoning datasets and LM architectures and show that our method significantly improves the ability of LMs to select optimal strategies on the first try, outperforming baseline models that rely on traditional self-refinement techniques in both accuracy and computational efficiency. On three mathematical datasets (GSM8K [ 5], SVAMP [ 19], ASDiv [ 17]) over three LLM",
    "conclusion": "also has to pay 10% tax on everything. How much did he pay in total? Response: Sub-question 1: How much did John spend on shirts? Answer to Sub-question 1: 3*$20=$ <<3*20=60 >>60 Sub-question 2: How much did John spend on tax? Answer to Sub-question 2: $60*.1=$\u00ab60*.1=6\u00bb6 Sub-question 3: How much did John spend in total? Answer to Sub-question 3: $60+$6=$ <<60+6=66 >>66 Final Answer: 66 <eos> [7 more examples randomly sampled from the training set] Input: Thomas is training at the gym to prepare for a competition. He trained for 5 hours every day for a month (30 days). If he continues to train for the next 12 days, how many hours will he spend on training in total? Response: Model generated response .. Figure6: 8-shot Least to Most demonstration. 17 SMART:Self-learning Meta-strategy Agent forReasoning Tasks Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Instruction: Solve the given math problem by writing a python program. Store your result as a variable named \u2019answer\u2019. Input: John buys 3 dress shirts. They sell for $20 each. He also has to pay 10% tax on everything. How much did he pay in total? Response: total_shirts = 3 cost_of_one_shirt = 20 total_cost_shirts = total_shirts * cost_of_one_shirt tax_rate = 0.1 tax_amount = tax_rate * total_cost_shirts total_cost = total_cost_shirts + tax_amount answer = total_cost <eos> [7 more examples randomly sampled from the training set] Input: Thomas is training at the gym to prepare for a competition. He trained for 5 hours every day for a month (30 days). If he continues to train for the next 12 days, how many hours will he spend on training in total? Response: Model generated response .. Figure7: 8-shot Program of Thought demonstration. 18"
  },
  {
    "id": "2508.02959v2",
    "title": "Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow",
    "authors": "Chia-Tung Ho, Jing Gong, Xufeng Yao, Yunsheng Bai, Abhishek B Akkur, Haoxing Ren",
    "published": "2025-08-04",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2508.02959v2",
    "pdf_url": "https://arxiv.org/pdf/2508.02959v2",
    "abstract": "Large language models (LLMs) excel at solving complex\ntasks by executing agentic workflows composed of detailed\ninstructions and structured operations. Yet, building general-\npurpose agents by manually embedding foundation mod-\nels into agentic systems such as Chain-of-Thought, Self-\nReflection, and ReACT through text interfaces limits scala-\nbility and efficiency. Recently, many researchers have sought\nto automate the generation and optimization of these work-\nflows through code-based representations. However, existing\nmethods often rely on labeled datasets to train and optimize\nworkflows, making them ineffective and inflexible for solv-\ning real-world, dynamic problems where labeled data is un-\navailable. To address this challenge, we introduce Polymath,\na self-optimizing agent with dynamic hierarchical workflow\nthat leverages the flexibility of task flow graphs and the ex-\npressiveness of code-represented workflows to solve a wide\nrange of real-world, dynamic problems. The proposed op-\ntimization methodology integrates multi-grid-inspired graph\noptimization with a self-reflection-guided evolutionary algo-\nrithm to refine workflows without labeled data. Experimental\nresults on six benchmark datasets across coding, math, and\nmulti-turn QA tasks show that Polymath achieves 8.1% av-\nerage improvement over state-of-the-art baselines. We will\nmake the source code publicly available upon acceptance.",
    "introduction": "et al. 2022; Hu et al. 2023), ReACT and tool use (Yao et al. 2022; Schick et al. 2023), and self-reflection (Shinn et al. 2023; Madaan et al. 2023). While these agentic workflows en- able LLMs to solve challenging problems, they are typically hand-engineered, task-specific, and labor-intensive to design and maintain. As the demand for LLM-driven applications expands, this reliance on manual workflow construction be- comes a bottleneck. It limits the scalability of LLM systems, slowing adaptation to new domains, and hindering the trans-fer of skills across tasks (Tang et al. 2023). Therefore, au- tomating agentic workflows for solving versatile and diverse tasks has emerged as a critical need. Many recent works focus on automating agentic work- flow discovery to reduce human involvement (Khattab et al. 2024; Yuksekgonul et al. 2024; Liu et al. 2023; Hu et al. 2024), yet full automation remains unsolved. DSPy (Khat- tab et al. 2024) requires manual setup, while methods like TextGrad (Yuksekgonul et al. 2024) and GPTSwarm (Zhuge et al. 2024) struggle to capture the diversity of workflows needed for broad task generalization (Yu, He, and Ying 2023; Yang et al. 2024b; Sun et al. 2023), since their op- timization objectives cannot represent the breadth of po- tential workflows. Although ADAS (Hu et al. 2024) and AFlow (Zhang et al. 2024a) improve expressiveness by rep- resenting workflows as code and refining them via execution feedback, they rely heavily on existing validation data and aim to generalize across task categories, limiting their adapt- ability to dynamic, real-world problems and task-specific challenges. On the other hand, Data Interpreter (Hong et al. 2024) proposed a task graph on top of a programmable node flow, but the approach lacks efficient self-learning and opti- mization. This highlights the critical need for more effective and adaptive techniques to fully automate the workflow gen- eration for dynamic, real-world problems to accelerate the application of LLMs across domains. In this work, we propose Polymath, a self-optimizing agent featuring a dynamic hierarchical workflow that lever- ages flexible task flow graphs combined with expressive, code-based workflows to tackle a broad range of real-world, dynamic problems. Moreover, we propose a novel hier- archical workflow optimization methodology, from multi- grid-inspired task flow graph optimization to an on- line self-reflection-guided evolutionary algorithm for code- represented workflow enhancement through LLM-based evaluators without the need for labeled datasets. Our con- tributions are as follows. \u2022 We propose a self-optimizing agent with dynamic hier- archical workflow that leverages the flexibility of task flow graphs and the expressiveness of subtask-level code- represented workflows to solve a wide range of real- world, dynamic problems. The task flow graph employs a divide-and-conquer approach to decompose and exe- cute subtasks based on the topological order, while thearXiv:2508.02959v2 [cs.AI] 7 Aug 2025 Final Answer: def text_match_wordz (text): # Regular expression to match a word containing the letter \u2026def workflow(instruction: str): instruction = \"### Instruction Background ### \\n\" + instruction + \" \\n\" <<<<<<< SEARCH ... # your designed workflow here. ======= import json # Task_3:",
    "conclusion": "### Provide suggestions, feedback, and reflection on the generated result. ### Task ### {current_task }: Calculate the final amount of milk in the bucket after 3 miles. Include the breakdown of milk amounts after each mile in y our response. ### Previous Generated Result ### {initial_response } Reflection: \"\"\" reflection_response = strong_reason_debug_assistant.initiate_chat (message= reflection_prompt ) reflection_output = extract_output (reflection_response , reason_agent_description [\"post_process_func \"]) # Incorporate reflection into final response final_response = f\"{ initial_response }\\n\\nReflection and improvements: \\n{reflection_output }\" # Extract the final numerical answer and ensure it's in the required boxed format import re numerical_answer = re.search (r'\\d+\\.\\d+', final_response ) if numerical_answer : boxed_answer = f\"$boxed {{{numerical_answer.group ()}}}$\" final_response = f\"{ final_response }\\n\\nFinal Answer: { boxed_answer }\" else: final_response = f\"{ final_response }\\n\\nError : No numerical answer found.\" return final_response # EVOLVE -BLOCK -ENDLLM Ensemble (Write code to calculate) Ranking & Get best response Reflection & Provide Final Answer Make sure the Output Format is Correct[Problem]: Sam is carrying a 2 gallon bucket of milk to his house from the barn, which is 3 miles away from his house. However, the bucket has a leak in it. For ea ch mile he walks, there is $ \\\\frac{2}{3}$ as much milk in the bucket as at the beginning of the mile. How many gallons of milk will be in the bucket when Sam gets home ? [Target Current sub task request to solve ]: Calculate the amount of milk remaining after each mile walked . Sam is carrying a 2 -gallon bucket of milk, and for each mile walked, there is 2/3 as much milk in the bucket as at the beginning of the mile .Figure 9: An example of generated code-represented workflows from self-reflection-guided evolutionary algorithm that includes ensembling approach, ranking or voting, reflection on top of coding and reasoning."
  },
  {
    "id": "2504.19565v3",
    "title": "Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training",
    "authors": "Meng Xiao, Xunxin Cai, Qingqing Long, Chengrui Wang, Yuanchun Zhou, Hengshu Zhu",
    "published": "2025-04-28",
    "category": "cs.CL",
    "arxiv_url": "http://arxiv.org/abs/2504.19565v3",
    "pdf_url": "https://arxiv.org/pdf/2504.19565v3",
    "abstract": "Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insuffi-\ncient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training\nin biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored\nexplicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedi-\ncal knowledge. Central to our approach is a collaborative multi-agent architecture, where specialized agents\u2014each guided by\nthe Medical Subject Headings (MeSH) hierarchy\u2014work in concert to autonomously extract, synthesize, and self-evaluate high-\nquality textual data from vast scientific literature. This agentic framework collectively generates and refines domain-specific\nquestion-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual\ninvolvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve no-\ntable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced\nproprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2,\ndespite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each\nagent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.\u2217\nKeywordsBiomedical Large Language Models, Agentic Corpus Distillation, Synthetic Question\u2013Answer Generation, Agentic\nAI, Knowledge Hierarchy Guidance\nCitationKnowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training.\nSci China Inf Sci, for review\n1",
    "introduction": "models has propelled bioinformatics into a new era [1], enabling the de- velopment of automated solutions across a spectrum of biomedical domains [2\u20135], and has demonstrated notable success for real-world biomedical question answering (QA) tasks [6\u20138]. However, the intricate and specialized nature of biomedical tasks means that general-purpose LLMs often fall short unless metic- ulously adapted and fine-tuned for the domain [9\u201311]. Progress in this area is further constrained by the scarcity of sufficiently large and high-quality biomedical corpora [12\u201314]. While existing open-source biomedical datasets are typically of high quality, their limited scale and narrow topical coverage restrict their utility for comprehensive LLM training. Conversely, directly leveraging the expansive body of domain-specific scientific literature offers the potential to cover a broader range of biomedical topics. Nevertheless, most of these resources are unannotated, and their inherent lack of structure, coupled with the absence of QA-format organization, greatly hinders their effective use for training question-answering models. Although the vast body of biomedical literature represents a valuable and authoritative resource, its complex terminology and dense conceptual structures pose significant barriers to automated process- ing and dataset construction [15,16]. Figure 1 visualizes the resulting data bottleneck. These challenges raise a crucial question:How can we automatically distill large-scale, high-quality QA pairs from scientific literature to empower biomedical LLM? * Corresponding author (email: hszhu@cnic.cn) \u2217The code supporting the findings of this study is available at DropBox.arXiv:2504.19565v3 [cs.CL] 18 Dec 2025 Sci China Inf Sci2 Existing Limitation and Challenge Base LLM Finetuned LLM Our Perspective Finetuned LLMLimited in Coverage Better ApplicationTraining CorporaAutomated Dataset DistillationOpen-sour ce CorporaOpen-sour ce Corpora Base LLM Finetuned LLMLimited by KnowledgeRaw Corpora Base LLM limited size large quantityhigh quality& Cold-start \uff011. Size issue of open-source train-sets 2. Quality issue of unlabeled document-sets Figure 1Analyzing the limitations and challenges of the existing pipeline. The motivation of this study is to utilize the high- quality but limited annotated corpus to generate large-scale training corpora from raw scientific documents. Prior research can be categorized into three mainstream categories. First, rule-based methods [17\u201319] rely on human-crafted standards for data cleaning and curation, which, while effective in reducing noise, are resource-intensive and difficult to scale. Second, knowledge graph-based approaches [20,21] structure biomedical information from texts into comprehensive graphs, but their dependence on manually curated sources limits both efficiency and scalability. Third, synthetic data generation methods [22\u201326] use LLMs to automate the creation of QA pairs and process large corpora. Yet, they often lack mechanisms for interdisciplinary collaboration [27, 28], resulting in insufficient diversity and robustness in the distilled data. Recent efforts [29] have introduced a knowledge hierarchy-guided [30] approach that leverages a single LLM agent to generate and evaluate biomedical QA data with improved alignment to domain ontologies. Its reliance on a single-agent, rule-based architecture presents inherent limitations in terms of diversity, cross-domain expertise, and collaborative reasoning [31]. In response to these limitations, we introduce theMulti-agent enhancedKnowledge hierArchy guIded biomedical dataset distiLlatIoN(m-KAILIN) framework\u2014a novel, fully automated, agentic frame- work designed to extract high-quality training corpora for biomedical LLMs. The",
    "conclusion": "In this work, we present a knowledge-driven, multi-agent framework for scientific corpus distillation\ntailored to biomedical large language model training. By leveraging a collaborative architecture\u2014where\nspecialized agents guided by biomedical ontologies autonomously generate, evaluate, and refine question-\nanswer pairs\u2014our approach addresses the limitations of existing open-source scientific corpora in both\nscale and quality. Through extensive experiments, we demonstrate that language models trained on our\nmulti-agent distilled datasets achieve substantial improvements in biomedical question-answering tasks,\noutperforming both strong open-source and proprietary baselines. Our ablation studies further validate\nthe effectiveness and synergy of each agent within the framework. This study highlights the potential of\nagentic, knowledge-guided corpus construction for advancing biomedical AI, and provides scalable tools\nand datasets to the community for future research."
  },
  {
    "id": "2510.05596v1",
    "title": "From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions",
    "authors": "Changyuan Zhao, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Geng Sun, Xianbin Wang, Shiwen Mao, Abbas Jamalipour",
    "published": "2025-10-07",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2510.05596v1",
    "pdf_url": "https://arxiv.org/pdf/2510.05596v1",
    "abstract": "\u2014Self-evolving agentic artificial intelligence (AI) of-\nfers a new paradigm for future wireless systems by enabling\nautonomous agents to continually adapt and improve without\nhuman intervention. Unlike static AI models, self-evolving agents\nembed an autonomous evolution cycle that updates models,\ntools, and workflows in response to environmental dynamics.\nThis paper presents a comprehensive overview of self-evolving\nagentic AI, highlighting its layered architecture, life cycle, and\nkey techniques, including tool intelligence, workflow optimization,\nself-reflection, and evolutionary learning. We further propose\na multi-agent cooperative self-evolving agentic AI framework,\nwhere multiple large language models (LLMs) are assigned role-\nspecialized prompts under the coordination of a supervisor agent.\nThrough structured dialogue, iterative feedback, and systematic\nvalidation, the system autonomously executes the entire life cycle\nwithout human intervention. A case study on antenna evolution in\nlow-altitude wireless networks (LA WNs) demonstrates how the\nframework autonomously upgrades fixed antenna optimization\ninto movable antenna optimization. Experimental results show\nthat the proposed self-evolving agentic AI autonomously improves\nbeam gain and restores degraded performance by up to 52.02%,\nconsistently surpassing the fixed baseline with little to no human\nintervention and validating its adaptability and robustness for\nnext-generation wireless intelligence.\nIndex Terms\u2014Agentification, self-evolving agentic AI, large\nlanguage models, low-altitude wireless networks,\nI.",
    "introduction": "this notion remained largely conceptual. C. Zhao is with the College of Computing and Data Science, Nanyang Technological University, Singapore, and CNRS@CREATE, 1 Create Way, 08-01 Create Tower, Singapore 138602 (e-mail: zhao0441@e.ntu.edu.sg). R. Zhang, J. Wang, and D. Niyato are with the College of Computing and Data Science, Nanyang Technological University, Singapore (e-mail: ruichen.zhang@ntu.edu.sg; jiacheng.wang@ntu.edu.sg; dniyato@ntu.edu.sg). G. Sun is with College of Computer Science and Technology, Jilin Univer- sity, China 130012, (e-mail: sungeng@jlu.edu.cn). X. Wang is with the Department of Electrical and Computer Engineer- ing, Western University, London, ON, N6A 5B9, Canada (e-mail: xian- bin.wang@uwo.ca). S. Mao is with the Department of Electrical and Computer Engineering, Auburn University, Auburn, USA (e-mail: smao@ieee.org). A. Jamalipour is with the School of Electrical and Computer Engineering, University of Sydney, Australia (e-mail: a.jamalipour@ieee.org).Building on this foundational idea, the paradigm of agentifi- cation,through the process of transforming static AI models into autonomous and adaptive agents, has recently gained traction [2]. This development underpins the emergence of self-evolving agentic AI [3], turning the theoretical promise of self-improvement into practical agent architectures. Self- evolving agentic AI represents a new generation of au- tonomous agent systems that can continuously adapt and self- improve through dynamic interaction with their environments, effectively bridging the powerful yet static capabilities of AI models with the continual adaptability required by edge systems [4]. In practice, such agents leverage a synergy of learning techniques: reinforcement learning for trial-and-error optimization, self-supervised learning for extracting structure from raw data, curriculum learning for staged skill growth, and self-reflection for diagnosing weaknesses [3]. Although existing techniques such as continuous learning, life-long learning, incremental learning, and domain adapta- tion offer mechanisms to update static models, they typically rely on human intervention and only target isolated stages of the model life cycle [3]. In contrast, self-evolving agents can autonomously execute the full AI agent life cycle without human intervention, from acquiring and curating new expe- riences to refining and integrating them to updating mod- els and deploying the improved capabilities. This evolution cycle allows the agent to continuously enhance individual components such as the model, memory, prompts, tools, and workflow strategies efficiently and effectively. In doing so, the system dynamically adapts to changing tasks, dynamic contexts, and varying resources while ensuring safety, stability, and sustained performance. Given the advantages of self-evolving agents, this emerg- ing concept is increasingly becoming a practical reality. A representative example is Google DeepMind\u2019s AlphaEvolve, an evolutionary coding agent that embodies the self-evolving paradigm in action1. AlphaEvolve iteratively performs closed- loop cycles of code generation, automated evaluation, and intelligent mutation, guided by large language models (LLMs) and evolutionary selection mechanisms. The system has achieved tangible impact by discovering algorithms surpassing 50-year-old benchmarks, reclaiming 0.7% of Google\u2019s global compute resources, and accelerating AI training. 1https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered- coding-agent-for-designing-advanced-algorithms/arXiv:2510.05596v1 [cs.AI] 7 Oct 2025 2 With the emergence of 6G and next-generation commu- nication technologies, edge and Internet of Things (IoT) devices are anticipated to operate with unprecedented levels of autonomy and intelligence [5]. Agentification provides an important step in this",
    "conclusion": "arXiv:2505.22311, 2025. [7] D. B. Acharya, K. Kuppan, and B. Divya, \u201cAgentic AI: Autonomous intelligence for complex goals\u2013a comprehensive survey,\u201dIEEE Access, 2025. [8] C. Zhao, J. Wang, R. Zhang, D. Niyato, G. Sun, H. Du, D. I. Kim, and A. Jamalipour, \u201cGenerative AI-enabled wireless communi- cations for robust low-altitude economy networking,\u201darXiv preprint arXiv:2502.18118, 2025. [9] T. Schick, J. Dwivedi-Yu, R. Dess `\u0131, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom, \u201cToolformer: Language models can teach themselves to use tools,\u201dAdvances in Neural Infor- mation Processing Systems, vol. 36, pp. 68 539\u201368 551, 2023. [10] G. Zhang, K. Chen, G. Wan, H. Chang, H. Cheng, K. Wang, S. Hu, and L. Bai, \u201cEvoflow: Evolving diverse agentic workflows on the fly,\u201darXiv preprint arXiv:2502.07373, 2025. [11] H. Sun, Y . Zhuang, L. Kong, B. Dai, and C. Zhang, \u201cAdaplanner: Adaptive planning from feedback with language models,\u201dAdvances in neural information processing systems, vol. 36, pp. 58 202\u201358 245, 2023. [12] Y . Xu, J. Wang, R. Zhang, D. Niyato, D. Rajan, L. Yu, H. Zhou, A. Jamalipour, and X. Wang, \u201cEnhancing wireless networks for IoT with large vision models: Foundations and applications,\u201darXiv preprint arXiv:2508.00583, 2025. [13] 3GPP, \u201cEvolved Universal Terrestrial Radio Access (E- UTRA); Radio Resource Control (RRC); Protocol specifica- tion,\u201d 3rd Generation Partnership Project (3GPP), Technical Specification (TS) 36.331, 04 2017, version 14.2.2. [On- line]. Available: https://portal.3gpp.org/desktopmodules/Specifications/ SpecificationDetails.aspx?specificationId=2440 [14] L. Zhu, W. Ma, W. Mei, Y . Zeng, Q. Wu, B. Ning, Z. Xiao, X. Shao, J. Zhang, and R. Zhang, \u201cA tutorial on movable antennas for wireless networks,\u201dIEEE Communications Surveys & Tutorials, pp. 1\u20131, 2025. [15] C. Qian, W. Liu, H. Liu, N. Chen, Y . Dang, J. Li, C. Yang, W. Chen, Y . Su, X. Conget al., \u201cChatdev: Communicative agents for software development,\u201darXiv preprint arXiv:2307.07924, 2023."
  },
  {
    "id": "2510.04618v1",
    "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
    "authors": "Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, Kunle Olukotun",
    "published": "2025-10-06",
    "category": "cs.LG",
    "arxiv_url": "http://arxiv.org/abs/2510.04618v1",
    "pdf_url": "https://arxiv.org/pdf/2510.04618v1",
    "abstract": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on\ncontext adaptation\u2014modifying inputs with instructions, strategies, or evidence, rather than weight updates.\nPrior approaches improve usability but often suffer from brevity bias, which drops domain insights for\nconcise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on\nthe adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (AgenticContextEngineering),\na framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies\nthrough a modular process of generation, reflection, and curation. ACE prevents collapse with structured,\nincremental updates that preserve detailed knowledge and scale with long-context models. Across agent\nand domain-specific benchmarks, ACE optimizes contexts both offline (e.g.,system prompts) and online (e.g.,\nagent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while\nsignificantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without\nlabeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard,\nACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder\ntest-challenge split, despite using a smaller open-source model. These results show that comprehensive,\nevolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.\n1",
    "introduction": "Base LLMICLGEPADC ACE40.042.545.047.550.052.555.057.560.0Accuracy (%)\n42.4%46.0%46.4%51.9%59.5%Agent: AppWorld\nBase LLMICLGEPADC ACE6870727476788082\n70.7%72.3%73.5%74.2%78.3%Domain Knowledge: FiNER\nBase LLMICLGEPADC ACE6668707274767880\n67.5%67.0%71.5%\n69.5%76.5%Numerical Reasoning: Formula\nFigure 1:Overall Performance Results.Our proposed framework, ACE, consistently outperforms strong\nbaselines across agent and domain-specific reasoning tasks.\nModern AI applications based on large language models (LLMs), such as LLM agents [ 49,52] and compound\nAI systems [ 55], increasingly depend oncontext adaptation. Instead of modifying model weights, contextarXiv:2510.04618v1  [cs.LG]  6 Oct 2025\nadaptation improves performance after model training by incorporating clarified instructions, structured\nreasoning steps, or domain-specific input formats directly into the model\u2019s inputs. Contexts underpin\nmany AI system components, including system prompts that guide downstream tasks [ 4,36], memory that\ncarries past facts and experiences [ 41,48], and factual evidence that reduces hallucination and supplements\nknowledge [6].\nAdapting throughcontextsrather thanweightsoffers several key advantages. Contexts are interpretable and\nexplainable for users and developers [ 45,47], allow rapid integration of new knowledge at runtime [ 7,27],\nand can be shared across models or modules in a compound system [ 23]. Meanwhile, advances in long-\ncontext LLMs [ 39] and context-efficient inference such as KV cache reuse [ 17,51] are making context-based",
    "conclusion": "the current playbook - Avoid redundancy - if similar advice already exists, only add new content that is a perfect complement to the existing playbook - Do NOT regenerate the entire playbook - only provide the additions needed - Focus on quality over quantity - a focused, well-organized playbook is better than an exhaustive one - Format your response as a PURE JSON object with specific sections - For any operation if no new content to add, return an empty list for the operations field - Be concise and specific - each addition should be actionable Training Context: Total token budget: {token_budget} tokens Training progress: Sample {current_step} out of {total_samples} Current Playbook Stats: {playbook_stats} Recent Reflection: {recent_reflection} Current Playbook: {current_playbook} Question Context: {question_context} Your Task: Output ONLY a valid JSON object with these exact fields: - reasoning: your chain of thought / reasoning / thinking process, detailed analysis and calculations - operations: a list of operations to be performed on the playbook - type: the type of operation to be performed - section: the section to add the bullet to - content: the new content of the bullet Available Operations: 1. ADD: Create new bullet points with fresh IDs - section: the section to add the new bullet to - content: the new content of the bullet. Note: no need to include the bullet_id in the content like \u2018[ctx-00263] helpful=1 harmful=0 ::\u2019, the bullet_id will be added by the system. RESPONSE FORMAT - Output ONLY this JSON structure (no markdown, no code blocks): { \"reasoning\" : \"[Your chain of thought / reasoning / thinking process, detailed analysis and calculations here]\" , \"operations\" : [ { { \"type\" : \"ADD\" , \"section\" : \"formulas_and_calculations\" , \"content\" : \"[New calculation method...]\" } } ] }Figure 14: ACE Curator prompt on FINER 23"
  },
  {
    "id": "2510.27051v1",
    "title": "Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement",
    "authors": "Aaditya Shukla, Sidney Knowles, Meenakshi Madugula, Dave Farris, Ryan Angilly, Santiago Pombo, Anbang Xu, Lu An, Abhinav Balasubramanian, Tan Yu, Jiaxiang Ren, Rama Akkiraju",
    "published": "2025-10-30",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2510.27051v1",
    "pdf_url": "https://arxiv.org/pdf/2510.27051v1",
    "abstract": "\u2014Enterprise AI agents must continuously adapt to\nmaintain accuracy, reduce latency, and remain aligned with user\nneeds. We present a practical implementation of a data flywheel\nin NVInfo AI, NVIDIA\u2019s Mixture-of-Experts (MoE) Knowledge\nAssistant serving over 30,000 employees. By operationalizing\na MAPE-driven data flywheel, we built a closed-loop system\nthat systematically addresses failures in retrieval-augmented\ngeneration (RAG) pipelines and enables continuous learning.\nOver a 3-month post-deployment period, we monitored feedback\nand collected 495 negative samples. Analysis revealed two major\nfailure modes: routing errors (5.25%) and query rephrasal errors\n(3.2%). Using NVIDIA NeMo microservices, we implemented\ntargeted improvements through fine-tuning. For routing, we\nreplaced a Llama 3.1 70B model with a fine-tuned 8B variant,\nachieving 96% accuracy, a 10\u00d7 reduction in model size, and 70%\nlatency improvement. For query rephrasal, fine-tuning yielded\na 3.7% gain in accuracy and a 40% latency reduction. Our\napproach demonstrates how human-in-the-loop (HITL) feedback,\nwhen structured within a data flywheel, transforms enterprise\nAI agents into self-improving systems. Key learnings include\napproaches to ensure agent robustness despite limited user\nfeedback, navigating privacy constraints, and executing staged\nrollouts in production. This work offers a repeatable blueprint\nfor building robust, adaptive enterprise AI agents capable of\nlearning from real-world usage at scale.\nIndex Terms\u2014MAPE control loops, data flywheel, MoE sys-\ntems, self-improving AI agents, continuous learning, user feed-\nback, parameter-efficient fine-tuning (PEFT), RAG, HITL, en-\nterprise AI, latency optimization\nI.",
    "introduction": "systems often deteriorates post-deployment due to evolv- ing user intent, domain drift, and the absence of systematic feedback integration. A central challenge in operationalizing such agents lies in enabling them to continuously adapt based on real-world usage patterns and user feedback, without requiring full-scale retraining or infrastructure overhauls. While retrieval-augmented generation (RAG) pipelines and Mixture-of-Experts (MoE) architectures have improved the relevance and efficiency of enterprise AI agents, most pro- duction deployments remain static and reactive. Feedbackmechanisms, if present, are frequently decoupled from the model improvement process. This disconnect results in stag- nant accuracy, increasing latency, and declining user trust. There is a pressing need for closed-loop systems that can monitor agent performance, analyze failure modes, and ex- ecute targeted optimizations in a cost-efficient and privacy- aware manner. In this work, we introduce a MAPE-based data flywheel framework that enables continuous learning in enterprise AI agents through a modular, feedback-driven pipeline. Adapted from self-adaptive control loops, this framework supports the deployment of agents that evolve incrementally over time. We apply this approach to NVIDIA\u2019s deployment of NVInfo AI, an internal Knowledge Assistant Agent that serves over 30,000 employees across diverse domains including engineer- ing, operations, HR, and sales. NVInfo AI integrates user feedback with performance telemetry to identify actionable failure signals and execute targeted updates using parameter- efficient fine-tuning (PEFT) and model specialization. Over a three-month observation window, we collected and analyzed 495 negative feedback samples, revealing two dom- inant sources of failure: routing errors (5.25 %) and query rephrasal errors (3.2 %). Utilizing NVIDIA NeMo microser- vices, we applied lightweight, component-specific fine-tuning strategies to improve performance. \u2022For routing, we reduced model size by a factor of ten (from 70 billion to 8 billion parameters) while maintaining 96% accuracy and reducing latency by 70%. \u2022For query rephrasal, we achieved a 3.7% improvement in accuracy (measured on a synthetic dataset generated from manually analyzed incorrect queries, expanded to 5,000 examples and split 80/10/10), along with a 40% reduction in response latency. This work makes three key contributions: \u2022We demonstratea novel application of the MAPE control loop to the domain of GenAI agent improvement, bridging observability and action in a continuous feedback pipeline. \u2022We presentan empirical analysis of post-deploymentarXiv:2510.27051v1 [cs.AI] 30 Oct 2025 failure modesin a production-grade enterprise AI agent, informed by real user feedback. \u2022We providea modular implementation blueprint using NVIDIA NeMo microservices, offering a practical archi- tecture for organizations seeking to build adaptive and self- correcting AI agents. II. BACKGROUND ANDRELATEDWORK A. From MAPE-K to Agentic AI: Foundations of Self-Adaptive Systems The MAPE-K (Monitor, Analyze, Plan, Execute \u2013 Knowl- edge) reference model, introduced by IBM [1], remains foundational for designing self-adaptive software systems by structuring behavior into a control loop that continuously responds to environmental changes, with its modular archi- tecture enabling broad adoption across multiple domains [2]\u2013 [6]. Central to its evolution is the Knowledge component, which supports long-term reasoning and intelligent adaptation, especially when integrated with machine learning to enable predictive and causal decision-making [7]\u2013[9]. Within agentic AI frameworks,",
    "conclusion": "{ \"Question\": { \"type\": \"string\", \"description\": \"Generated Question from the input document.\" }, \"Answer\": { \"type\": \"string\", \"description\": \"Corresponding Answer from the input document that answers the Question .\" }, \"Thought\": { \"type\": \"string\", \"description\": \"Short analysis of your understanding from the Question.\" }, \"Process\": { \"type\": \"string\", \"description\": \"I need to use the Enterprise Knowledge tool.\" }, \"Action\": { \"type\": \"string\", \"description\": \"EnterpriseKnowledge\" }, \"Action Input\": { \"type\": \"list\", \"description\": \"A single line Python list of rephrased queries.\" } } } Examples Input Document:<Content of input document> Input Document url:<url of input document> Output { \"Question\": \"I am based in the Netherlands, when is pay day?\", \"Answer\": \"25th of every month\", \"Thought\": \"Payroll timing question; include location keywords in rephrased queries.\", \"Process\": \"I need to use the Enterprise Knowledge tool\", \"Action\": \"EnterpriseKnowledge\", \"Action Input\": [ \"payday schedule netherlands\", \"netherlands pay days\" ] } Input Document:<Content of input document> Input Document url:<url of input document> Output { \"Question\": \"point me to gpu fcv page?\", \"Answer\": \"https://nvidia.sharepoint.com/sites/TechnicalTraining/ASIC%20teams.aspx\", \"Thought\": \"Needs GPU FCV (Full Chip Verification) page.\", \"Process\": \"I need to use the Enterprise Knowledge tool\", \"Action\": \"EnterpriseKnowledge\", \"Action Input\": [ \"gpu fcv page company\", \"fcv gpu url\" ] } Input Document:<Content of input document> Input Document url:<url of input document> Output { \"Question\": \"ok, i\u2019m looking for an nvidia icon for biotech / pharmaceuticals to use in a presentation. can you help me find that?\", \"Answer\": \"https://nvidia.sharepoint.com/sites/nvinfo/brand/Pages/default.aspx\", \"Thought\": \"Needs a company icon for biotech/pharma use.\", \"Process\": \"I need to use the Enterprise Knowledge tool\", \"Action\": \"EnterpriseKnowledge\", \"Action Input\": [ \"company icons\", \"company logos biotech\" ] } Task output format Generate3 pairsby following the instructions based on the Input Document. Strictly return only a Python list of pairs and nothing else. Input Document:<Content of input document> Input Document url:<url of input document> Output:###"
  }
]