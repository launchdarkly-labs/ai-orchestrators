[
  {
    "id": "2508.00271v2",
    "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning",
    "authors": "Hongjin Qian, Zheng Liu",
    "published": "2025-08-01",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2508.00271v2",
    "pdf_url": "https://arxiv.org/pdf/2508.00271v2",
    "abstract": "Technical Report METAAGENT : TOWARD SELF-EVOLVING AGENT VIA TOOL META-LEARNING Hongjin Qian, Zheng Liu\u2217 BAAI {chienqhj, zhengliu1026 }@gmail.com ABSTRACT In this work, we propose MetaAgent, an agentic paradigm inspired by the princi- ple of learning-by-doing, where expertise is developed through hands-on prac- tice and continual self-improvement. MetaAgent starts with a minimal work- flow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent au- tonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as meta tool learning , through which MetaAgent incrementally refines its reasoning and tool-use strate- gies, without changing model parameters or requiring further post-training. Eval- uated on challenging knowledge discovery benchmarks, including GAIA, Web- WalkerQA, and BrowseComp, MetaAgent consistently outperforms workflow- based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowl- edge discovery. We provide our source codes in this repository . 1 I NTRODUCTION Recent information-seeking systems powered by large language models (LLMs), such as ChatGPT, have achieved remarkable success (Ouyang et al., 2022; Google, 2023). These systems can provide instant answers to almost any type of information need, ranging from simple factual queries to complex information aggregation (Zhao et al., 2024a; Wang et al., 2024). However, when faced with more complex tasks that require synthesizing information across multi-step reasoning or interacting with external tools, most current LLMs struggle to deliver accurate solutions,",
    "introduction": "especially in deep knowledge discovery scenarios (Li et al., 2025c; Phan et al., 2025; Zhu et al., 2024). As illustrated in Figure 1, the query \u201cWhich ICLR 26 hotel is nearest to the venue within C275/night?\u201d exemplifies a simplified deep knowledge discovery scenario. To answer this ques- tion, an agent must first identify the list of conference hotels through web search, then check current currency exchange rates. By accurately converting prices to the target currency using computational tools and comparing the distances of eligible hotels to the venue, the agent can determine the correct answer. However, even when equipped with search tools, standard LLMs struggle with such queries because they cannot effectively manage the sequential reasoning and tool use required to integrate information from multiple sources and execute each step of the problem (Qian et al., 2025; Wei et al., 2025; Zhao et al., 2024a). To overcome such challenges, recent advances in agentic AI have focused on harnessing the rea- soning capabilities of LLMs not only for generating answers, but also for planning and executing multi-step solutions to complex tasks, enabling scalable deep reasoning even at test time (OpenAI, 2025; Snell et al., 2024). In this agentic paradigm, the LLM acts as a central coordinator: it de- composes a complex query into manageable sub-tasks, interacts with external tools, such as search \u2217Corresponding Author 1arXiv:2508.00271v2 [cs.AI] 1 Sep 2025 Technical Report Which ICLR 26 hotel is nearest to the venue within \u20ac275/night? Conference hotel price list: InterContinental 349 USD Toronto Marriott 379 USD \u2026 13% sale tax, 8.5%\u2026 USD to Euro exchange rate 379 USD x (1+TR) x CR = \u2026 349 USD x (1+TR) x CR = \u2026 \u2026 Compare eligible hotels by distance Vanilla LLM/RAGAgentic Solver Lacks dynamic interaction with external tools, making closed-loop reasoning impossible.Good AnswerIncorrect AnswerExternal Tools MetaAgent Dynamic Context Engineering Starting from Minimal Design Building In-House Tools Learning From Success & FailureSelf-Re%nementSelf-Expansion Learning Tools by Using Tools Figure 1: Illustration of a deep knowledge discovery task with sequential information dependen- cies, requiring multi-step reasoning and dynamic tool use to arrive at the correct answer. These tasks challenge standard LLMs and RAG approaches, which lack flexible tool interaction. Agentic systems address this limitation. MetaAgent demonstrates this approach by starting with a minimal, adaptable design and evolving through continual, data-driven task completion. engines or code compilers, and incrementally integrates the information obtained at each step into its reasoning trajectory (Li et al., 2025a; Jin et al., 2025b; Li et al., 2025b; Zheng et al., 2025; Dong et al., 2025). By following this paradigm, agentic systems can resolve intricate dependencies and deliver accurate solutions to deep knowledge discovery problems that static LLMs alone cannot handle (Li et al., 2025c). To implement agentic systems, there are currently two main approaches. The first approach is to manually design task-specific workflows, where human experts predefine how the agent should plan tasks and use tools (Li et al., 2025a; Wu et al., 2025c; Soni et al., 2025). Although this method can be effective",
    "conclusion": "Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572 , 2025b. Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the deep research. arXiv preprint arXiv:2502.04644 , 2025c. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR) , 2023. Weinan Zhang, Junwei Liao, Ning Li, Kounianhua Du, and Jianghao Lin. Agentic information retrieval. arXiv preprint arXiv:2410.09713 , 2024. Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao. Pro- cess vs. outcome reward: Which is better for agentic rag reinforcement learning, 2025. URL https://arxiv.org/abs/2505.14069 . Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna K. Qiu, and Lili Qiu. Retrieval aug- mented generation (rag) and beyond: A comprehensive survey on how to make your llms use external data more wisely, 2024a. URL https://arxiv.org/abs/2409.14924 . Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji- Rong Wen. A survey of large language models, 2024b. URL https://arxiv.org/abs/ 2303.18223 . Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environ- ments, 2025. URL https://arxiv.org/abs/2504.03160 . Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zhicheng Dou, and Ji-Rong Wen. Large language models for information retrieval: A survey, 2024. URL https://arxiv.org/abs/2308.07107 . 14"
  },
  {
    "id": "2501.11425v3",
    "title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training",
    "authors": "Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, Jiecao Chen",
    "published": "2025-01-20",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2501.11425v3",
    "pdf_url": "https://arxiv.org/pdf/2501.11425v3",
    "abstract": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Siyu Yuan1,2,\u2217,\u2020,Zehui Chen2,\u2217,\u2020,Zhiheng Xi1,\u2020,Junjie Ye1,2,\u2020,Zhengyin Du2,Jiecao Chen2 1Fudan University ,2ByteDance Seed \u2217Equal contributions ,\u2020Work done at ByteDance Seed Abstract Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive and agentic environments. Existing work primarily focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is notoriously difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent toReflect on the fly. Unlike traditional methods that reward or penalize actions solely based on correctness, our approach leverages Monte Carlo Tree Search (MCTS) to construct training samples that recover correct trajectories from erroneous ones. A key challenge of agent task reflection lies in the necessity for timely revision rather than waiting until the end of a rollout to revise errors. To address this, we introduce a model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in a failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that this approach continuously improves the model\u2019s ability to recover from errors and enables earlier/timely error correction. Extensive experiments on three representative interactive and agentic environments show that the proposed framework effectively equips agents",
    "introduction": "to identify and correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%). Date:March 25, 2025 Correspondence: Siyu Yuan at syyuan21@m.fudan.edu.cn Project Page: https://github.com/bytedance/Agent-R 1arXiv:2501.11425v3 [cs.AI] 24 Mar 2025 1 Introduction Large Language Models (LLMs) have become foundational tools in solving complex tasks across interactive and agentic environments [ 5,7,45,52,60]. These LLM-based agents are increasingly employed in scenarios requiring capabilities such as autonomous decision-making, error correction, and task optimization [ 22,31]. Despite the impressive performance of the existing methods, their reliance on behavior cloning from stronger experts poses significant limitations: due to the adoption of all-correct trajectories for training, it struggles to proactively self-correct errors, leading to cascading failures and suboptimal task performance [ 50,59]. This limitation arises from an inability to effectively detect errors or revise trajectories dynamically once errors occur, highlighting the need for methods emphasizing timely revision capabilities. Previous work has proposed methods relying on explicit error signals or reward functions for self- correction. However, these methods mainly focus on single-turn scenarios, such as code repair [ 4,15], tool use [ 26], and mathematical reasoning [ 12,18]. In contrast, tasks in interactive and agentic environments usually involve multi-turn interactions and do not reveal explicit error signals until reaching the terminal state. Additionally, unlike mathematical reasoning [ 49], designing high-quality reward functions to critique intermediate actions in long interactive trajectories remains difficult. Wang et al. [43]improves agents\u2019 self-reflection in embodied tasks, but it heavily relies on expert models for correction during training data construction. A critical bottleneck in enhancing error recovery in interactive and agentic environments is the lack of step-level reflection data. Traditional approaches to collecting these datasets involve labor-intensive annotation processes, which are both time-consuming and costly [ 20,57,63]. Without robust reflection data, models face challenges in identifying and correcting their own errors, limiting their utility as intelligent agents. Constructing reflection datasets is thus essential for building agents capable of self-reflection and better decision-making. However, how to automatically construct such training samples is non-trivial. A significant challenge of agent task reflection lies in the necessity for timely revision rather than waiting until the end of a rollout to revise errors. If corrections are applied only at the end of the trajectory, the delayed revisions prevent agents from learning to detect and address errors as they occur, undermining their capacity for real-time self-reflection. Furthermore, delayed revisions may leave catastrophic errors unaddressed, particularly those occurring early in the trajectory. To address these challenges, we propose Agent-R, a novel framework designed to enable LLM-based agentstoperformon-the-flyreflectionandself-improvement. Unlikepreviousreward-basedapproaches, which directly penalize or reward actions based solely on outcome correctness [ 32,37,40,51], Agent-R introduces a dynamic self-training framework that revises errors at the step level. By leveraging Monte Carlo Tree Search (MCTS) [ 16], Agent-R dynamically constructs training samples that recover correct trajectories from erroneous ones, effectively guiding the agent to navigate complex decision spaces. Specifically, Agent-R identifies the most suitable revision step (based on the current actor model) in an incorrect trajectory and connects it with the",
    "conclusion": "sleeve, classic fit, and color heather grey, as specified in the task description. Therefore, Judgment: Bad Judgment (Irrelevant Actions): The action is bad becauseitisnotrelevant tothetaskof finding men\u2019s shirts with specific characteristics. Therefore, Judgment: Bad Sciworld Judgment (Invalid Actions): There is no orange mentioned in the environment, so thisactionisinvalid. It\u2019s not a helpful action, but rather an invalid input. Therefore, Judgment: Bad Judgment (Mismatch Observation): The current task is to determine if the aluminum foil is electrically conductive. The agent has already connected the aluminum foil to the battery and the green light bulb, and observed that the light bulb turned on. This suggests that the aluminum foil is electrically conductive. Moving the aluminum foil to the yellow box would mean that the agent is giving up on the possibility of the aluminum foil being conductive, whichisnotinlinewiththecurrentobservation . Therefore, Judgment: Bad Judgment (Irrelevant Actions): The agent is not using the art studio\u2019s resources effectively, as thecup containing redpaintisnotrelatedtothetask of creating green paint. Therefore, Judgment: Bad TextCraft Judgment (Invalid Actions): The agent is stuck in an infinite loop of trying to execute an invalidaction, which is not helpful in solving the task. Therefore, Judgment: Bad Judgment (Mismatch Observation): The action is not good because it is not possible to craft a dried kelp block with 9 dried kelp. Theobservation suggests thattheagentlacksthenecessary itemstocraftthe block. This action is actually harmful to the task of crafting the block. Therefore, Judgment: Bad Judgment (Irrelevant Actions): The agent is trying to craft diamond boots, which requires 4 diamonds. The agent has already obtained 4 diamonds, which is sufficient to craft the boots. However, the agent is trying to craft a diamond block instead, which requires 9 diamonds. Thisactionisnothelpfulin achieving thegoalofcrafting diamond boots, as it will waste the 4 diamonds the agent already has. Therefore, Judgment: Bad 26"
  },
  {
    "id": "2601.11974v1",
    "title": "Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement",
    "authors": "Xinmeng Hou, Peiliang Gong, Bohao Qu, Wuqi Wang, Qing Guo, Yang Liu",
    "published": "2026-01-17",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2601.11974v1",
    "pdf_url": "https://arxiv.org/pdf/2601.11974v1",
    "abstract": "While Large Language Models (LLMs) en-\nable complex autonomous behavior, current\nagents remain constrained by static, human-\ndesigned prompts that limit adaptability. Ex-\nisting self-improving frameworks attempt to\nbridge this gap but typically rely on ineffi-\ncient, multi-turn recursive loops that incur\nhigh computational costs. To address this,\nwe propose Metacognitive Agent Reflective\nSelf-improvement (MARS), a framework that\nachieves efficient self-evolution within a sin-\ngle recurrence cycle. Inspired by educational\npsychology, MARS mimics human learning by\nintegrating principle-based reflection (abstract-\ning normative rules to avoid errors) and pro-\ncedural reflection (deriving step-by-step strate-\ngies for success). By synthesizing these in-\nsights into optimized instructions, MARS al-\nlows agents to systematically refine their rea-\nsoning logic without continuous online feed-\nback. Extensive experiments on six bench-\nmarks demonstrate that MARS outperforms\nstate-of-the-art self-evolving systems while sig-\nnificantly reducing computational overhead.\nCode are available at https://anonymous.\n4open.science/r/MARS-9F16\n1",
    "introduction": "Principles (Concise Enhancement) Injecting Procedure (Reasoning Enhancement)Human Metacognitive Re\ufb02ection Metacognitive Agent with Relective Self-improvementInspires ConstraintsGuides Step-wise ReasoningFigure 1: The Cognitive Inspiration behind MARS. This framework parallels human reflection with the MARS (Metacognitive Agent with Reflective Self- improvement) agent, converting baseline agent failures into principled-based and procedural instructions to syn- thesize enhanced prompts. G\u00f6del machines (Schmidhuber, 2007), which for- malized self-referential systems that rewrite their own code. Although formal proof requirements make the original framework impractical (Steune- brink and Schmidhuber, 2011), it has inspired mod- ern self-improving systems that rely on empirical validation instead. However, current self-improvement frameworks for LLM agents tend to be constrained by multi- turn recursiveness, which results in inefficient learn- ing and adaptation, as well as excessive computa- tional resource usage. Humans, by contrast, are able to resolve previous errors and adapt to new solutions more efficiently through structured learn- ing approaches. Research in education science has identified two complementary paradigms for guid- ing learners (Hiebert and Lefevre, 1986; Ander- son, 1983). The first isprinciple-based learning, which focuses on helping learners avoid mistakes by establishing conceptual categories of what is correct versus incorrect, and understanding the un- derlying rules that govern a domain (Hiebert and Lefevre, 1986; Rittle-Johnson et al., 2001). The second isprocedural learning, which emphasizesarXiv:2601.11974v1 [cs.AI] 17 Jan 2026 using prior experience and step-by-step reasoning to increase the likelihood of successful outcomes (Anderson, 1983; Kolb, 1984). Rather than learn- ing in isolation, humans benefit most when they integrate both approaches through systematic re- flection and summarization of their experiences. Studies in metacognition have shown that struc- tured reflection\u2014where learners explicitly analyze what worked, what failed, and why\u2014significantly improves learning efficiency and knowledge trans- fer (Flavell, 1979; Kaplan et al., 2013; Stanton et al., 2021). Furthermore, research on productive failure demonstrates that learning from one\u2019s own errors, when properly guided, leads to deeper con- ceptual understanding than direct instruction alone (Kapur, 2014, 2010). In this work, we proposeMARS(Metacognitive Agent withReflectiveSelf-improvement), a frame- work that enables multi-agent systems to achieve efficient self-improvement within a single recur- rence cycle by integrating both principle-based and procedural learning approaches. Inspired by human metacognitive learning, MARS allows agents to systematically reflect on their experiences, extract- ing general principles that help avoid past mistakes while simultaneously deriving procedural knowl- edge that replicates successful strategies. Unlike existing self-evolving agent frameworks that rely on multi-turn recursive improvement, which often leads to inefficient learning and excessive computa- tional costs, MARS consolidates the learning pro- cess through structured summarization, enabling agents to maximize adaptation efficiency in each improvement cycle. Our main contributions are as follows: \u2022We propose MARS, a self-improvement framework for multi-agent systems that inte- grates principle-based and procedural learning inspired by human meta-cognitive theory. \u2022We introduce a triple-pathway reflection mechanism that extracts: (1) normative princi- ples for error avoidance, (2) procedural strate- gies for success replication, and (3) a unified synthesis of both pathways. \u2022We design a structured summarization mod- ule that consolidates learning within a single cycle, reducing computational overhead from multi-turn recursive",
    "conclusion": "+= \"### Critical Warnings by Question Type:\\n\\n\" 15forenhinsorted_enh[:8]: # Top-weighted groups 16text += f\"**{enh.question_type} ({\u2019/\u2019.join(enh. topics)})** \" 17text += f\"({enh.num_questions} failures):\\n\" 18text += \"[!] \" + \" | \".join(enh.key_warnings[:3]) + \"\\n\" 19text += f\" -> {enh.enhanced_prompt_addition}\\n\\n\" 20all_prompts[\u2019concise\u2019] = base_prompt + text # P\u2019 = P + E 21 22# Reasoning enhancement E^(r): process-oriented hints 23if\u2019reasoning\u2019inself.enhancement_types: 24text = f\"\\n## GUIDANCE FOR {category.upper()}\\n\" 25text += \"### Key Considerations by Problem Type:\\n\\n\" 26forenhinsorted_enh[:6]: 27text += f\"* {enh.question_type} ({\u2019/\u2019.join(enh. topics)}): \" 28text += f\"{enh.enhanced_prompt_addition}\\n\" 29all_prompts[\u2019reasoning\u2019] = base_prompt + text 30 31# Specific enhancement E^(c+r): combines concise + reasoning 32# Includes: mistakes (concise) + verification (concise) + approach (reasoning) 33if\u2019specific\u2019inself.enhancement_types: 34text = f\"\\n## GUIDANCE FOR {category.upper()}\\n\" 35forenhinsorted_enh[:10]: 36text += f\"**{enh.question_type} - {\u2019 & \u2019.join(enh. topics)}**\\n\" 37# From concise: explicit warnings as mistake patterns 38text += \"Common Mistakes:\\n\" 39forminenh.common_mistakes[:3]: text += f\" x {m }\\n\" 40# From concise: action sequences as verification 41text += \"Verification Steps:\\n\" 42forsinenh.verification_steps[:4]: text += f\" + {s}\\n\" 43# From reasoning: process-oriented approach 44text += f\"Approach: {enh.type_specific_approach}\\n \\n\" 45all_prompts[\u2019specific\u2019] = base_prompt + text 46 47returnall_prompts Listing 4: Code for enhancement generation and prompt aggregation. G.5 Hybrid Selection Listing 5 implements the hybrid strategy (Equa- tion 6) that selects optimal enhancement per cate- gory. 1defselect_hybrid_enhancement(self, val_data: Dict[str, List], 2enhancements: Dict) -> Dict[str, str]: 3\"\"\"Select E*_c = argmax Acc(E, V_c) for each category c 4where E in {E^(c), E^(r), E^(c+r)} (concise, reasoning, specific)\"\"\" 5 6optimal = {} 7# specific = concise + reasoning (c+r) 8etypes = [\u2019concise\u2019, \u2019reasoning\u2019, \u2019specific\u2019] 9 10forcategory, val_questionsinval_data.items(): 11best_acc, best_type = 0, \u2019concise\u2019 12 13foretypeinetypes: 14enhanced_prompt = enhancements.get(f\"{category}_{ etype}\") 15if notenhanced_prompt:continue 16 17correct =sum(1forqinval_questions 18ifself.evaluate(enhanced_prompt, q) 19== q[\u2019correct_answer\u2019]) 20accuracy = correct /len(val_questions) 21 22ifaccuracy > best_acc: 23best_acc, best_type = accuracy, etype 24 25optimal[category] = best_type 26print(f\" {category}: \u2019{best_type}\u2019 (acc: {best_acc :.1%})\") 27 28returnoptimal Listing 5: Code for hybrid enhancement selection."
  },
  {
    "id": "2511.19436v1",
    "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection",
    "authors": "Qiang Wang, Xinyuan Gao, SongLin Dong, Jizhou Han, Jiangyang Li, Yuhang He, Yihong Gong",
    "published": "2025-11-24",
    "category": "cs.CV",
    "arxiv_url": "http://arxiv.org/abs/2511.19436v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19436v1",
    "abstract": "We presentVDC-Agent, a self-evolving framework for\nVideo Detailed Captioning that requires neither human an-\nnotations nor larger teacher models. The agent forms a\nclosed loop of caption generation, principle-guided scor-\ning (score and textual suggestions), and prompt refinement.\nWhen caption quality regresses, a self-reflection path lever-\nages the previous chain-of-thought to amend the update.\nRunning this process on unlabeled videos produces tra-\njectories of(caption,score)pairs. We convert the trajec-\ntories into preference tuples and filter out samples with\nJSON parsing errors, resulting in VDC-Agent-19K, which\ncontains 18,886 automatically constructed pairs. We then\nfine-tune the base MLLM on this dataset using an easy-\nto-hard curriculum direct preference optimization. Built\non Qwen2.5-VL-7B-Instruct, ourVDC-Agent-7Battains\nstate-of-the-art performance on the VDC benchmark with\n49.08%average accuracy and2.50score, surpassing spe-\ncialized video captioners and improving over the base\nmodel by+5.13%accuracy and+0.27score at similar in-\nference cost.\n1.",
    "introduction": "w/o human feedback w/o external modelsHigh-Quality CaptionVideo High-Quality CaptionHuman Annotations Train Caption Scorer Large-Scale Caption Generation + FilteringVideo High-Quality CaptionMLLM 1 More Powerful MLLM Scoring / Fusion FilteringMLLM 2 MLLM 3 (a) Human-Aligned Pipeline (b) Stronger Teacher Guidance (c) Agentic Self-Reflection (Proposed) PromptSuggestion ReflectionInitial PromptFigure 1.Comparison of video captioning paradigms.(a) Human-aligned pipelines rely on manual annotations to train cap- tion scorers. (b) Multi-MLLM-based pipelines depend on multiple or stronger MLLMs for scoring or fusion. (c) Our proposedVDC- Agentachieves self-improvement through agentic self-reflection, requiring neither human annotations nor larger models. caption datasets [29, 33, 38]. Although these methods have shown strong performance, they typicallyrely on ei- ther distilling caption generation capabilities from more powerful MLLMs (proprietary models like GPT-4V [1] or open-source alternatives like Qwen-72B [2]) or incor- porating extensive manual annotations for human prefer- ence alignment, as shown in Fig. 1 (a)(b). For instance, ShareGPT4Video [4] constructs datasets using GPT-4V , fol- lowed by manual verification and filtering to enhance cap- tion quality. Cockatiel [33] and OwlCap [49] rely on exten- sive human annotations to train caption scorers, or use more powerful captioning models to fuse captions from differ- 1arXiv:2511.19436v1 [cs.CV] 24 Nov 2025 ent MLLMs. A VC-DPO [38] and VideoCap-R1 [29] build upon datasets constructed with powerful captioning models and employ reinforcement learning to further improve per- formance. However, these approaches face several inherent limi- tations: prohibitive human annotation costs, access barri- ers to proprietary APIs, and substantial computational re- sources required for large-scale model inference. Conse- quently, enabling models to achieve autonomous reflection and iterative improvement in caption generation, without dependence on stronger MLLMs or extensive human anno- tations, has become critical for advancing beyond the cur- rent paradigm. To address this challenge,we propose to treat the captioner itself as an autonomous agent that can generate, evaluate, and refine its own captions through it- erative self-reflection. To this end, we proposeVDC-Agent, a self-evolving video captioning framework that enables MLLMs to im- prove themselves through iterative self-reflection without requiring stronger external supervision. As depicted in Fig. 1 (c), VDC-Agent forms a closed-loop system that con- tinuously refines its captioning ability by alternating be- tween caption generation, evaluation, and prompt refine- ment. Given a collection of unlabeled videos, the model first generates captions using an initial prompt. It then con- ducts self-assessment based on a set of principles describ- ing what constitutes a good caption (such as coverage of objects, actions, and temporal dynamics), assigns a quality score to the caption, and produces textual suggestions for improvement. These suggestions guide an internalprompt refiner, which updates the prompt in the next iteration. If the newly generated caption is even worse than the previ- ous one, the model triggers aself-reflectionmechanism that revisits the chain of thought used in the last prompt refine- ment, diagnosing why the previous update failed and avoid- ing the same mistake in subsequent steps. Through repeated cycles of caption\u2013evaluation\u2013refinement, our VDC-Agent can generate higher-quality video descriptions. To internalize VDC-Agent\u2019s self-reflection capability into the MLLM, enabling it to achieve",
    "conclusion": "2024. 2, 6 [44] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action.arXiv preprint arXiv:2303.11381, 2023. 3 [45] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. InThe eleventh international conference on learning representations, 2022. 3 [46] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appa- gent: Multimodal agents as smartphone users. InProceed- ings of the 2025 CHI Conference on Human Factors in Com- puting Systems, pages 1\u201320, 2025. 3 [47] Quan Zhang, Jinwei Fang, Rui Yuan, Xi Tang, Yuxin Qi, Ke Zhang, and Chun Yuan. Weakly supervised temporal action localization via dual-prior collaborative learning guided by multimodal large language models. InProceedings of the Computer Vision and Pattern Recognition Conference, pages 24139\u201324148, 2025. 1 [48] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Zi- wei Liu, and Chunyuan Li. Video instruction tuning with synthetic data.arXiv preprint arXiv:2410.02713, 2024. 2 [49] Chunlin Zhong, Qiuxia Hou, Zhangjun Zhou, Shuang Hao, Haonan Lu, Yanhao Zhang, He Tang, and Xiang Bai. Owlcap: Harmonizing motion-detail for video captioning 10 via hmd-270k and caption set equivalence reward.arXiv preprint arXiv:2508.18634, 2025. 1, 6 [50] Yaoyao Zhong, Wei Ji, Junbin Xiao, Yicong Li, Weihong Deng, and Tat-Seng Chua. Video question answering: Datasets, algorithms and challenges. InProceedings of the 2022 conference on empirical methods in natural language processing, pages 6439\u20136455, 2022. 1 [51] Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, and Cordelia Schmid. Streaming dense video captioning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 18243\u201318252, 2024. 1 11"
  },
  {
    "id": "2410.10934v2",
    "title": "Agent-as-a-Judge: Evaluate Agents with Agents",
    "authors": "Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, J\u00fcrgen Schmidhuber",
    "published": "2024-10-14",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2410.10934v2",
    "pdf_url": "https://arxiv.org/pdf/2410.10934v2",
    "abstract": "Agent-as-a-Judge: Evaluate Agents with Agents Mingchen Zhuge1,2,Changsheng Zhao1,Dylan R. Ashley2,Wenyi Wang2,Dmitrii Khizbullin2, Yunyang Xiong1,Zechun Liu1,Ernie Chang1,Raghuraman Krishnamoorthi1,Yuandong Tian1, Yangyang Shi1,Vikas Chandra1,J\u00a8 urgen Schmidhuber2 1Meta AI,2KAUST Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes\u2014ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI , a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems\u2014by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement. Date: October 18, 2024 Correspondence: mingchen.zhuge@kaust.edu.sa ,cszhao@meta.com Dataset: https://huggingface.co/devai-benchmark Project: https://github.com/metauto-ai/agent-as-a-judge Note: First four authors made core contributions. KAUST crafted the dataset. Work done while Mingchen was interning at Meta, with Changsheng leading. 1 Introduction Recent years have seen multimodal agentic systems move from occasionally being able to solve small toy problems to being regularly deployed for challenging real-world problems (the dream of most AI research). Yet, the current evaluation methods and the available benchmarks for agentic systems are struggling to keep up with these rapid advances, dramatically slowing true progress. We believe that the current issue with evaluating agentic systems stems from the lack of feedback during the",
    "introduction": "Recent years have seen multimodal agentic systems move from occasionally being able to solve small toy\nproblems to being regularly deployed for challenging real-world problems (the dream of most AI research).\nYet, the current evaluation methods and the available benchmarks for agentic systems are struggling to keep\nup with these rapid advances, dramatically slowing true progress.\nWe believe that the current issue with evaluating agentic systems stems from the lack of feedback during the\nintermediate task-solving stages for these nontraditional systems. Agentic systems think more like humans,\noften act step-by-step (Wooldridge, 1999) and often host very human-like symbolic communications internally\nto solve problems (Zhuge et al., 2023). And thus agentic systems should be evaluated like a human, with\nrich evaluative feedback which looks at the full thought and action trajectory; evaluating an agentic system\nin the traditional way is like evaluating a student using multiple-choice testing\u2014a comparatively unreliable\nestimator (Park, 2010). For example, while SWE-Bench (Yang et al., 2024a) is widespread, its evaluation",
    "conclusion": "In this work, we introduced the Agent-as-a-Judge method to use agentic systems to evaluate\nagentic systems. We simultaneously released DevAI: a new benchmark that evaluates the code-generating\nability of agentic systems on complete AI development tasks when used with Agent-as-a-Judge. We went on\nto show that Agent-as-a-Judge outperforms existing methods on this task and that it performs similarly to an\nensemble of expert human evaluators. Altogether, we believe that the above opens the door for scaling up\nagentic far more than before."
  },
  {
    "id": "2510.06711v1",
    "title": "Inefficiencies of Meta Agents for Agent Design",
    "authors": "Batu El, Mert Yuksekgonul, James Zou",
    "published": "2025-10-08",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2510.06711v1",
    "pdf_url": "https://arxiv.org/pdf/2510.06711v1",
    "abstract": "Recent works began to automate the design of\nagentic systems using meta-agents that propose\nand iteratively refine new agent architectures.\nIn this paper, we examine three key challenges\nin a common class of meta-agents.First, we\ninvestigate how a meta-agent learns across iter-\nations and find that simply expanding the con-\ntext with all previous agents, as proposed by\nprevious works, performs worse than ignoring\nprior designs entirely. We show that the perfor-\nmance improves with an evolutionary approach.\nSecond, although the meta-agent designs multi-\nple agents during training, it typically commits\nto a single agent at test time. We find that the\ndesigned agents have low behavioral diversity,\nlimiting the potential for their complementary\nuse.Third, we assess when automated design is\neconomically viable. We find that only in a few\ncases\u2014specifically, two datasets\u2014the overall\ncost of designing and deploying the agents is\nlower than that of human-designed agents when\ndeployed on over 15,000 examples. In contrast,\nthe performance gains for other datasets do not\njustify the design cost, regardless of scale.\n1",
    "introduction": "end, recent works have taken the firststeps in the direction of automating the design of agentic systems (Hu et al., 2024; Li et al., 2024; Saad-Falcon et al., 2024; Niu et al., 2025; Nie et al., 2025; Shang et al., 2025; Wang et al., 2025; Ye et al., 2025; Zhang et al., 2025b,a). Our work fo- cuses on a common class of meta-agents that follow thesample\u2013evaluate\u2013iteratepattern (see Figure 1, Algorithm 1) and highlights three challenges. Meta LearningWe begin by examining the as- sumption that the meta-agent effectively learns from previously discovered agents. Our analysis reveals that the meta-agent framework proposed by Hu et al. (2024) does not meaningfully leverage prior designs. In fact, it performs worse than a base- line that ignores prior designs entirely. In contrast, we demonstrate that an evolutionary context cura- tion strategy, where the generation of the next agent is conditioned on the previous best-performing agents (parents), yields improved performance. Diversity and ComplementarityWhile the meta-agent generates a set of candidate agents, typ- ically only one is deployed, neglecting potential synergies among them. If the designed agents were behaviorally diverse, where each specializes in par- ticular types of queries, this would enable dynamic selection of the most suitable agent per query. How- ever, we find that the designed agents often lack be- havioral diversity, which is even more pronounced when evolutionary strategies are used. Economic ViabilityFor a meta-agent to be eco- nomically viable, the fixed cost of designing a new agent must be justified by corresponding improve- ments in performance. We formalize this trade-off by defining the total cost of a meta-designed agent as the sum of a fixed design cost and a per-example inference cost. This raises the key question:How many test examples are needed before the cost per correct response becomes lower when using the designed agent?In our experiments, we find thisarXiv:2510.06711v1 [cs.AI] 8 Oct 2025 Figure 1:Overview of the meta-agent framework.The Meta-Agent iteratively samples and evaluates agents, refining its outputs through a feedback loop. We focus on three key dimensions: (1) learning from previously designed agents; (2) diversity and complementarity of generated agents; and (3) economic viability. break-even point occurs at approximately 15,000 examples for MMLU and DROP. In contrast, for other datasets, the performance gains do not justify the design cost, regardless of the scale of deploy- ment. 2 Related Works Our primary reference is ADAS (Hu et al., 2024), which has introduced meta-agent search with the idea of searching for agents in the code space. MAS-GPT (Ye et al., 2025) and ScoreFlow (Wang et al., 2025) develop meta-agents by training a model to dynamically generate multi-agent systems for a given query. AgentSquare (Shang et al., 2025) and Archon (Saad-Falcon et al., 2024) explore mod- ular agent architectures and use discrete module recombination to efficiently search design spaces. AutoFlow (Li et al., 2024), Weak-for-Strong (Nie et al., 2025), and ADAS (Hu et al., 2024) use a meta agent that follows the sample-evaluate-iterate paradigm (Algorithm 1). Other recent meta-agent approaches include Multi-agent Supernet (Zhang",
    "conclusion": "includes all 90 agents designed across 3 runs. Figure 5:Average inference cost per test query of the best agents.For best agent in the initial library F (Initial, see Appendix A.1), best agent designed by meta agent with \u03d5C(Cumulative), best agents designed by meta agent with \u03d5P(Parallel) , best agent designed by meta agent with \u03d5E(Evolutionary). Averaged across the single best agents from 3 runs. Best agent is selected based on the highest training performance. Figure 6: Score matrix S, where each row corresponds to an agent and each column to a dataset example. A cell is white if the agent answers correctly and black otherwise. For DROP, gray indicates intermediate F1 scores; for GPQA, gray denotes partial correctness across repeated attempts. The normalized rows, si, serve as agent embeddings, capturing performance across training questions. Figure 7: Cosine similarity matrixC, with agents reordered by descending average similarity to all other agents. Figure 8: Histograms of agent similarities (entries of C), excluding agents with zero performance (all black rows of Sin Figure 6, and corresponding dark blue rows and columns of Cin Figure 7). Only the upper triangular entries ofC(excluding the diagonal) are used, as Cis symmetric. Each subplot shows histograms of similarity scores (x-axis) and their frequency (y-axis). Figure 9: Figure 2 with (1 - Hamming distance) as the similarity metric. All nonzero entries ofSare set to1. Figure 10: Figure 8 with (1 - Hamming distance) as the similarity metric. All nonzero entries ofSare set to1. Figure 11: Training performance of designed agents across iterations. The dotted red line shows the performance of the best agent from the initial library. Figure 12: Design cost of the next agent across iterations. While costs remain stable with Parallel and Evolutionary context curation, they increase linearly with increasing context length in Cumulative context curation."
  },
  {
    "id": "2410.16128v1",
    "title": "SMART: Self-learning Meta-strategy Agent for Reasoning Tasks",
    "authors": "Rongxing Liu, Kumar Shridhar, Manish Prajapat, Patrick Xia, Mrinmaya Sachan",
    "published": "2024-10-21",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2410.16128v1",
    "pdf_url": "https://arxiv.org/pdf/2410.16128v1",
    "abstract": "SMART: Self-learning Meta-strategy Agent for Reasoning Tasks Rongxing Liu*1, Kumar Shridhar*1, Manish Prajapat1, 2, Patrick Xia3and Mrinmaya Sachan1 1ETH Zurich,2ETH AI Center,3Microsoft Tasks requiring deductive reasoning, especially those involving multiple steps, often demand adaptive strategies such as intermediate generation of rationales or programs, as no single approach is universally optimal. While Language Models (LMs) can enhance their outputs through iterative self-refinement and strategy adjustments, they frequently fail to apply the most effective strategy in their first attempt. This inefficiency raises the question: Can LMs learn to select the optimal strategy in the first attempt, without a need for refinement? To address this challenge, we introduce SMART(Self-learning Meta-strategy Agent forReasoning Tasks), a novel framework that enables LMs to autonomously learn and select the most effective strategies for various reasoning tasks. We model the strategy selection process as a Markov Decision Process and leverage reinforcement learning-driven continuous self-improvement to allow the model to find the suitable strategy to solve a given task. Unlike traditional self- refinement methods that rely on multiple inference passes or external feedback, SMARTallows an LM to internalize the outcomes of its own reasoning processes and adjust its strategy accordingly, aiming for correct solutions on the first attempt. Our experiments across various reasoning datasets and with different model architectures demonstrate that SMARTsignificantly enhances the ability of models to choose optimal strategies without external guidance (+15 points on the GSM8K dataset). By achieving higher accuracy with a single inference pass, SMART not only improves performance but also reduces computational costs for refinement-based strategies, paving the way for more efficient and intelligent reasoning in LMs. https://github.com/kumar-shridhar/SMART/ 1. Introduction When people first encounter complex reasoning tasks, such as solving mathematical problems, they often make mistakes or approach them inefficiently [ 3]. However, with experience, humans tend to improve their performance by replacing",
    "introduction": "ineffective or incorrect strategies with more effective ones, using a mix of strategies tailored to the specific task [1, 2, 15, 30, inter alia ]. Language Models (LMs) similarly struggle with reasoning tasks, sometimes producing incoherent results [11,16,25]. Acommonremedyistoresampletheoutput, aprocessknownas refinement . Thisrefinement may involve reusing the same reasoning approach [ 16] or adopting an entirely new one [ 25]. In addition, providing feedback on initial results has proven beneficial during resampling [ 11,13,24,34,inter alia ]. This raises a critical question: Can LMs be taught to optimize their choice of reasoning strategy for specific tasks overtime on the first trial, much like humans do? To address this question, we propose a novel framework called SMART(Self-learning Meta-strategy Agent forReasoning Tasks), which allows LMs to learn optimal strategy selection through a continuous self-learning approach. We model the task of identifying the optimal strategy as a Markov Decision Process (MDP) [ 21,28], where the agent (LM) starts with its pre-trained knowledge and iteratively improves its performance by learning from its own outputs and strategy choices. By integrating the LM\u2019s reasoning abilities with reinforcement learning-driven self-improvement, the agent can simulate different reasoning (*)equal contribution. Correspondence at: { rongxingtianxia611@163.com, shkumar@ethz.ch }arXiv:2410.16128v1 [cs.AI] 21 Oct 2024 SMART:Self-learning Meta-strategy Agent forReasoning Tasks LMt-1Question : John has 10 apples, Jack has 20 apples, .....L2M: How many apples are .. LMt-1Question : John has 10 apples, Jack has 20 apples, .....<incorrect strategy > L2MLMt-1Question : John has 10 apples, Jack has 20 apples, .....PoT: def num_apples(): CoT: There are in total 10+ ..LMtLabelTraining Label RefinementPoT: def .. CoT: There ..Initial Sampling Iterative Refinement1 2 Figure1:Our proposed methodology : In the first step (initial sampling), an agent (LM) chooses a strategy and solves the given task with it. If it is correct, the process ends successfully. If an incorrect strategy is chosen, the agent iteratively refines its strategy, taking previous strategies into account. The process stops when a correct strategy is chosen to solve a task, or when a stopping criterion such as the number of attempts is reached. All correct strategies are used to further refine the model, and the process is repeated. During testing, we sample once from LM \ud835\udc61without refinement. strategies, evaluate their effectiveness based on past outcomes, and adjust its strategy choice accordingly. Our approach differs from traditional methods by focusing on iterative reward-based learning, which encourages the agent to produce the correct inference on the first attempt without resampling. This not only improves cost efficiency - only one sampling step is required during inference - but also results in a more generalizable model capable of adapting its strategy selection based on the specific task. We validate SMARTon a variety of reasoning datasets and LM architectures and show that our method significantly improves the ability of LMs to select optimal strategies on the first try, outperforming baseline models that rely on traditional self-refinement techniques in both accuracy and computational efficiency. On three mathematical datasets (GSM8K [ 5], SVAMP [ 19], ASDiv [ 17]) over three LLM",
    "conclusion": "also has to pay 10% tax on everything. How much did he pay in total? Response: Sub-question 1: How much did John spend on shirts? Answer to Sub-question 1: 3*$20=$ <<3*20=60 >>60 Sub-question 2: How much did John spend on tax? Answer to Sub-question 2: $60*.1=$\u00ab60*.1=6\u00bb6 Sub-question 3: How much did John spend in total? Answer to Sub-question 3: $60+$6=$ <<60+6=66 >>66 Final Answer: 66 <eos> [7 more examples randomly sampled from the training set] Input: Thomas is training at the gym to prepare for a competition. He trained for 5 hours every day for a month (30 days). If he continues to train for the next 12 days, how many hours will he spend on training in total? Response: Model generated response .. Figure6: 8-shot Least to Most demonstration. 17 SMART:Self-learning Meta-strategy Agent forReasoning Tasks Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Instruction: Solve the given math problem by writing a python program. Store your result as a variable named \u2019answer\u2019. Input: John buys 3 dress shirts. They sell for $20 each. He also has to pay 10% tax on everything. How much did he pay in total? Response: total_shirts = 3 cost_of_one_shirt = 20 total_cost_shirts = total_shirts * cost_of_one_shirt tax_rate = 0.1 tax_amount = tax_rate * total_cost_shirts total_cost = total_cost_shirts + tax_amount answer = total_cost <eos> [7 more examples randomly sampled from the training set] Input: Thomas is training at the gym to prepare for a competition. He trained for 5 hours every day for a month (30 days). If he continues to train for the next 12 days, how many hours will he spend on training in total? Response: Model generated response .. Figure7: 8-shot Program of Thought demonstration. 18"
  },
  {
    "id": "2508.02959v2",
    "title": "Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow",
    "authors": "Chia-Tung Ho, Jing Gong, Xufeng Yao, Yunsheng Bai, Abhishek B Akkur, Haoxing Ren",
    "published": "2025-08-04",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2508.02959v2",
    "pdf_url": "https://arxiv.org/pdf/2508.02959v2",
    "abstract": "Large language models (LLMs) excel at solving complex\ntasks by executing agentic workflows composed of detailed\ninstructions and structured operations. Yet, building general-\npurpose agents by manually embedding foundation mod-\nels into agentic systems such as Chain-of-Thought, Self-\nReflection, and ReACT through text interfaces limits scala-\nbility and efficiency. Recently, many researchers have sought\nto automate the generation and optimization of these work-\nflows through code-based representations. However, existing\nmethods often rely on labeled datasets to train and optimize\nworkflows, making them ineffective and inflexible for solv-\ning real-world, dynamic problems where labeled data is un-\navailable. To address this challenge, we introduce Polymath,\na self-optimizing agent with dynamic hierarchical workflow\nthat leverages the flexibility of task flow graphs and the ex-\npressiveness of code-represented workflows to solve a wide\nrange of real-world, dynamic problems. The proposed op-\ntimization methodology integrates multi-grid-inspired graph\noptimization with a self-reflection-guided evolutionary algo-\nrithm to refine workflows without labeled data. Experimental\nresults on six benchmark datasets across coding, math, and\nmulti-turn QA tasks show that Polymath achieves 8.1% av-\nerage improvement over state-of-the-art baselines. We will\nmake the source code publicly available upon acceptance.",
    "introduction": "et al. 2022; Hu et al. 2023), ReACT and tool use (Yao et al. 2022; Schick et al. 2023), and self-reflection (Shinn et al. 2023; Madaan et al. 2023). While these agentic workflows en- able LLMs to solve challenging problems, they are typically hand-engineered, task-specific, and labor-intensive to design and maintain. As the demand for LLM-driven applications expands, this reliance on manual workflow construction be- comes a bottleneck. It limits the scalability of LLM systems, slowing adaptation to new domains, and hindering the trans-fer of skills across tasks (Tang et al. 2023). Therefore, au- tomating agentic workflows for solving versatile and diverse tasks has emerged as a critical need. Many recent works focus on automating agentic work- flow discovery to reduce human involvement (Khattab et al. 2024; Yuksekgonul et al. 2024; Liu et al. 2023; Hu et al. 2024), yet full automation remains unsolved. DSPy (Khat- tab et al. 2024) requires manual setup, while methods like TextGrad (Yuksekgonul et al. 2024) and GPTSwarm (Zhuge et al. 2024) struggle to capture the diversity of workflows needed for broad task generalization (Yu, He, and Ying 2023; Yang et al. 2024b; Sun et al. 2023), since their op- timization objectives cannot represent the breadth of po- tential workflows. Although ADAS (Hu et al. 2024) and AFlow (Zhang et al. 2024a) improve expressiveness by rep- resenting workflows as code and refining them via execution feedback, they rely heavily on existing validation data and aim to generalize across task categories, limiting their adapt- ability to dynamic, real-world problems and task-specific challenges. On the other hand, Data Interpreter (Hong et al. 2024) proposed a task graph on top of a programmable node flow, but the approach lacks efficient self-learning and opti- mization. This highlights the critical need for more effective and adaptive techniques to fully automate the workflow gen- eration for dynamic, real-world problems to accelerate the application of LLMs across domains. In this work, we propose Polymath, a self-optimizing agent featuring a dynamic hierarchical workflow that lever- ages flexible task flow graphs combined with expressive, code-based workflows to tackle a broad range of real-world, dynamic problems. Moreover, we propose a novel hier- archical workflow optimization methodology, from multi- grid-inspired task flow graph optimization to an on- line self-reflection-guided evolutionary algorithm for code- represented workflow enhancement through LLM-based evaluators without the need for labeled datasets. Our con- tributions are as follows. \u2022 We propose a self-optimizing agent with dynamic hier- archical workflow that leverages the flexibility of task flow graphs and the expressiveness of subtask-level code- represented workflows to solve a wide range of real- world, dynamic problems. The task flow graph employs a divide-and-conquer approach to decompose and exe- cute subtasks based on the topological order, while thearXiv:2508.02959v2 [cs.AI] 7 Aug 2025 Final Answer: def text_match_wordz (text): # Regular expression to match a word containing the letter \u2026def workflow(instruction: str): instruction = \"### Instruction Background ### \\n\" + instruction + \" \\n\" <<<<<<< SEARCH ... # your designed workflow here. ======= import json # Task_3:",
    "conclusion": "### Provide suggestions, feedback, and reflection on the generated result. ### Task ### {current_task }: Calculate the final amount of milk in the bucket after 3 miles. Include the breakdown of milk amounts after each mile in y our response. ### Previous Generated Result ### {initial_response } Reflection: \"\"\" reflection_response = strong_reason_debug_assistant.initiate_chat (message= reflection_prompt ) reflection_output = extract_output (reflection_response , reason_agent_description [\"post_process_func \"]) # Incorporate reflection into final response final_response = f\"{ initial_response }\\n\\nReflection and improvements: \\n{reflection_output }\" # Extract the final numerical answer and ensure it's in the required boxed format import re numerical_answer = re.search (r'\\d+\\.\\d+', final_response ) if numerical_answer : boxed_answer = f\"$boxed {{{numerical_answer.group ()}}}$\" final_response = f\"{ final_response }\\n\\nFinal Answer: { boxed_answer }\" else: final_response = f\"{ final_response }\\n\\nError : No numerical answer found.\" return final_response # EVOLVE -BLOCK -ENDLLM Ensemble (Write code to calculate) Ranking & Get best response Reflection & Provide Final Answer Make sure the Output Format is Correct[Problem]: Sam is carrying a 2 gallon bucket of milk to his house from the barn, which is 3 miles away from his house. However, the bucket has a leak in it. For ea ch mile he walks, there is $ \\\\frac{2}{3}$ as much milk in the bucket as at the beginning of the mile. How many gallons of milk will be in the bucket when Sam gets home ? [Target Current sub task request to solve ]: Calculate the amount of milk remaining after each mile walked . Sam is carrying a 2 -gallon bucket of milk, and for each mile walked, there is 2/3 as much milk in the bucket as at the beginning of the mile .Figure 9: An example of generated code-represented workflows from self-reflection-guided evolutionary algorithm that includes ensembling approach, ranking or voting, reflection on top of coding and reasoning."
  },
  {
    "id": "2504.19565v3",
    "title": "Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training",
    "authors": "Meng Xiao, Xunxin Cai, Qingqing Long, Chengrui Wang, Yuanchun Zhou, Hengshu Zhu",
    "published": "2025-04-28",
    "category": "cs.CL",
    "arxiv_url": "http://arxiv.org/abs/2504.19565v3",
    "pdf_url": "https://arxiv.org/pdf/2504.19565v3",
    "abstract": "Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insuffi-\ncient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training\nin biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored\nexplicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedi-\ncal knowledge. Central to our approach is a collaborative multi-agent architecture, where specialized agents\u2014each guided by\nthe Medical Subject Headings (MeSH) hierarchy\u2014work in concert to autonomously extract, synthesize, and self-evaluate high-\nquality textual data from vast scientific literature. This agentic framework collectively generates and refines domain-specific\nquestion-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual\ninvolvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve no-\ntable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced\nproprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2,\ndespite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each\nagent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.\u2217\nKeywordsBiomedical Large Language Models, Agentic Corpus Distillation, Synthetic Question\u2013Answer Generation, Agentic\nAI, Knowledge Hierarchy Guidance\nCitationKnowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training.\nSci China Inf Sci, for review\n1",
    "introduction": "models has propelled bioinformatics into a new era [1], enabling the de- velopment of automated solutions across a spectrum of biomedical domains [2\u20135], and has demonstrated notable success for real-world biomedical question answering (QA) tasks [6\u20138]. However, the intricate and specialized nature of biomedical tasks means that general-purpose LLMs often fall short unless metic- ulously adapted and fine-tuned for the domain [9\u201311]. Progress in this area is further constrained by the scarcity of sufficiently large and high-quality biomedical corpora [12\u201314]. While existing open-source biomedical datasets are typically of high quality, their limited scale and narrow topical coverage restrict their utility for comprehensive LLM training. Conversely, directly leveraging the expansive body of domain-specific scientific literature offers the potential to cover a broader range of biomedical topics. Nevertheless, most of these resources are unannotated, and their inherent lack of structure, coupled with the absence of QA-format organization, greatly hinders their effective use for training question-answering models. Although the vast body of biomedical literature represents a valuable and authoritative resource, its complex terminology and dense conceptual structures pose significant barriers to automated process- ing and dataset construction [15,16]. Figure 1 visualizes the resulting data bottleneck. These challenges raise a crucial question:How can we automatically distill large-scale, high-quality QA pairs from scientific literature to empower biomedical LLM? * Corresponding author (email: hszhu@cnic.cn) \u2217The code supporting the findings of this study is available at DropBox.arXiv:2504.19565v3 [cs.CL] 18 Dec 2025 Sci China Inf Sci2 Existing Limitation and Challenge Base LLM Finetuned LLM Our Perspective Finetuned LLMLimited in Coverage Better ApplicationTraining CorporaAutomated Dataset DistillationOpen-sour ce CorporaOpen-sour ce Corpora Base LLM Finetuned LLMLimited by KnowledgeRaw Corpora Base LLM limited size large quantityhigh quality& Cold-start \uff011. Size issue of open-source train-sets 2. Quality issue of unlabeled document-sets Figure 1Analyzing the limitations and challenges of the existing pipeline. The motivation of this study is to utilize the high- quality but limited annotated corpus to generate large-scale training corpora from raw scientific documents. Prior research can be categorized into three mainstream categories. First, rule-based methods [17\u201319] rely on human-crafted standards for data cleaning and curation, which, while effective in reducing noise, are resource-intensive and difficult to scale. Second, knowledge graph-based approaches [20,21] structure biomedical information from texts into comprehensive graphs, but their dependence on manually curated sources limits both efficiency and scalability. Third, synthetic data generation methods [22\u201326] use LLMs to automate the creation of QA pairs and process large corpora. Yet, they often lack mechanisms for interdisciplinary collaboration [27, 28], resulting in insufficient diversity and robustness in the distilled data. Recent efforts [29] have introduced a knowledge hierarchy-guided [30] approach that leverages a single LLM agent to generate and evaluate biomedical QA data with improved alignment to domain ontologies. Its reliance on a single-agent, rule-based architecture presents inherent limitations in terms of diversity, cross-domain expertise, and collaborative reasoning [31]. In response to these limitations, we introduce theMulti-agent enhancedKnowledge hierArchy guIded biomedical dataset distiLlatIoN(m-KAILIN) framework\u2014a novel, fully automated, agentic frame- work designed to extract high-quality training corpora for biomedical LLMs. The",
    "conclusion": "In this work, we present a knowledge-driven, multi-agent framework for scientific corpus distillation\ntailored to biomedical large language model training. By leveraging a collaborative architecture\u2014where\nspecialized agents guided by biomedical ontologies autonomously generate, evaluate, and refine question-\nanswer pairs\u2014our approach addresses the limitations of existing open-source scientific corpora in both\nscale and quality. Through extensive experiments, we demonstrate that language models trained on our\nmulti-agent distilled datasets achieve substantial improvements in biomedical question-answering tasks,\noutperforming both strong open-source and proprietary baselines. Our ablation studies further validate\nthe effectiveness and synergy of each agent within the framework. This study highlights the potential of\nagentic, knowledge-guided corpus construction for advancing biomedical AI, and provides scalable tools\nand datasets to the community for future research."
  },
  {
    "id": "2510.05596v1",
    "title": "From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions",
    "authors": "Changyuan Zhao, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Geng Sun, Xianbin Wang, Shiwen Mao, Abbas Jamalipour",
    "published": "2025-10-07",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2510.05596v1",
    "pdf_url": "https://arxiv.org/pdf/2510.05596v1",
    "abstract": "\u2014Self-evolving agentic artificial intelligence (AI) of-\nfers a new paradigm for future wireless systems by enabling\nautonomous agents to continually adapt and improve without\nhuman intervention. Unlike static AI models, self-evolving agents\nembed an autonomous evolution cycle that updates models,\ntools, and workflows in response to environmental dynamics.\nThis paper presents a comprehensive overview of self-evolving\nagentic AI, highlighting its layered architecture, life cycle, and\nkey techniques, including tool intelligence, workflow optimization,\nself-reflection, and evolutionary learning. We further propose\na multi-agent cooperative self-evolving agentic AI framework,\nwhere multiple large language models (LLMs) are assigned role-\nspecialized prompts under the coordination of a supervisor agent.\nThrough structured dialogue, iterative feedback, and systematic\nvalidation, the system autonomously executes the entire life cycle\nwithout human intervention. A case study on antenna evolution in\nlow-altitude wireless networks (LA WNs) demonstrates how the\nframework autonomously upgrades fixed antenna optimization\ninto movable antenna optimization. Experimental results show\nthat the proposed self-evolving agentic AI autonomously improves\nbeam gain and restores degraded performance by up to 52.02%,\nconsistently surpassing the fixed baseline with little to no human\nintervention and validating its adaptability and robustness for\nnext-generation wireless intelligence.\nIndex Terms\u2014Agentification, self-evolving agentic AI, large\nlanguage models, low-altitude wireless networks,\nI.",
    "introduction": "this notion remained largely conceptual. C. Zhao is with the College of Computing and Data Science, Nanyang Technological University, Singapore, and CNRS@CREATE, 1 Create Way, 08-01 Create Tower, Singapore 138602 (e-mail: zhao0441@e.ntu.edu.sg). R. Zhang, J. Wang, and D. Niyato are with the College of Computing and Data Science, Nanyang Technological University, Singapore (e-mail: ruichen.zhang@ntu.edu.sg; jiacheng.wang@ntu.edu.sg; dniyato@ntu.edu.sg). G. Sun is with College of Computer Science and Technology, Jilin Univer- sity, China 130012, (e-mail: sungeng@jlu.edu.cn). X. Wang is with the Department of Electrical and Computer Engineer- ing, Western University, London, ON, N6A 5B9, Canada (e-mail: xian- bin.wang@uwo.ca). S. Mao is with the Department of Electrical and Computer Engineering, Auburn University, Auburn, USA (e-mail: smao@ieee.org). A. Jamalipour is with the School of Electrical and Computer Engineering, University of Sydney, Australia (e-mail: a.jamalipour@ieee.org).Building on this foundational idea, the paradigm of agentifi- cation,through the process of transforming static AI models into autonomous and adaptive agents, has recently gained traction [2]. This development underpins the emergence of self-evolving agentic AI [3], turning the theoretical promise of self-improvement into practical agent architectures. Self- evolving agentic AI represents a new generation of au- tonomous agent systems that can continuously adapt and self- improve through dynamic interaction with their environments, effectively bridging the powerful yet static capabilities of AI models with the continual adaptability required by edge systems [4]. In practice, such agents leverage a synergy of learning techniques: reinforcement learning for trial-and-error optimization, self-supervised learning for extracting structure from raw data, curriculum learning for staged skill growth, and self-reflection for diagnosing weaknesses [3]. Although existing techniques such as continuous learning, life-long learning, incremental learning, and domain adapta- tion offer mechanisms to update static models, they typically rely on human intervention and only target isolated stages of the model life cycle [3]. In contrast, self-evolving agents can autonomously execute the full AI agent life cycle without human intervention, from acquiring and curating new expe- riences to refining and integrating them to updating mod- els and deploying the improved capabilities. This evolution cycle allows the agent to continuously enhance individual components such as the model, memory, prompts, tools, and workflow strategies efficiently and effectively. In doing so, the system dynamically adapts to changing tasks, dynamic contexts, and varying resources while ensuring safety, stability, and sustained performance. Given the advantages of self-evolving agents, this emerg- ing concept is increasingly becoming a practical reality. A representative example is Google DeepMind\u2019s AlphaEvolve, an evolutionary coding agent that embodies the self-evolving paradigm in action1. AlphaEvolve iteratively performs closed- loop cycles of code generation, automated evaluation, and intelligent mutation, guided by large language models (LLMs) and evolutionary selection mechanisms. The system has achieved tangible impact by discovering algorithms surpassing 50-year-old benchmarks, reclaiming 0.7% of Google\u2019s global compute resources, and accelerating AI training. 1https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered- coding-agent-for-designing-advanced-algorithms/arXiv:2510.05596v1 [cs.AI] 7 Oct 2025 2 With the emergence of 6G and next-generation commu- nication technologies, edge and Internet of Things (IoT) devices are anticipated to operate with unprecedented levels of autonomy and intelligence [5]. Agentification provides an important step in this",
    "conclusion": "arXiv:2505.22311, 2025. [7] D. B. Acharya, K. Kuppan, and B. Divya, \u201cAgentic AI: Autonomous intelligence for complex goals\u2013a comprehensive survey,\u201dIEEE Access, 2025. [8] C. Zhao, J. Wang, R. Zhang, D. Niyato, G. Sun, H. Du, D. I. Kim, and A. Jamalipour, \u201cGenerative AI-enabled wireless communi- cations for robust low-altitude economy networking,\u201darXiv preprint arXiv:2502.18118, 2025. [9] T. Schick, J. Dwivedi-Yu, R. Dess `\u0131, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom, \u201cToolformer: Language models can teach themselves to use tools,\u201dAdvances in Neural Infor- mation Processing Systems, vol. 36, pp. 68 539\u201368 551, 2023. [10] G. Zhang, K. Chen, G. Wan, H. Chang, H. Cheng, K. Wang, S. Hu, and L. Bai, \u201cEvoflow: Evolving diverse agentic workflows on the fly,\u201darXiv preprint arXiv:2502.07373, 2025. [11] H. Sun, Y . Zhuang, L. Kong, B. Dai, and C. Zhang, \u201cAdaplanner: Adaptive planning from feedback with language models,\u201dAdvances in neural information processing systems, vol. 36, pp. 58 202\u201358 245, 2023. [12] Y . Xu, J. Wang, R. Zhang, D. Niyato, D. Rajan, L. Yu, H. Zhou, A. Jamalipour, and X. Wang, \u201cEnhancing wireless networks for IoT with large vision models: Foundations and applications,\u201darXiv preprint arXiv:2508.00583, 2025. [13] 3GPP, \u201cEvolved Universal Terrestrial Radio Access (E- UTRA); Radio Resource Control (RRC); Protocol specifica- tion,\u201d 3rd Generation Partnership Project (3GPP), Technical Specification (TS) 36.331, 04 2017, version 14.2.2. [On- line]. Available: https://portal.3gpp.org/desktopmodules/Specifications/ SpecificationDetails.aspx?specificationId=2440 [14] L. Zhu, W. Ma, W. Mei, Y . Zeng, Q. Wu, B. Ning, Z. Xiao, X. Shao, J. Zhang, and R. Zhang, \u201cA tutorial on movable antennas for wireless networks,\u201dIEEE Communications Surveys & Tutorials, pp. 1\u20131, 2025. [15] C. Qian, W. Liu, H. Liu, N. Chen, Y . Dang, J. Li, C. Yang, W. Chen, Y . Su, X. Conget al., \u201cChatdev: Communicative agents for software development,\u201darXiv preprint arXiv:2307.07924, 2023."
  },
  {
    "id": "2510.04618v1",
    "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
    "authors": "Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, Kunle Olukotun",
    "published": "2025-10-06",
    "category": "cs.LG",
    "arxiv_url": "http://arxiv.org/abs/2510.04618v1",
    "pdf_url": "https://arxiv.org/pdf/2510.04618v1",
    "abstract": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on\ncontext adaptation\u2014modifying inputs with instructions, strategies, or evidence, rather than weight updates.\nPrior approaches improve usability but often suffer from brevity bias, which drops domain insights for\nconcise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on\nthe adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (AgenticContextEngineering),\na framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies\nthrough a modular process of generation, reflection, and curation. ACE prevents collapse with structured,\nincremental updates that preserve detailed knowledge and scale with long-context models. Across agent\nand domain-specific benchmarks, ACE optimizes contexts both offline (e.g.,system prompts) and online (e.g.,\nagent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while\nsignificantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without\nlabeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard,\nACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder\ntest-challenge split, despite using a smaller open-source model. These results show that comprehensive,\nevolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.\n1",
    "introduction": "Base LLMICLGEPADC ACE40.042.545.047.550.052.555.057.560.0Accuracy (%)\n42.4%46.0%46.4%51.9%59.5%Agent: AppWorld\nBase LLMICLGEPADC ACE6870727476788082\n70.7%72.3%73.5%74.2%78.3%Domain Knowledge: FiNER\nBase LLMICLGEPADC ACE6668707274767880\n67.5%67.0%71.5%\n69.5%76.5%Numerical Reasoning: Formula\nFigure 1:Overall Performance Results.Our proposed framework, ACE, consistently outperforms strong\nbaselines across agent and domain-specific reasoning tasks.\nModern AI applications based on large language models (LLMs), such as LLM agents [ 49,52] and compound\nAI systems [ 55], increasingly depend oncontext adaptation. Instead of modifying model weights, contextarXiv:2510.04618v1  [cs.LG]  6 Oct 2025\nadaptation improves performance after model training by incorporating clarified instructions, structured\nreasoning steps, or domain-specific input formats directly into the model\u2019s inputs. Contexts underpin\nmany AI system components, including system prompts that guide downstream tasks [ 4,36], memory that\ncarries past facts and experiences [ 41,48], and factual evidence that reduces hallucination and supplements\nknowledge [6].\nAdapting throughcontextsrather thanweightsoffers several key advantages. Contexts are interpretable and\nexplainable for users and developers [ 45,47], allow rapid integration of new knowledge at runtime [ 7,27],\nand can be shared across models or modules in a compound system [ 23]. Meanwhile, advances in long-\ncontext LLMs [ 39] and context-efficient inference such as KV cache reuse [ 17,51] are making context-based",
    "conclusion": "the current playbook - Avoid redundancy - if similar advice already exists, only add new content that is a perfect complement to the existing playbook - Do NOT regenerate the entire playbook - only provide the additions needed - Focus on quality over quantity - a focused, well-organized playbook is better than an exhaustive one - Format your response as a PURE JSON object with specific sections - For any operation if no new content to add, return an empty list for the operations field - Be concise and specific - each addition should be actionable Training Context: Total token budget: {token_budget} tokens Training progress: Sample {current_step} out of {total_samples} Current Playbook Stats: {playbook_stats} Recent Reflection: {recent_reflection} Current Playbook: {current_playbook} Question Context: {question_context} Your Task: Output ONLY a valid JSON object with these exact fields: - reasoning: your chain of thought / reasoning / thinking process, detailed analysis and calculations - operations: a list of operations to be performed on the playbook - type: the type of operation to be performed - section: the section to add the bullet to - content: the new content of the bullet Available Operations: 1. ADD: Create new bullet points with fresh IDs - section: the section to add the new bullet to - content: the new content of the bullet. Note: no need to include the bullet_id in the content like \u2018[ctx-00263] helpful=1 harmful=0 ::\u2019, the bullet_id will be added by the system. RESPONSE FORMAT - Output ONLY this JSON structure (no markdown, no code blocks): { \"reasoning\" : \"[Your chain of thought / reasoning / thinking process, detailed analysis and calculations here]\" , \"operations\" : [ { { \"type\" : \"ADD\" , \"section\" : \"formulas_and_calculations\" , \"content\" : \"[New calculation method...]\" } } ] }Figure 14: ACE Curator prompt on FINER 23"
  },
  {
    "id": "2510.27051v1",
    "title": "Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement",
    "authors": "Aaditya Shukla, Sidney Knowles, Meenakshi Madugula, Dave Farris, Ryan Angilly, Santiago Pombo, Anbang Xu, Lu An, Abhinav Balasubramanian, Tan Yu, Jiaxiang Ren, Rama Akkiraju",
    "published": "2025-10-30",
    "category": "cs.AI",
    "arxiv_url": "http://arxiv.org/abs/2510.27051v1",
    "pdf_url": "https://arxiv.org/pdf/2510.27051v1",
    "abstract": "\u2014Enterprise AI agents must continuously adapt to\nmaintain accuracy, reduce latency, and remain aligned with user\nneeds. We present a practical implementation of a data flywheel\nin NVInfo AI, NVIDIA\u2019s Mixture-of-Experts (MoE) Knowledge\nAssistant serving over 30,000 employees. By operationalizing\na MAPE-driven data flywheel, we built a closed-loop system\nthat systematically addresses failures in retrieval-augmented\ngeneration (RAG) pipelines and enables continuous learning.\nOver a 3-month post-deployment period, we monitored feedback\nand collected 495 negative samples. Analysis revealed two major\nfailure modes: routing errors (5.25%) and query rephrasal errors\n(3.2%). Using NVIDIA NeMo microservices, we implemented\ntargeted improvements through fine-tuning. For routing, we\nreplaced a Llama 3.1 70B model with a fine-tuned 8B variant,\nachieving 96% accuracy, a 10\u00d7 reduction in model size, and 70%\nlatency improvement. For query rephrasal, fine-tuning yielded\na 3.7% gain in accuracy and a 40% latency reduction. Our\napproach demonstrates how human-in-the-loop (HITL) feedback,\nwhen structured within a data flywheel, transforms enterprise\nAI agents into self-improving systems. Key learnings include\napproaches to ensure agent robustness despite limited user\nfeedback, navigating privacy constraints, and executing staged\nrollouts in production. This work offers a repeatable blueprint\nfor building robust, adaptive enterprise AI agents capable of\nlearning from real-world usage at scale.\nIndex Terms\u2014MAPE control loops, data flywheel, MoE sys-\ntems, self-improving AI agents, continuous learning, user feed-\nback, parameter-efficient fine-tuning (PEFT), RAG, HITL, en-\nterprise AI, latency optimization\nI.",
    "introduction": "systems often deteriorates post-deployment due to evolv- ing user intent, domain drift, and the absence of systematic feedback integration. A central challenge in operationalizing such agents lies in enabling them to continuously adapt based on real-world usage patterns and user feedback, without requiring full-scale retraining or infrastructure overhauls. While retrieval-augmented generation (RAG) pipelines and Mixture-of-Experts (MoE) architectures have improved the relevance and efficiency of enterprise AI agents, most pro- duction deployments remain static and reactive. Feedbackmechanisms, if present, are frequently decoupled from the model improvement process. This disconnect results in stag- nant accuracy, increasing latency, and declining user trust. There is a pressing need for closed-loop systems that can monitor agent performance, analyze failure modes, and ex- ecute targeted optimizations in a cost-efficient and privacy- aware manner. In this work, we introduce a MAPE-based data flywheel framework that enables continuous learning in enterprise AI agents through a modular, feedback-driven pipeline. Adapted from self-adaptive control loops, this framework supports the deployment of agents that evolve incrementally over time. We apply this approach to NVIDIA\u2019s deployment of NVInfo AI, an internal Knowledge Assistant Agent that serves over 30,000 employees across diverse domains including engineer- ing, operations, HR, and sales. NVInfo AI integrates user feedback with performance telemetry to identify actionable failure signals and execute targeted updates using parameter- efficient fine-tuning (PEFT) and model specialization. Over a three-month observation window, we collected and analyzed 495 negative feedback samples, revealing two dom- inant sources of failure: routing errors (5.25 %) and query rephrasal errors (3.2 %). Utilizing NVIDIA NeMo microser- vices, we applied lightweight, component-specific fine-tuning strategies to improve performance. \u2022For routing, we reduced model size by a factor of ten (from 70 billion to 8 billion parameters) while maintaining 96% accuracy and reducing latency by 70%. \u2022For query rephrasal, we achieved a 3.7% improvement in accuracy (measured on a synthetic dataset generated from manually analyzed incorrect queries, expanded to 5,000 examples and split 80/10/10), along with a 40% reduction in response latency. This work makes three key contributions: \u2022We demonstratea novel application of the MAPE control loop to the domain of GenAI agent improvement, bridging observability and action in a continuous feedback pipeline. \u2022We presentan empirical analysis of post-deploymentarXiv:2510.27051v1 [cs.AI] 30 Oct 2025 failure modesin a production-grade enterprise AI agent, informed by real user feedback. \u2022We providea modular implementation blueprint using NVIDIA NeMo microservices, offering a practical archi- tecture for organizations seeking to build adaptive and self- correcting AI agents. II. BACKGROUND ANDRELATEDWORK A. From MAPE-K to Agentic AI: Foundations of Self-Adaptive Systems The MAPE-K (Monitor, Analyze, Plan, Execute \u2013 Knowl- edge) reference model, introduced by IBM [1], remains foundational for designing self-adaptive software systems by structuring behavior into a control loop that continuously responds to environmental changes, with its modular archi- tecture enabling broad adoption across multiple domains [2]\u2013 [6]. Central to its evolution is the Knowledge component, which supports long-term reasoning and intelligent adaptation, especially when integrated with machine learning to enable predictive and causal decision-making [7]\u2013[9]. Within agentic AI frameworks,",
    "conclusion": "{ \"Question\": { \"type\": \"string\", \"description\": \"Generated Question from the input document.\" }, \"Answer\": { \"type\": \"string\", \"description\": \"Corresponding Answer from the input document that answers the Question .\" }, \"Thought\": { \"type\": \"string\", \"description\": \"Short analysis of your understanding from the Question.\" }, \"Process\": { \"type\": \"string\", \"description\": \"I need to use the Enterprise Knowledge tool.\" }, \"Action\": { \"type\": \"string\", \"description\": \"EnterpriseKnowledge\" }, \"Action Input\": { \"type\": \"list\", \"description\": \"A single line Python list of rephrased queries.\" } } } Examples Input Document:<Content of input document> Input Document url:<url of input document> Output { \"Question\": \"I am based in the Netherlands, when is pay day?\", \"Answer\": \"25th of every month\", \"Thought\": \"Payroll timing question; include location keywords in rephrased queries.\", \"Process\": \"I need to use the Enterprise Knowledge tool\", \"Action\": \"EnterpriseKnowledge\", \"Action Input\": [ \"payday schedule netherlands\", \"netherlands pay days\" ] } Input Document:<Content of input document> Input Document url:<url of input document> Output { \"Question\": \"point me to gpu fcv page?\", \"Answer\": \"https://nvidia.sharepoint.com/sites/TechnicalTraining/ASIC%20teams.aspx\", \"Thought\": \"Needs GPU FCV (Full Chip Verification) page.\", \"Process\": \"I need to use the Enterprise Knowledge tool\", \"Action\": \"EnterpriseKnowledge\", \"Action Input\": [ \"gpu fcv page company\", \"fcv gpu url\" ] } Input Document:<Content of input document> Input Document url:<url of input document> Output { \"Question\": \"ok, i\u2019m looking for an nvidia icon for biotech / pharmaceuticals to use in a presentation. can you help me find that?\", \"Answer\": \"https://nvidia.sharepoint.com/sites/nvinfo/brand/Pages/default.aspx\", \"Thought\": \"Needs a company icon for biotech/pharma use.\", \"Process\": \"I need to use the Enterprise Knowledge tool\", \"Action\": \"EnterpriseKnowledge\", \"Action Input\": [ \"company icons\", \"company logos biotech\" ] } Task output format Generate3 pairsby following the instructions based on the Input Document. Strictly return only a Python list of pairs and nothing else. Input Document:<Content of input document> Input Document url:<url of input document> Output:###"
  }
]